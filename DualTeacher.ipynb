{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearning DualTeacher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:54:08.478341Z",
     "iopub.status.busy": "2025-08-16T15:54:08.478082Z",
     "iopub.status.idle": "2025-08-16T15:54:53.925665Z",
     "shell.execute_reply": "2025-08-16T15:54:53.924076Z",
     "shell.execute_reply.started": "2025-08-16T15:54:08.478323Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge-score) (2024.2.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=6a1623a469f08851a1994092de20c4c54ce9c65bfd3beef88799342baee9d2b4\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 15:54:35.712752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755359676.074921      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755359676.176076      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs disponibili: 2\n",
      "GPU 0: Tesla T4\n",
      "GPU 1: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Configurazioni\n",
    "MODEL_PATH = \"/kaggle/input/olmo-model/semeval25-unlearning-1B-model\"\n",
    "DATA_PATH = \"/kaggle/input/olmo-model/semeval25-unlearning-data\"\n",
    "\n",
    "print(f\"GPUs disponibili: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Caricamento Dati e Modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:54:53.928809Z",
     "iopub.status.busy": "2025-08-16T15:54:53.927770Z",
     "iopub.status.idle": "2025-08-16T15:54:55.468476Z",
     "shell.execute_reply": "2025-08-16T15:54:55.467556Z",
     "shell.execute_reply.started": "2025-08-16T15:54:53.928773Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94017127e0da4a0b9023caaefb130e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19bdaf32bab424d9f7cecc0fa116e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497b96a663df403e81621b9376db9fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset salvati e tokenizer caricato\n"
     ]
    }
   ],
   "source": [
    "# Caricamento dataset\n",
    "retain_train_df = pd.read_parquet(f\"{DATA_PATH}/data/retain_train-00000-of-00001.parquet\", engine='pyarrow')\n",
    "retain_validation_df = pd.read_parquet(f\"{DATA_PATH}/data/retain_validation-00000-of-00001.parquet\", engine='pyarrow')\n",
    "forget_train_df = pd.read_parquet(f\"{DATA_PATH}/data/forget_train-00000-of-00001.parquet\", engine='pyarrow')\n",
    "forget_validation_df = pd.read_parquet(f\"{DATA_PATH}/data/forget_validation-00000-of-00001.parquet\", engine='pyarrow')\n",
    "\n",
    "# Salvataggio in formato JSONL\n",
    "!mkdir -p train validation\n",
    "retain_train_df.to_json('train/retain.jsonl', orient='records', lines=True)\n",
    "forget_train_df.to_json('train/forget.jsonl', orient='records', lines=True)\n",
    "retain_validation_df.to_json('validation/retain.jsonl', orient='records', lines=True)\n",
    "forget_validation_df.to_json('validation/forget.jsonl', orient='records', lines=True)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Dataset salvati e tokenizer caricato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:54:55.469690Z",
     "iopub.status.busy": "2025-08-16T15:54:55.469467Z",
     "iopub.status.idle": "2025-08-16T15:54:55.478117Z",
     "shell.execute_reply": "2025-08-16T15:54:55.477328Z",
     "shell.execute_reply.started": "2025-08-16T15:54:55.469667Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UnlearningDataset(Dataset):\n",
    "    def __init__(self, data_source, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if isinstance(data_source, pd.DataFrame):\n",
    "            self.data = data_source\n",
    "            print(f\"Caricati {len(self.data)} esempi dal DataFrame\")\n",
    "        elif isinstance(data_source, str):\n",
    "            data_list = []\n",
    "            with open(data_source, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    item = json.loads(line.strip())\n",
    "                    data_list.append(item)\n",
    "            self.data = pd.DataFrame(data_list)\n",
    "            print(f\"Caricati {len(self.data)} esempi da {data_source}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        prompt = self.tokenizer(\n",
    "            self.data.iloc[idx][\"input\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            return_tensors=None\n",
    "        )\n",
    "        labels = self.tokenizer(\n",
    "            f\"{self.data.iloc[idx]['input']} {self.data.iloc[idx]['output']}\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True, \n",
    "            max_length=512, \n",
    "            return_tensors=None\n",
    "        )\n",
    "        attention_mask = prompt[\"attention_mask\"]\n",
    "        start_locs = self.tokenizer(self.data.iloc[idx][\"input\"])\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(prompt[\"input_ids\"]),\n",
    "            \"attention_mask\": torch.tensor(attention_mask),\n",
    "            \"start_locs\": len(start_locs[\"input_ids\"]) - 1,\n",
    "            \"labels\": torch.tensor(labels[\"input_ids\"]),\n",
    "            \"split\": 1 if self.data.iloc[idx][\"split\"] == \"forget\" else 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:54:55.480221Z",
     "iopub.status.busy": "2025-08-16T15:54:55.480021Z",
     "iopub.status.idle": "2025-08-16T15:54:55.520835Z",
     "shell.execute_reply": "2025-08-16T15:54:55.520099Z",
     "shell.execute_reply.started": "2025-08-16T15:54:55.480205Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricati 2248 esempi dal DataFrame\n",
      "Dataset creato con 2248 esempi\n"
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloader\n",
    "batch_size = 4\n",
    "train_data = pd.concat([retain_train_df, forget_train_df], ignore_index=True)\n",
    "dataset = UnlearningDataset(train_data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Dataset creato con {len(dataset)} esempi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DualTeacher Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualTeacherTrainer:\n",
    "    def __init__(self, model_path, tokenizer, teacher_lora_config, student_lora_config, device_map=None):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.teacher_lora_config = teacher_lora_config\n",
    "        self.student_lora_config = student_lora_config\n",
    "        self.device_map = device_map or {\"student\": \"cuda:0\", \"teacher\": \"cuda:1\"}\n",
    "        \n",
    "        self.good_teacher = None\n",
    "        self.student_model = None\n",
    "        self.initial_state_dict = {}\n",
    "        \n",
    "    def setup_models(self):\n",
    "        \"\"\"Initialize and setup both teacher and student models\"\"\"\n",
    "        print(\"ðŸ”§ Setting up models...\")\n",
    "        \n",
    "        # Setup good teacher with LoRA (for training)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(self.model_path, local_files_only=True)\n",
    "        self.good_teacher = get_peft_model(base_model, self.teacher_lora_config)\n",
    "        self.good_teacher = self.good_teacher.to(self.device_map[\"teacher\"])\n",
    "        self.good_teacher.print_trainable_parameters()\n",
    "        \n",
    "        # Setup student model with LoRA\n",
    "        base_model_student = AutoModelForCausalLM.from_pretrained(self.model_path, local_files_only=True)\n",
    "        self.student_model = get_peft_model(base_model_student, self.student_lora_config)\n",
    "        self.student_model = self.student_model.to(self.device_map[\"student\"])\n",
    "        self.student_model.print_trainable_parameters()\n",
    "        \n",
    "        # Save initial state for task vector calculation\n",
    "        for name, param in self.student_model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.initial_state_dict[name] = param.data.clone()\n",
    "        \n",
    "        print(\"âœ… Models setup completed\")\n",
    "    \n",
    "    def merge_and_unload_teacher(self):\n",
    "        \"\"\"Convert good teacher from LoRA to base model after training\"\"\"\n",
    "        print(\"ðŸ”„ Converting good teacher to base model...\")\n",
    "        \n",
    "        # Merge LoRA weights into base model\n",
    "        merged_model = self.good_teacher.merge_and_unload()\n",
    "        \n",
    "        # Replace the LoRA model with the merged base model\n",
    "        self.good_teacher = merged_model.to(self.device_map[\"teacher\"])\n",
    "        \n",
    "        # Set to eval mode and freeze\n",
    "        self.good_teacher.eval()\n",
    "        for param in self.good_teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        print(\"âœ… Good teacher converted to base model and frozen\")\n",
    "        \n",
    "    def create_bad_teacher_logits(self, good_teacher_outputs):\n",
    "        \"\"\"Create bad teacher logits for unlearning\"\"\"\n",
    "        device = self.device_map[\"teacher\"]\n",
    "        return torch.normal(\n",
    "            mean=0, std=1, size=good_teacher_outputs.logits.shape\n",
    "        ).to(device) + torch.ones(good_teacher_outputs.logits.shape[-1]).to(device)\n",
    "    \n",
    "    def compute_kl_divergence(self, batch):\n",
    "        \"\"\"Compute KL divergence loss for dual teacher training\"\"\"\n",
    "        student_device = self.device_map[\"student\"]\n",
    "        teacher_device = self.device_map[\"teacher\"]\n",
    "        \n",
    "        # Student forward pass (with LoRA)\n",
    "        student_outputs = self.student_model(\n",
    "            batch[\"input_ids\"].to(student_device),\n",
    "            attention_mask=batch[\"attention_mask\"].to(student_device),\n",
    "            labels=batch[\"labels\"].to(student_device),\n",
    "        )\n",
    "        \n",
    "        # Teacher forward pass (base model, no LoRA)\n",
    "        with torch.no_grad():\n",
    "            good_teacher_outputs = self.good_teacher(\n",
    "                batch[\"input_ids\"].to(teacher_device),\n",
    "                attention_mask=batch[\"attention_mask\"].to(teacher_device),\n",
    "                labels=batch[\"labels\"].to(teacher_device),\n",
    "            )\n",
    "        \n",
    "        # Create bad teacher and combine\n",
    "        l = torch.unsqueeze(batch[\"split\"], -1)\n",
    "        l = torch.unsqueeze(l, -1)\n",
    "        \n",
    "        bad_teacher = self.create_bad_teacher_logits(good_teacher_outputs)\n",
    "        prob_p = torch.nn.functional.softmax(bad_teacher, -1)\n",
    "        prob_f = torch.nn.functional.softmax(good_teacher_outputs.logits, -1)\n",
    "        prob_q = torch.nn.functional.softmax(student_outputs.logits, -1).to(teacher_device)\n",
    "        \n",
    "        out_teacher = (1 - l.to(teacher_device)) * prob_f + l.to(teacher_device) * prob_p\n",
    "        \n",
    "        loss = (out_teacher * (torch.log(out_teacher + 1e-12) - torch.log(prob_q + 1e-12))).sum(-1).mean()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train_good_teacher(self, dataloader, num_epochs=2, lr=1e-4):\n",
    "        \"\"\"Train the good teacher on retain samples using LoRA\"\"\"\n",
    "        print(\"ðŸš€ Training good teacher with LoRA...\")\n",
    "        \n",
    "        self.good_teacher.train()\n",
    "        optimizer = torch.optim.Adam(self.good_teacher.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"ðŸ“… Epoca {epoch + 1}/{num_epochs} - Good Teacher Training\")\n",
    "            \n",
    "            epoch_losses = []\n",
    "            retain_batches_processed = 0\n",
    "            \n",
    "            with tqdm(total=len(dataloader), desc=f\"Good Teacher Epoca {epoch+1}\") as pbar:\n",
    "                for batch in dataloader:\n",
    "                    # Filter retain samples only\n",
    "                    split = batch['split']\n",
    "                    retain_mask = (split == 0)\n",
    "                    \n",
    "                    if not retain_mask.any():\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract retain samples\n",
    "                    input_ids = batch['input_ids'][retain_mask].to(self.device_map[\"teacher\"])\n",
    "                    attention_mask = batch['attention_mask'][retain_mask].to(self.device_map[\"teacher\"])\n",
    "                    labels = batch['labels'][retain_mask].to(self.device_map[\"teacher\"])\n",
    "                    \n",
    "                    if input_ids.size(0) == 0:\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = self.good_teacher(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    epoch_losses.append(loss.item())\n",
    "                    retain_batches_processed += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                    if retain_batches_processed % 100 == 0:\n",
    "                        pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "            if epoch_losses:\n",
    "                avg_loss = np.mean(epoch_losses)\n",
    "                print(f\"ðŸ“Š Good Teacher Epoca {epoch+1} - Loss medio: {avg_loss:.4f}\")\n",
    "        \n",
    "        print(\"âœ… Good teacher training completed\")\n",
    "        \n",
    "        # Convert to base model after training\n",
    "        self.merge_and_unload_teacher()\n",
    "        \n",
    "    def train_student(self, dataloader, num_epochs=4, lr=1e-4):\n",
    "        \"\"\"Train student model with dual teacher approach (student uses LoRA, teacher is base model)\"\"\"\n",
    "        print(\"ðŸš€ Training student with LoRA against base model teacher...\")\n",
    "        \n",
    "        self.student_model.train()\n",
    "        optimizer = torch.optim.AdamW(self.student_model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_losses = []\n",
    "            \n",
    "            with tqdm(total=len(dataloader), desc=f\"Student Epoca {epoch+1}\") as pbar:\n",
    "                for batch in dataloader:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.compute_kl_divergence(batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    epoch_losses.append(loss.item())\n",
    "                    pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "                    pbar.update(1)\n",
    "            \n",
    "            avg_loss = np.mean(epoch_losses)\n",
    "            print(f\"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Save model after each epoch\n",
    "            self.save_model(f\"studentmodel_epoch_{epoch+1}\")\n",
    "        \n",
    "        print(\"âœ… Student training completed\")\n",
    "        \n",
    "    def save_model(self, save_path):\n",
    "        \"\"\"Save student model and tokenizer\"\"\"\n",
    "        self.student_model.save_pretrained(save_path)\n",
    "        self.tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "    def save_good_teacher(self, save_path):\n",
    "        \"\"\"Save good teacher base model\"\"\"\n",
    "        self.good_teacher.save_pretrained(save_path)\n",
    "        \n",
    "    def calculate_task_vector(self):\n",
    "        \"\"\"Calculate task vector from initial to final state\"\"\"\n",
    "        task_vector = {}\n",
    "        for name, param in self.student_model.named_parameters():\n",
    "            if param.requires_grad and name in self.initial_state_dict:\n",
    "                task_vector[name] = param.data - self.initial_state_dict[name]\n",
    "        return task_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Trainer and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:54:55.548798Z",
     "iopub.status.busy": "2025-08-16T15:54:55.548454Z",
     "iopub.status.idle": "2025-08-16T15:55:48.528242Z",
     "shell.execute_reply": "2025-08-16T15:55:48.527344Z",
     "shell.execute_reply.started": "2025-08-16T15:54:55.548774Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Setting up models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e0175687e54992a2efbbc1276197fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 1,288,175,616 || trainable%: 0.6512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74932dd73572444e8c0ec694676c1cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 1,283,981,312 || trainable%: 0.3267\n",
      "âœ… Models setup completed\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA for teacher and student\n",
    "teacher_lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "student_lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = DualTeacherTrainer(\n",
    "    model_path=MODEL_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    teacher_lora_config=teacher_lora_config,\n",
    "    student_lora_config=student_lora_config,\n",
    "    device_map={\"student\": \"cuda:0\", \"teacher\": \"cuda:1\"}\n",
    ")\n",
    "\n",
    "# Setup models\n",
    "trainer.setup_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:55:48.529500Z",
     "iopub.status.busy": "2025-08-16T15:55:48.529203Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training good teacher with LoRA...\n",
      "ðŸ“… Epoca 1/2 - Good Teacher Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Good Teacher Epoca 1:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 475/562 [13:53<03:07,  2.16s/it, Loss=7.1021]"
     ]
    }
   ],
   "source": [
    "# Train good teacher (will be converted to base model after training)\n",
    "trainer.train_good_teacher(dataloader, num_epochs=2, lr=1e-4)\n",
    "\n",
    "# Save good teacher\n",
    "trainer.save_good_teacher(\"good_teacher_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train student with dual teacher approach\n",
    "# Student uses LoRA, good teacher is now base model\n",
    "trainer.train_student(dataloader, num_epochs=4, lr=1e-4)\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model(\"studentmodel_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results and Task Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "os.makedirs('balanced_results', exist_ok=True)\n",
    "\n",
    "# Save student model\n",
    "trainer.save_model('balanced_results/balanced_model')\n",
    "\n",
    "# Calculate and save task vector\n",
    "task_vector = trainer.calculate_task_vector()\n",
    "torch.save(task_vector, 'balanced_results/task_vector.pt')\n",
    "\n",
    "print(\"âœ… Results saved in balanced_results/\")\n",
    "print(\"- balanced_model/: Student model with unlearning\")\n",
    "print(\"- task_vector.pt: Task vector for future applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import types\n",
    "\n",
    "try:\n",
    "    import evaluation\n",
    "    import importlib\n",
    "    importlib.reload(evaluation)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def run_evaluation(\n",
    "    data_path,\n",
    "    checkpoint_path,\n",
    "    output_dir=\"eval_results\",\n",
    "    mia_data_path=None,\n",
    "    mmlu_metrics_file_path=None,\n",
    "    max_new_tokens=256,\n",
    "    batch_size=25,\n",
    "    debug=False,\n",
    "    compute_metrics_only=False,\n",
    "    seed=42,\n",
    "    keep_files=False,\n",
    "):\n",
    "    try:\n",
    "        # Costruiamo un oggetto args simile a quello di argparse\n",
    "        args = types.SimpleNamespace(\n",
    "            data_path=data_path,\n",
    "            checkpoint_path=checkpoint_path,\n",
    "            output_dir=output_dir,\n",
    "            mia_data_path=mia_data_path,\n",
    "            mmlu_metrics_file_path=mmlu_metrics_file_path,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            batch_size=batch_size,\n",
    "            debug=debug,\n",
    "            compute_metrics_only=compute_metrics_only,\n",
    "            seed=seed,\n",
    "            keep_files=keep_files,\n",
    "        )\n",
    "\n",
    "        # Verifica che i file esistano\n",
    "        print(f\"ðŸ” Verificando paths...\")\n",
    "        print(f\"  Data path: {data_path}\")\n",
    "        print(f\"  Checkpoint path: {checkpoint_path}\")\n",
    "        print(f\"  Output dir: {output_dir}\")\n",
    "        \n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"Data path not found: {data_path}\")\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Checkpoint path not found: {checkpoint_path}\")\n",
    "        if not os.path.exists(os.path.join(data_path, 'forget.jsonl')):\n",
    "            raise FileNotFoundError(f\"forget.jsonl not found in {data_path}\")\n",
    "        if not os.path.exists(os.path.join(data_path, 'retain.jsonl')):\n",
    "            raise FileNotFoundError(f\"retain.jsonl not found in {data_path}\")\n",
    "\n",
    "        # Normalizza i path (come nello script originale)\n",
    "        from pathlib import Path\n",
    "        if args.output_dir is None:\n",
    "            args.output_dir = os.getcwd()\n",
    "        else:\n",
    "            args.output_dir = args.output_dir.rstrip('/')\n",
    "            Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Lancia direttamente le funzioni\n",
    "        import random, torch, numpy as np\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "\n",
    "        from accelerate import Accelerator\n",
    "        accelerator = Accelerator()\n",
    "\n",
    "        if not args.compute_metrics_only:\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "            from peft import PeftModel, LoraConfig\n",
    "            \n",
    "            print(f\"ðŸ“¥ Loading model from {args.checkpoint_path}...\")\n",
    "            \n",
    "            # Carica il modello PEFT (LoRA) se Ã¨ salvato come tale\n",
    "            try:\n",
    "                # Prima prova a caricare come modello PEFT\n",
    "                base_model_path = MODEL_PATH  # Usa il path del modello base\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    base_model_path, \n",
    "                    local_files_only=True,\n",
    "                    torch_dtype=torch.bfloat16\n",
    "                )\n",
    "                model = PeftModel.from_pretrained(base_model, args.checkpoint_path)\n",
    "                print(\"âœ… Loaded as PEFT model\")\n",
    "            except:\n",
    "                # Se fallisce, prova a caricare come modello normale\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    args.checkpoint_path,\n",
    "                    torch_dtype=torch.bfloat16,\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "                print(\"âœ… Loaded as regular model\")\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_path)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            print(\"ðŸš€ Starting inference...\")\n",
    "            evaluation.inference(args, model, tokenizer)\n",
    "            \n",
    "            if args.mia_data_path is not None:\n",
    "                print(\"ðŸ” Starting MIA attacks...\")\n",
    "                evaluation.mia_attacks(args, model, tokenizer)\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            print(\"ðŸ“Š Computing metrics...\")\n",
    "            evaluation.compute_metrics(args)\n",
    "            print(\"âœ… Evaluation completed!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# === Step 4: Esegui evaluation ===\n",
    "print(\"ðŸŽ¯ Starting evaluation process...\")\n",
    "\n",
    "# Verifica che i file esistano prima di iniziare\n",
    "if os.path.exists(\"validation/forget.jsonl\") and os.path.exists(\"validation/retain.jsonl\"):\n",
    "    if os.path.exists(\"balanced_results/balanced_model/\"):\n",
    "        run_evaluation(\n",
    "            data_path=\"validation/\",  # cartella relativa con forget.jsonl e retain.jsonl\n",
    "            checkpoint_path=\"balanced_results/balanced_model/\",  # cartella relativa con i pesi del modello\n",
    "            output_dir=\"eval_results\",\n",
    "            debug=True  # Attiva debug per vedere cosa succede\n",
    "        )\n",
    "    else:\n",
    "        print(\"âŒ Model checkpoint not found at balanced_results/balanced_model/\")\n",
    "        print(\"   Make sure the training completed successfully\")\n",
    "else:\n",
    "    print(\"âŒ Validation files not found\")\n",
    "    print(\"   Expected: validation/forget.jsonl and validation/retain.jsonl\")\n",
    "    print(\"   Make sure the data processing completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8051727,
     "sourceId": 12737770,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
