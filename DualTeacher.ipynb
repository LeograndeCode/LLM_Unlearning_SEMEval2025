{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearning DualTeacher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T13:13:33.561512Z",
     "iopub.status.busy": "2025-08-28T13:13:33.560958Z",
     "iopub.status.idle": "2025-08-28T13:14:20.335927Z",
     "shell.execute_reply": "2025-08-28T13:14:20.334886Z",
     "shell.execute_reply.started": "2025-08-28T13:13:33.561485Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge-score) (2024.2.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=e47f1035cd16c4addde6d0eb205c42b738eb3b9aa482003d10d8f1276e8fad9a\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 13:14:02.784761: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756386843.124822      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756386843.232129      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs disponibili: 2\n",
      "GPU 0: Tesla T4\n",
      "GPU 1: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Configurazioni\n",
    "MODEL_PATH = \"/kaggle/input/olmo-model/semeval25-unlearning-1B-model\"\n",
    "DATA_PATH = \"/kaggle/input/olmo-model/semeval25-unlearning-data\"\n",
    "MIA_VAL_PATH  = \"/kaggle/input/mia-dataset-val\"\n",
    "MIA_TRAIN_PATH  = \"/kaggle/input/mia-dataset\"\n",
    "GOOD_TEACHER_PATH = \"/kaggle/input/good-teacher\"\n",
    "\n",
    "print(f\"GPUs disponibili: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Caricamento Dati e Modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T13:31:19.364593Z",
     "iopub.status.busy": "2025-08-28T13:31:19.364349Z",
     "iopub.status.idle": "2025-08-28T13:31:21.207024Z",
     "shell.execute_reply": "2025-08-28T13:31:21.206261Z",
     "shell.execute_reply.started": "2025-08-28T13:31:19.364577Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb9fac40a7e48d0a21314bef6175597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f35b3521c714491974b9bc133ed10a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751ac7b7cfad4fb1a07d1dca3b8d9b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset salvati e tokenizer caricato\n"
     ]
    }
   ],
   "source": [
    "# Caricamento dataset\n",
    "retain_train_df = pd.read_parquet(f\"{DATA_PATH}/data/retain_train-00000-of-00001.parquet\", engine='pyarrow')\n",
    "retain_validation_df = pd.read_parquet(f\"{DATA_PATH}/data/retain_validation-00000-of-00001.parquet\", engine='pyarrow')\n",
    "forget_train_df = pd.read_parquet(f\"{DATA_PATH}/data/forget_train-00000-of-00001.parquet\", engine='pyarrow')\n",
    "forget_validation_df = pd.read_parquet(f\"{DATA_PATH}/data/forget_validation-00000-of-00001.parquet\", engine='pyarrow')\n",
    "\n",
    "# Salvataggio in formato JSONL\n",
    "!mkdir -p train validation\n",
    "retain_train_df.to_json('train/retain.jsonl', orient='records', lines=True)\n",
    "forget_train_df.to_json('train/forget.jsonl', orient='records', lines=True)\n",
    "retain_validation_df.to_json('validation/retain.jsonl', orient='records', lines=True)\n",
    "forget_validation_df.to_json('validation/forget.jsonl', orient='records', lines=True)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Dataset salvati e tokenizer caricato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T13:32:17.804870Z",
     "iopub.status.busy": "2025-08-28T13:32:17.804544Z",
     "iopub.status.idle": "2025-08-28T13:32:17.812267Z",
     "shell.execute_reply": "2025-08-28T13:32:17.811675Z",
     "shell.execute_reply.started": "2025-08-28T13:32:17.804847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UnlearningDataset(Dataset):\n",
    "    def __init__(self, data_source, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if isinstance(data_source, pd.DataFrame):\n",
    "            self.data = data_source\n",
    "            print(f\"Caricati {len(self.data)} esempi dal DataFrame\")\n",
    "        elif isinstance(data_source, str):\n",
    "            data_list = []\n",
    "            with open(data_source, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    item = json.loads(line.strip())\n",
    "                    data_list.append(item)\n",
    "            self.data = pd.DataFrame(data_list)\n",
    "            print(f\"Caricati {len(self.data)} esempi da {data_source}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        input_text = item[\"input\"]\n",
    "        output_text = item[\"output\"]\n",
    "\n",
    "        # Tokenizzazione unica\n",
    "        combined = f\"{input_text} {output_text}\"\n",
    "        tokenized = self.tokenizer(\n",
    "            combined,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Tokenizzazione solo dell'input per start_locs\n",
    "        input_ids = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n",
    "            \"start_locs\": input_ids.size(0),  # posizione dove finisce l'input\n",
    "            \"labels\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"split\": 1 if item.get(\"split\", \"retain\") == \"forget\" else 0\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T13:32:20.514472Z",
     "iopub.status.busy": "2025-08-28T13:32:20.514182Z",
     "iopub.status.idle": "2025-08-28T13:32:20.520915Z",
     "shell.execute_reply": "2025-08-28T13:32:20.520061Z",
     "shell.execute_reply.started": "2025-08-28T13:32:20.514449Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricati 2248 esempi dal DataFrame\n",
      "Dataset creato con 2248 esempi\n"
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloader\n",
    "batch_size = 4\n",
    "train_data = pd.concat([retain_train_df, forget_train_df], ignore_index=True)\n",
    "dataset = UnlearningDataset(train_data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Dataset creato con {len(dataset)} esempi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DualTeacher Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T08:38:55.231163Z",
     "iopub.status.busy": "2025-08-28T08:38:55.230887Z",
     "iopub.status.idle": "2025-08-28T08:38:55.255005Z",
     "shell.execute_reply": "2025-08-28T08:38:55.254335Z",
     "shell.execute_reply.started": "2025-08-28T08:38:55.231140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DualTeacherTrainer:\n",
    "    def __init__(self, model_path, tokenizer, teacher_lora_config, student_lora_config, device_map=None):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.teacher_lora_config = teacher_lora_config\n",
    "        self.student_lora_config = student_lora_config\n",
    "        self.device_map = device_map or {\"student\": \"cuda:0\", \"teacher\": \"cuda:1\"}\n",
    "        \n",
    "        self.good_teacher = None\n",
    "        self.student_model = None\n",
    "        self.initial_state_dict = {}\n",
    "        \n",
    "        \n",
    "    def setup_models(self,skip_teacher_setup=False):\n",
    "        \"\"\"Initialize and setup both teacher and student models\"\"\"\n",
    "        print(\"üîß Setting up models...\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(self.model_path, local_files_only=True)\n",
    "        if skip_teacher_setup is False:\n",
    "            # Setup good teacher with LoRA (for training)\n",
    "            \n",
    "            self.good_teacher = get_peft_model(base_model, self.teacher_lora_config, local_files_only=True)\n",
    "            self.good_teacher = self.good_teacher.to(self.device_map[\"teacher\"])\n",
    "            self.good_teacher.print_trainable_parameters()\n",
    "            \n",
    "        # Setup student model with LoRA\n",
    "        self.student_model = get_peft_model(base_model, self.student_lora_config)\n",
    "        self.student_model = self.student_model.to(self.device_map[\"student\"])\n",
    "        self.student_model.print_trainable_parameters()\n",
    "        \n",
    "        # Save initial state for task vector calculation\n",
    "        for name, param in self.student_model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.initial_state_dict[name] = param.data.clone()\n",
    "        \n",
    "        print(\"‚úÖ Models setup completed\")\n",
    "        \n",
    "    \n",
    "    def create_bad_teacher_logits(self, good_teacher_logits: torch.Tensor):\n",
    "        \"\"\"Create more realistic bad teacher logits\"\"\"\n",
    "        # Invece di -logits * 2.0, usa uniform distribution\n",
    "        vocab_size = good_teacher_logits.size(-1)\n",
    "        uniform_logits = torch.ones_like(good_teacher_logits) / vocab_size\n",
    "        return uniform_logits + torch.randn_like(good_teacher_logits) * 0.1\n",
    "\n",
    "        \n",
    "    \n",
    "    def compute_kl_divergence(self, batch):\n",
    "        # Devices\n",
    "        student_device = self.device_map[\"student\"]\n",
    "        teacher_device = self.device_map[\"teacher\"]\n",
    "    \n",
    "        # Inputs\n",
    "        input_ids_student = batch[\"input_ids\"].to(student_device)\n",
    "        attention_mask_student = batch[\"attention_mask\"].to(student_device)\n",
    "        labels_student = batch[\"labels\"].to(student_device)\n",
    "        split = batch[\"split\"].float().to(student_device)\n",
    "    \n",
    "        # Student forward\n",
    "        student_logits = self.student_model(input_ids_student, attention_mask=attention_mask_student).logits\n",
    "        student_log_probs = torch.nn.functional.log_softmax(student_logits, dim=-1).to(teacher_device)\n",
    "    \n",
    "        # Teacher forward (no grad)\n",
    "        input_ids_teacher = batch[\"input_ids\"].to(teacher_device)\n",
    "        attention_mask_teacher = batch[\"attention_mask\"].to(teacher_device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            good_teacher_logits = self.good_teacher(input_ids_teacher, attention_mask=attention_mask_teacher).logits\n",
    "            bad_teacher_logits = self.create_bad_teacher_logits(good_teacher_logits)\n",
    "            good_teacher_probs = torch.nn.functional.softmax(good_teacher_logits, dim=-1)\n",
    "            bad_teacher_probs = torch.nn.functional.softmax(bad_teacher_logits, dim=-1)\n",
    "    \n",
    "        # Masks retain/forget\n",
    "        retain_mask = (split <= 0.5).to(teacher_device)\n",
    "        forget_mask = (split > 0.5).to(teacher_device)\n",
    "    \n",
    "        total_loss = 0.0\n",
    "        if retain_mask.any():\n",
    "            retain_kl = torch.nn.functional.kl_div(\n",
    "                student_log_probs[retain_mask],\n",
    "                good_teacher_probs[retain_mask.bool()],\n",
    "                reduction=\"none\",\n",
    "                log_target=False\n",
    "            ).sum(dim=-1)  # somma su vocab\n",
    "            retain_kl = retain_kl.mean()\n",
    "            total_loss += 1.5 * retain_kl  # retain_weight\n",
    "    \n",
    "        if forget_mask.any():\n",
    "            forget_kl = torch.nn.functional.kl_div(\n",
    "                student_log_probs[forget_mask],\n",
    "                bad_teacher_probs[forget_mask.bool()],\n",
    "                reduction=\"none\",\n",
    "                log_target=False\n",
    "            ).sum(dim=-1)\n",
    "            forget_kl = forget_kl.mean()\n",
    "            total_loss += 5.0 * forget_kl  # forget_weight\n",
    "    \n",
    "        # Entropia student per regolarizzazione\n",
    "        entropy_loss = -(student_log_probs.exp() * student_log_probs).sum(-1).mean()\n",
    "    \n",
    "        return total_loss + 0.2 * entropy_loss\n",
    "\n",
    "    \n",
    "        \n",
    "    def train_good_teacher(self, dataloader, num_epochs=2, lr=1e-4, save_path=\"good_teacher_adapter\"):\n",
    "        \"\"\"Train the good teacher on retain samples using LoRA\"\"\"\n",
    "        print(\"üöÄ Training good teacher with LoRA...\")\n",
    "\n",
    "        self.good_teacher.to(self.device_map[\"teacher\"])\n",
    "        self.good_teacher.train()\n",
    "        optimizer = torch.optim.AdamW(self.good_teacher.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"üìÖ Epoca {epoch + 1}/{num_epochs} - Good Teacher Training\")\n",
    "            \n",
    "            epoch_losses = []\n",
    "            retain_batches_processed = 0\n",
    "            \n",
    "            with tqdm(total=len(dataloader), desc=f\"Good Teacher Epoca {epoch+1}\") as pbar:\n",
    "                for batch in dataloader:\n",
    "                    # Filter retain samples only\n",
    "                    split = batch['split']\n",
    "                    retain_mask = (split == 0)\n",
    "                    \n",
    "                    if not retain_mask.any():\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract retain samples\n",
    "                    input_ids = batch['input_ids'][retain_mask].to(self.device_map[\"teacher\"])\n",
    "                    attention_mask = batch['attention_mask'][retain_mask].to(self.device_map[\"teacher\"])\n",
    "                    labels = batch['labels'][retain_mask].to(self.device_map[\"teacher\"])\n",
    "                    \n",
    "                    if input_ids.size(0) == 0:\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = self.good_teacher(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    epoch_losses.append(loss.item())\n",
    "                    retain_batches_processed += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                    if retain_batches_processed % 100 == 0:\n",
    "                        pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "            if epoch_losses:\n",
    "                avg_loss = np.mean(epoch_losses)\n",
    "                print(f\"üìä Good Teacher Epoca {epoch+1} - Loss medio: {avg_loss:.4f}\")\n",
    "        \n",
    "        print(\"‚úÖ Good teacher training completed\")\n",
    "        \n",
    "        # Save only LoRA adapter\n",
    "        self.save_good_teacher(save_path)\n",
    "        print(f\"üíæ Good teacher adapter salvato in {save_path}\")\n",
    "        \n",
    "        \n",
    "    def train_student(self, dataloader, num_epochs=4, lr=1e-4):\n",
    "        \"\"\"Train student model with dual teacher approach (student uses LoRA, teacher is base model)\"\"\"\n",
    "        print(\"üöÄ Training student with LoRA against base model teacher...\")\n",
    "        \n",
    "        self.student_model.train()\n",
    "        optimizer = torch.optim.AdamW(self.student_model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_losses = []\n",
    "            \n",
    "            with tqdm(total=len(dataloader), desc=f\"Student Epoca {epoch+1}\") as pbar:\n",
    "                for batch in dataloader:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.compute_kl_divergence(batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    epoch_losses.append(loss.item())\n",
    "                    pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "                    pbar.update(1)\n",
    "            \n",
    "            avg_loss = np.mean(epoch_losses)\n",
    "            print(f\"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Save model after each epoch\n",
    "            self.save_model(f\"studentmodel_epoch_{epoch+1}\")\n",
    "        \n",
    "        print(\"‚úÖ Student training completed\")\n",
    "        \n",
    "    def save_model(self, save_path):\n",
    "        \"\"\"Save student model and tokenizer\"\"\"\n",
    "        self.student_model.save_pretrained(save_path)\n",
    "        self.tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "    def save_good_teacher(self, save_path):\n",
    "        \"\"\"Save only the LoRA adapter of the good teacher\"\"\"\n",
    "        self.good_teacher.save_pretrained(save_path)\n",
    "\n",
    "\n",
    "    def load_good_teacher_from_adapter(self, adapter_path):\n",
    "        \"\"\"Reload good teacher from base model + adapter\"\"\"\n",
    "        print(f\"üìÇ Caricamento good teacher da adapter {adapter_path} ...\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(self.model_path, local_files_only=True)\n",
    "        self.good_teacher = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "        self.good_teacher = self.good_teacher.to(self.device_map[\"teacher\"])\n",
    "        \n",
    "        # Freeze params\n",
    "        self.good_teacher.eval()\n",
    "        for param in self.good_teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"‚úÖ Good teacher caricato e congelato\")\n",
    "\n",
    "    def load_teacher(self,GOOD_TEACHER_PATH):\n",
    "        self.good_teacher = AutoModelForCausalLM.from_pretrained(GOOD_TEACHER_PATH)\n",
    "        self.good_teacher.eval()  # congelalo\n",
    "        for param in self.good_teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.good_teacher.to(trainer.device_map[\"teacher\"])\n",
    "        print(\"‚úÖ Good teacher caricato e congelato\")\n",
    "\n",
    "    \n",
    "    def calculate_task_vector(self):\n",
    "        \"\"\"Calculate task vector from initial to final state\"\"\"\n",
    "        task_vector = {}\n",
    "        for name, param in self.student_model.named_parameters():\n",
    "            if param.requires_grad and name in self.initial_state_dict:\n",
    "                task_vector[name] = param.data - self.initial_state_dict[name]\n",
    "        return task_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Trainer and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T08:38:55.255956Z",
     "iopub.status.busy": "2025-08-28T08:38:55.255783Z",
     "iopub.status.idle": "2025-08-28T08:38:55.275788Z",
     "shell.execute_reply": "2025-08-28T08:38:55.275157Z",
     "shell.execute_reply.started": "2025-08-28T08:38:55.255942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_good_teacher = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T08:38:55.276838Z",
     "iopub.status.busy": "2025-08-28T08:38:55.276561Z",
     "iopub.status.idle": "2025-08-28T08:38:55.291973Z",
     "shell.execute_reply": "2025-08-28T08:38:55.291455Z",
     "shell.execute_reply.started": "2025-08-28T08:38:55.276816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configure LoRA for teacher and student\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = DualTeacherTrainer(\n",
    "    model_path=MODEL_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    teacher_lora_config=lora_config,\n",
    "    student_lora_config=lora_config,\n",
    "    device_map={\"student\": \"cuda:0\", \"teacher\": \"cuda:1\"}\n",
    ")\n",
    "# # Setup models\n",
    "\n",
    "if train_good_teacher:\n",
    "    trainer.setup_models()\n",
    "    # Train good teacher (will be converted to base model after training)\n",
    "    trainer.train_good_teacher(dataloader, num_epochs=5, lr=5e-5) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-28T08:41:16.741Z",
     "iopub.execute_input": "2025-08-28T08:38:55.292828Z",
     "iopub.status.busy": "2025-08-28T08:38:55.292588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup dei modelli (student viene comunque inizializzato)\n",
    "trainer.setup_models(skip_teacher_setup=True)\n",
    "\n",
    "trainer.load_good_teacher_from_adapter(GOOD_TEACHER_PATH)\n",
    "# trainer.load_teacher(GOOD_TEACHER_PATH)\n",
    "\n",
    "# Ora puoi allenare direttamente lo student\n",
    "trainer.train_student(dataloader, num_epochs=5, lr=3e-5)\n",
    "trainer.save_model(\"studentmodel_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-28T08:41:16.741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import types\n",
    "import random, torch, numpy as np\n",
    "\n",
    "try:\n",
    "    import evaluation\n",
    "    import importlib\n",
    "    importlib.reload(evaluation)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def run_evaluation(\n",
    "    data_path,\n",
    "    checkpoint_path,\n",
    "    output_dir=\"eval_results\",\n",
    "    mia_data_path=MIA_TRAIN_PATH,\n",
    "    mia_data_val_path=MIA_VAL_PATH,\n",
    "    mmlu_metrics_file_path=None,\n",
    "    max_new_tokens=256,\n",
    "    batch_size=25,\n",
    "    debug=False,\n",
    "    compute_metrics_only=False,\n",
    "    seed=42,\n",
    "    keep_files=False,\n",
    "):\n",
    "    try:\n",
    "        # Costruiamo un oggetto args simile a quello di argparse\n",
    "        args = types.SimpleNamespace(\n",
    "            data_path=data_path,\n",
    "            checkpoint_path=checkpoint_path,\n",
    "            output_dir=output_dir,\n",
    "            mia_data_path=mia_data_path,\n",
    "            mia_data_val_path=mia_data_val_path,\n",
    "            mmlu_metrics_file_path=mmlu_metrics_file_path,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            batch_size=batch_size,\n",
    "            debug=debug,\n",
    "            compute_metrics_only=compute_metrics_only,\n",
    "            seed=seed,\n",
    "            keep_files=keep_files,\n",
    "        )\n",
    "\n",
    "        # Verifica che i file esistano\n",
    "        print(f\"üîç Verificando paths...\")\n",
    "        print(f\"  Data path: {data_path}\")\n",
    "        print(f\"  Checkpoint path: {checkpoint_path}\")\n",
    "        print(f\"  Output dir: {output_dir}\")\n",
    "        \n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"Data path not found: {data_path}\")\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Checkpoint path not found: {checkpoint_path}\")\n",
    "        if not os.path.exists(os.path.join(data_path, 'forget.jsonl')):\n",
    "            raise FileNotFoundError(f\"forget.jsonl not found in {data_path}\")\n",
    "        if not os.path.exists(os.path.join(data_path, 'retain.jsonl')):\n",
    "            raise FileNotFoundError(f\"retain.jsonl not found in {data_path}\")\n",
    "\n",
    "        # Normalizza i path (come nello script originale)\n",
    "        from pathlib import Path\n",
    "        if args.output_dir is None:\n",
    "            args.output_dir = os.getcwd()\n",
    "        else:\n",
    "            args.output_dir = args.output_dir.rstrip('/')\n",
    "            Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Lancia direttamente le funzioni\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "\n",
    "        from accelerate import Accelerator\n",
    "        accelerator = Accelerator()\n",
    "\n",
    "        if not args.compute_metrics_only:\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "            from peft import PeftModel\n",
    "            \n",
    "            print(f\"üì• Loading model from {args.checkpoint_path}...\")\n",
    "            \n",
    "            # Carica il modello PEFT (LoRA) se √® salvato come tale\n",
    "            try:\n",
    "                # Prima prova a caricare come modello PEFT\n",
    "                base_model_path = MODEL_PATH  # Usa il path del modello base\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    base_model_path, \n",
    "                    local_files_only=True,\n",
    "                    torch_dtype=torch.bfloat16\n",
    "                )\n",
    "                model = PeftModel.from_pretrained(base_model, args.checkpoint_path)\n",
    "                print(\"‚úÖ Loaded as PEFT model\")\n",
    "            except:  \n",
    "                # Se fallisce, prova a caricare come modello normale\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    args.checkpoint_path,\n",
    "                    torch_dtype=torch.bfloat16,\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "                print(\"‚úÖ Loaded as regular model\")\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_path)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            print(\"üöÄ Starting inference...\")\n",
    "            evaluation.inference(args, model, tokenizer)\n",
    "            \n",
    "            # if args.mia_data_path is not None:\n",
    "                # print(\"üîç Starting MIA attacks...\")\n",
    "                # evaluation.mia_attacks(args, model, tokenizer)\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            print(\"üìä Computing metrics...\")\n",
    "            evaluation.compute_metrics(args)\n",
    "            print(\"‚úÖ Evaluation completed!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# === Step 4: Esegui evaluation ===\n",
    "print(\"üéØ Starting evaluation process...\")\n",
    "\n",
    "# Verifica che i file esistano prima di iniziare\n",
    "if os.path.exists(\"validation/forget.jsonl\") and os.path.exists(\"validation/retain.jsonl\"):\n",
    "    if os.path.exists(\"studentmodel_final/\"):\n",
    "        run_evaluation(\n",
    "            data_path=\"validation/\",  # cartella relativa con forget.jsonl e retain.jsonl\n",
    "            checkpoint_path=\"studentmodel_final/\",  # cartella relativa con i pesi del modello\n",
    "            output_dir=\"eval_results\",\n",
    "            debug=True  # Attiva debug per vedere cosa succede\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚ùå Model checkpoint not found at balanced_results/balanced_model/\")\n",
    "        print(\"   Make sure the training completed successfully\")\n",
    "else:\n",
    "    print(\"‚ùå Validation files not found\")\n",
    "    print(\"   Expected: validation/forget.jsonl and validation/retain.jsonl\")\n",
    "    print(\"   Make sure the data processing completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-28T13:33:07.380869Z",
     "iopub.status.busy": "2025-08-28T13:33:07.380563Z",
     "iopub.status.idle": "2025-08-28T13:33:08.059003Z",
     "shell.execute_reply": "2025-08-28T13:33:08.058381Z",
     "shell.execute_reply.started": "2025-08-28T13:33:07.380851Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Avviando calcolo metriche...\n",
      "üìÅ Percorso risultati: /kaggle/input/eval-results\n",
      "üìÅ Output directory: final_metrics\n",
      "üìä Iniziando calcolo metriche...\n",
      "üîç Processando forget set...\n",
      "   Trovati 1 file: ['/kaggle/input/eval-results/forget_0.csv']\n",
      "   Caricato /kaggle/input/eval-results/forget_0.csv: 254 righe\n",
      "   Dataset combinato: 254 righe\n",
      "   Calcolando metriche per 254 esempi...\n",
      "   Regurgitation score medio: 0.7811\n",
      "   Knowledge score medio: 0.3526\n",
      "   ‚úÖ forget set completato\n",
      "üîç Processando retain set...\n",
      "   Trovati 1 file: ['/kaggle/input/eval-results/retain_0.csv']\n",
      "   Caricato /kaggle/input/eval-results/retain_0.csv: 278 righe\n",
      "   Dataset combinato: 278 righe\n",
      "   Calcolando metriche per 278 esempi...\n",
      "   Regurgitation score medio: 0.7845\n",
      "   Knowledge score medio: 0.3862\n",
      "   ‚úÖ retain set completato\n",
      "üîç Processando MIA data...\n",
      "/kaggle/input/mia-score/member.jsonl\n",
      "pattern\n",
      "/kaggle/input/mia-score/member.jsonl\n",
      "   Trovati file member: ['/kaggle/input/mia-score/member.jsonl']\n",
      "‚ùå Errore durante il calcolo: name 'student_model' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_36/4153794819.py\", line 275, in <cell line: 0>\n",
      "    results = compute_metrics()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_36/4153794819.py\", line 201, in compute_metrics\n",
      "    nll = compute_nll(text)         # <--- funzione che abbiamo definito prima\n",
      "          ^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_36/4153794819.py\", line 20, in compute_nll\n",
      "    outputs = student_model(**enc, labels=enc[\"input_ids\"])\n",
      "              ^^^^^^^^^^^^^\n",
      "NameError: name 'student_model' is not defined\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import glob\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import shutil\n",
    "# from statistics import mean, harmonic_mean\n",
    "# from rouge_score import rouge_scorer\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Configurazione paths\n",
    "# RESULTS_PATH = \"/kaggle/input/eval-results\"\n",
    "# MIA_TRAIN_PATH = \"/kaggle/input/mia-score/\"\n",
    "# OUTPUT_DIR = \"final_metrics\"\n",
    "\n",
    "# def compute_nll(text: str):\n",
    "#     enc = tokenizer(text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "#     with torch.no_grad():\n",
    "#         outputs = student_model(**enc, labels=enc[\"input_ids\"])\n",
    "#         # loss gi√† √® la NLL media per token\n",
    "#         nll = outputs.loss.item()\n",
    "#     return nll\n",
    "\n",
    "# # === Funzione per caricare JSONL in DataFrame con NLL ===\n",
    "# def load_jsonl_with_nll(path, label):\n",
    "#     records = []\n",
    "#     with open(path, \"r\") as f:\n",
    "#         for line in f:\n",
    "#             obj = json.loads(line)\n",
    "#             text = obj.get(\"document\", \"\")  # <-- puoi cambiare in \"sentence_completion_task\"][\"input\"]\n",
    "#             nll = compute_nll(text)\n",
    "#             records.append({\n",
    "#                 \"id\": obj[\"id\"],\n",
    "#                 \"text\": text,\n",
    "#                 \"nll\": nll,\n",
    "#                 \"label\": label   # 1 = member, 0 = nonmember\n",
    "#             })\n",
    "#     return pd.DataFrame(records)\n",
    "\n",
    "# def compute_auc(member_loss, nonmember_loss):\n",
    "#     \"\"\"Calcola AUC per MIA attack\"\"\"\n",
    "#     assert not np.any(np.isnan(member_loss))\n",
    "#     assert not np.any(np.isnan(nonmember_loss))\n",
    "#     combined_loss = member_loss + nonmember_loss \n",
    "#     combined_loss = -1 * np.array(combined_loss)\n",
    "#     combined_labels = len(member_loss) * [1] + len(nonmember_loss) * [0]\n",
    "#     fp, tp, _ = roc_curve(combined_labels, combined_loss)\n",
    "#     auc_score = float(auc(fp, tp))\n",
    "#     return auc_score\n",
    "\n",
    "# def compute_metrics():\n",
    "#     \"\"\"Calcola le metriche dai file CSV esistenti\"\"\"\n",
    "#     print(\"üìä Iniziando calcolo metriche...\")\n",
    "    \n",
    "#     # Crea directory output\n",
    "#     Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "#     results = {}\n",
    "#     aggregate_scores_list = []\n",
    "    \n",
    "#     # Processa forget e retain sets\n",
    "#     for split in ['forget', 'retain']:\n",
    "#         print(f\"üîç Processando {split} set...\")\n",
    "        \n",
    "#         # Cerca file CSV per questo split\n",
    "#         pattern = f\"{RESULTS_PATH}/{split}_*.csv\"\n",
    "#         files = glob.glob(pattern)\n",
    "        \n",
    "#         if len(files) == 0:\n",
    "#             print(f\"‚ùå Nessun file trovato per pattern: {pattern}\")\n",
    "#             continue\n",
    "        \n",
    "#         print(f\"   Trovati {len(files)} file: {files}\")\n",
    "        \n",
    "#         # Carica e concatena tutti i CSV\n",
    "#         df_list = []\n",
    "#         for f in files:\n",
    "#             try:\n",
    "#                 df_temp = pd.read_csv(f)\n",
    "#                 df_list.append(df_temp)\n",
    "#                 print(f\"   Caricato {f}: {len(df_temp)} righe\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"   ‚ùå Errore caricando {f}: {e}\")\n",
    "        \n",
    "#         if not df_list:\n",
    "#             print(f\"‚ùå Nessun file valido per {split}\")\n",
    "#             continue\n",
    "            \n",
    "#         df = pd.concat(df_list, ignore_index=True)\n",
    "#         print(f\"   Dataset combinato: {len(df)} righe\")\n",
    "        \n",
    "#         # Inizializza colonne per le metriche\n",
    "#         df['regurgitation-score-rouge-1'] = None\n",
    "#         df['regurgitation-score'] = None\n",
    "#         df['knowledge-score'] = None\n",
    "        \n",
    "#         ground_truths = df['expected_output'].tolist()\n",
    "#         gen_outputs = df['model_output'].tolist()\n",
    "        \n",
    "#         print(f\"   Calcolando metriche per {len(ground_truths)} esempi...\")\n",
    "        \n",
    "#         # Calcola metriche per ogni esempio\n",
    "#         for i, (gen, gt) in enumerate(zip(gen_outputs, ground_truths)):\n",
    "#             if pd.isna(df.loc[i, 'id']):\n",
    "#                 continue\n",
    "                \n",
    "#             example_id = str(df.loc[i, 'id'])\n",
    "            \n",
    "#             # Task di regurgitation (string completion)\n",
    "#             if example_id.endswith('sc') or 'sc' in example_id:\n",
    "#                 try:\n",
    "#                     rouge_scores = scorer.score(str(gt), str(gen))\n",
    "#                     df.loc[i, 'regurgitation-score-rouge-1'] = rouge_scores['rouge1'].recall\n",
    "#                     df.loc[i, 'regurgitation-score'] = rouge_scores['rougeL'].recall\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"   ‚ö†Ô∏è Errore ROUGE per esempio {i}: {e}\")\n",
    "                    \n",
    "#             # Task di knowledge (question answering)  \n",
    "#             elif example_id.endswith('qa') or 'qa' in example_id:\n",
    "#                 try:\n",
    "#                     is_correct = int(str(gt).strip().lower() == str(gen).strip().lower())\n",
    "#                     df.loc[i, 'knowledge-score'] = is_correct\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"   ‚ö†Ô∏è Errore QA per esempio {i}: {e}\")\n",
    "        \n",
    "#         # Converti None in NaN per il calcolo\n",
    "#         df['regurgitation-score'] = pd.to_numeric(df['regurgitation-score'], errors='coerce')\n",
    "#         df['knowledge-score'] = pd.to_numeric(df['knowledge-score'], errors='coerce')\n",
    "        \n",
    "#         # Calcola metriche aggregate per questo split\n",
    "#         regurg_mean = np.nanmean(df['regurgitation-score'])\n",
    "#         knowledge_mean = np.nanmean(df['knowledge-score'])\n",
    "        \n",
    "#         print(f\"   Regurgitation score medio: {regurg_mean:.4f}\")\n",
    "#         print(f\"   Knowledge score medio: {knowledge_mean:.4f}\")\n",
    "        \n",
    "#         results[split+'-set'] = {\n",
    "#             'overall-regurgitation-score': regurg_mean, \n",
    "#             'overall-knowledge-score': knowledge_mean\n",
    "#         }\n",
    "        \n",
    "#         # Metriche per task (con gestione NaN)\n",
    "#         task_groups = df.groupby('task')[['regurgitation-score', 'knowledge-score']]\n",
    "#         split_aggregate_scores_dict = {}\n",
    "        \n",
    "#         for task, group in task_groups:\n",
    "#             task_scores = {}\n",
    "#             regurg_vals = pd.to_numeric(group['regurgitation-score'], errors='coerce')\n",
    "#             knowledge_vals = pd.to_numeric(group['knowledge-score'], errors='coerce')\n",
    "            \n",
    "#             if not regurg_vals.isna().all():\n",
    "#                 task_scores['regurgitation-score'] = np.nanmean(regurg_vals)\n",
    "#             if not knowledge_vals.isna().all():\n",
    "#                 task_scores['knowledge-score'] = np.nanmean(knowledge_vals)\n",
    "                \n",
    "#             split_aggregate_scores_dict[task] = task_scores\n",
    "        \n",
    "#         results[split+'-set'].update(split_aggregate_scores_dict)\n",
    "        \n",
    "#         # Aggrega scores per la metrica finale\n",
    "#         split_aggregate_score_values = [float(val) for inner in split_aggregate_scores_dict.values() for val in inner.values() if not np.isnan(val)]\n",
    "        \n",
    "#         # Per forget set, inverti i valori (vogliamo \"dimenticare\")\n",
    "#         if split == 'forget':\n",
    "#             split_aggregate_score_values = [(1 - val) for val in split_aggregate_score_values]\n",
    "        \n",
    "#         aggregate_scores_list.extend(split_aggregate_score_values)\n",
    "        \n",
    "#         print(f\"   ‚úÖ {split} set completato\")\n",
    "    \n",
    "#     # Processa MIA se disponibile\n",
    "#     print(\"üîç Processando MIA data...\")\n",
    "#     mia_files_found = False\n",
    "    \n",
    "#     for dataset in ['member', 'nonmember']:\n",
    "#         pattern = f\"{MIA_TRAIN_PATH}{dataset}.jsonl\"\n",
    "#         print(pattern)\n",
    "#         files = glob.glob(pattern)\n",
    "        \n",
    "#         if files:\n",
    "#             mia_files_found = True\n",
    "#             break\n",
    "    \n",
    "#     if mia_files_found:\n",
    "#         mia_results = {}\n",
    "#         for dataset in ['member', 'nonmember']:\n",
    "#             pattern = f\"{MIA_TRAIN_PATH}{dataset}.jsonl\"\n",
    "#             print(\"pattern\")\n",
    "#             print(pattern)\n",
    "#             files = glob.glob(pattern)\n",
    "            \n",
    "#             if files:\n",
    "#                 print(f\"   Trovati file {dataset}: {files}\")\n",
    "#                 records = []\n",
    "#                 for f in files:\n",
    "#                     df_raw = pd.read_json(f, lines=True)\n",
    "#                     for _, row in df_raw.iterrows():\n",
    "#                         text = row.get(\"document\", \"\")  # oppure sentence_completion_task[\"input\"]\n",
    "#                         nll = compute_nll(text)         # <--- funzione che abbiamo definito prima\n",
    "#                         records.append(nll)\n",
    "#                 mia_results[dataset] = records\n",
    "#                 print(f\"   {dataset}: {len(mia_results[dataset])} esempi\")\n",
    "    \n",
    "#         if 'member' in mia_results and 'nonmember' in mia_results:\n",
    "#             auc = compute_auc(mia_results['member'], mia_results['nonmember'])\n",
    "#             results['mia_loss_acc'] = auc\n",
    "#             print(f\"   ‚úÖ MIA AUC: {auc:.4f}\")\n",
    "#         else:\n",
    "#             print(\"   ‚ùå Dati MIA incompleti\")\n",
    "#     else:\n",
    "#         print(\"   ‚ùå Nessun file MIA trovato\")\n",
    "    \n",
    "        \n",
    "#         # Calcola metriche finali\n",
    "#     results['aggregated-terms'] = aggregate_scores_list\n",
    "    \n",
    "#     if aggregate_scores_list:\n",
    "#         task_aggregate = harmonic_mean(aggregate_scores_list)\n",
    "#         results['harmonic-mean-task-aggregate'] = task_aggregate\n",
    "#         print(f\"üìà Task aggregate (harmonic mean): {task_aggregate:.4f}\")\n",
    "#     else:\n",
    "#         task_aggregate = -1\n",
    "#         results['harmonic-mean-task-aggregate'] = -1\n",
    "#         print(\"‚ùå Impossibile calcolare task aggregate\")\n",
    "    \n",
    "#     results['aggregate-score'] = -1\n",
    "    \n",
    "#     # Calcola score finale se abbiamo MIA\n",
    "#     if 'mia_loss_acc' in results and task_aggregate > 0:\n",
    "#         mia_final_score = 1 - abs(results['mia_loss_acc'] - 0.5) * 2\n",
    "#         results['mia_final_score'] = mia_final_score\n",
    "#         results['aggregate-score'] = mean([task_aggregate, mia_final_score])\n",
    "#         print(f\"üéØ Final aggregate score: {results['aggregate-score']:.4f}\")\n",
    "    \n",
    "#     # Salva risultati\n",
    "#     metrics_file = os.path.join(OUTPUT_DIR, 'evaluation_results.json')\n",
    "#     with open(metrics_file, 'w') as outptr:\n",
    "#         json.dump(results, outptr, indent=2)\n",
    "    \n",
    "#     print(f\"üíæ Risultati salvati in: {metrics_file}\")\n",
    "    \n",
    "#     # Stampa riepilogo\n",
    "#     print(\"\\nüìã RIEPILOGO RISULTATI:\")\n",
    "#     print(\"=\" * 50)\n",
    "    \n",
    "#     for key, value in results.items():\n",
    "#         if isinstance(value, dict):\n",
    "#             print(f\"{key}:\")\n",
    "#             for sub_key, sub_value in value.items():\n",
    "#                 if isinstance(sub_value, float):\n",
    "#                     print(f\"  {sub_key}: {sub_value:.4f}\")\n",
    "#                 else:\n",
    "#                     print(f\"  {sub_key}: {sub_value}\")\n",
    "#         elif isinstance(value, (int, float)) and key != 'aggregated-terms':\n",
    "#             print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "#     return results\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"üöÄ Avviando calcolo metriche...\")\n",
    "#     print(f\"üìÅ Percorso risultati: {RESULTS_PATH}\")\n",
    "#     print(f\"üìÅ Output directory: {OUTPUT_DIR}\")\n",
    "    \n",
    "#     # Verifica che esistano i file necessari\n",
    "#     if not os.path.exists(RESULTS_PATH):\n",
    "#         print(f\"‚ùå Path risultati non trovato: {RESULTS_PATH}\")\n",
    "#         exit(1)\n",
    "    \n",
    "    \n",
    "#     # Esegui calcolo\n",
    "#     try:\n",
    "#         results = compute_metrics()\n",
    "#         print(\"‚úÖ Calcolo metriche completato con successo!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Errore durante il calcolo: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8051727,
     "sourceId": 12737770,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8153355,
     "sourceId": 12886974,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8155392,
     "sourceId": 12890024,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8155743,
     "sourceId": 12890625,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8159200,
     "sourceId": 12895681,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
