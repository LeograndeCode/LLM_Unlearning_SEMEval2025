{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearning DualTeacher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T08:57:00.292860Z",
     "iopub.status.busy": "2025-08-13T08:57:00.292585Z",
     "iopub.status.idle": "2025-08-13T08:57:46.074046Z",
     "shell.execute_reply": "2025-08-13T08:57:46.073234Z",
     "shell.execute_reply.started": "2025-08-13T08:57:00.292838Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge-score) (2024.2.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=b7686c2acd68e692f06be82969fee75fa84358da33f591aed52b2901ed8b07ec\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 08:57:29.114998: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755075449.500673      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755075449.609497      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs disponibili: 2\n",
      "GPU 0: Tesla T4\n",
      "GPU 1: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Configurazioni\n",
    "MODEL_PATH = \"/kaggle/input/olmo-model/semeval25-unlearning-1B-model\"\n",
    "DATA_PATH = \"/kaggle/input/olmo-model/semeval25-unlearning-data\"\n",
    "\n",
    "print(f\"GPUs disponibili: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Caricamento Dati e Modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T08:57:46.075945Z",
     "iopub.status.busy": "2025-08-13T08:57:46.075474Z",
     "iopub.status.idle": "2025-08-13T08:57:48.343240Z",
     "shell.execute_reply": "2025-08-13T08:57:48.342277Z",
     "shell.execute_reply.started": "2025-08-13T08:57:46.075924Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc2ad348a3d47e79b0339f354298c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fc2350ae8345b4b06dd9aaf876547a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87ebebbc93e497abcf94be9f905eef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset salvati e tokenizer caricato\n"
     ]
    }
   ],
   "source": [
    "# Caricamento dataset\n",
    "retain_train_df = pd.read_parquet(f\"{DATA_PATH}/data/retain_train-00000-of-00001.parquet\", engine='pyarrow')\n",
    "retain_validation_df = pd.read_parquet(f\"{DATA_PATH}/data/retain_validation-00000-of-00001.parquet\", engine='pyarrow')\n",
    "forget_train_df = pd.read_parquet(f\"{DATA_PATH}/data/forget_train-00000-of-00001.parquet\", engine='pyarrow')\n",
    "forget_validation_df = pd.read_parquet(f\"{DATA_PATH}/data/forget_validation-00000-of-00001.parquet\", engine='pyarrow')\n",
    "\n",
    "# Salvataggio in formato JSONL\n",
    "!mkdir -p train validation\n",
    "retain_train_df.to_json('train/retain.jsonl', orient='records', lines=True)\n",
    "forget_train_df.to_json('train/forget.jsonl', orient='records', lines=True)\n",
    "retain_validation_df.to_json('validation/retain.jsonl', orient='records', lines=True)\n",
    "forget_validation_df.to_json('validation/forget.jsonl', orient='records', lines=True)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Dataset salvati e tokenizer caricato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configurazione Modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T08:57:48.344619Z",
     "iopub.status.busy": "2025-08-13T08:57:48.344310Z",
     "iopub.status.idle": "2025-08-13T08:58:57.130752Z",
     "shell.execute_reply": "2025-08-13T08:58:57.130086Z",
     "shell.execute_reply.started": "2025-08-13T08:57:48.344583Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6949a6842670498897f4ec68b5d97f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 1,283,981,312 || trainable%: 0.3267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8a66f53ef04249bb8ca50e18f80464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelli configurati su GPU separate\n"
     ]
    }
   ],
   "source": [
    "# Configurazione LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "# Modello studente (GPU 0)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "student_model = get_peft_model(base_model, lora_config)\n",
    "student_model = student_model.to(\"cuda:0\")\n",
    "student_model.print_trainable_parameters()\n",
    "\n",
    "# Insegnante competente (GPU 1)\n",
    "good_teacher = AutoModelForCausalLM.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "good_teacher = good_teacher.to(\"cuda:1\")\n",
    "for param in good_teacher.parameters():\n",
    "    param.requires_grad = False\n",
    "good_teacher.eval()\n",
    "\n",
    "print(\"Modelli configurati su GPU separate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bad Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T08:58:57.132384Z",
     "iopub.status.busy": "2025-08-13T08:58:57.132169Z",
     "iopub.status.idle": "2025-08-13T08:58:57.139097Z",
     "shell.execute_reply": "2025-08-13T08:58:57.138243Z",
     "shell.execute_reply.started": "2025-08-13T08:58:57.132367Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insegnante incompetente creato con vocab_size=50304\n"
     ]
    }
   ],
   "source": [
    "class badTeacher(nn.Module):\n",
    "    \"\"\"Insegnante che genera distribuzioni casuali per l'unlearning\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, device=\"cuda:1\"):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        # Distribuzione uniforme (massima entropia = massima confusione)\n",
    "        logits = torch.zeros(batch_size, seq_len, self.vocab_size, device=self.device)\n",
    "        # Piccolo rumore per evitare degenerazione\n",
    "        logits += torch.randn_like(logits) * 0.01\n",
    "        \n",
    "        from types import SimpleNamespace\n",
    "        return SimpleNamespace(logits=logits)\n",
    "\n",
    "# Creare insegnante incompetente\n",
    "vocab_size = student_model.config.vocab_size\n",
    "bad_teacher = badTeacher(vocab_size, device=\"cuda:1\")\n",
    "bad_teacher.eval()\n",
    "\n",
    "print(f\"Insegnante incompetente creato con vocab_size={vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset e DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T08:58:57.140028Z",
     "iopub.status.busy": "2025-08-13T08:58:57.139808Z",
     "iopub.status.idle": "2025-08-13T08:58:57.171474Z",
     "shell.execute_reply": "2025-08-13T08:58:57.170900Z",
     "shell.execute_reply.started": "2025-08-13T08:58:57.140012Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricati 1136 esempi da train/retain.jsonl\n",
      "Caricati 1112 esempi da train/forget.jsonl\n",
      "DataLoader creati: retain=568 batch, forget=556 batch\n"
     ]
    }
   ],
   "source": [
    "class UnlearningDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, tokenizer, max_length=256):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line.strip())\n",
    "                self.data.append(item)\n",
    "        \n",
    "        print(f\"Caricati {len(self.data)} esempi da {jsonl_path}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Estrarre testo\n",
    "        if 'text' in item:\n",
    "            text = item['text']\n",
    "        else:\n",
    "            text = str(list(item.values())[0])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': encoding['input_ids'].squeeze()\n",
    "        }\n",
    "\n",
    "# Creare dataset e dataloader\n",
    "retain_dataset = UnlearningDataset('train/retain.jsonl', tokenizer)\n",
    "forget_dataset = UnlearningDataset('train/forget.jsonl', tokenizer)\n",
    "\n",
    "batch_size = 2\n",
    "retain_loader = DataLoader(retain_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "forget_loader = DataLoader(forget_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "print(f\"DataLoader creati: retain={len(retain_loader)} batch, forget={len(forget_loader)} batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T08:58:57.172508Z",
     "iopub.status.busy": "2025-08-13T08:58:57.172277Z",
     "iopub.status.idle": "2025-08-13T08:58:57.183163Z",
     "shell.execute_reply": "2025-08-13T08:58:57.182440Z",
     "shell.execute_reply.started": "2025-08-13T08:58:57.172482Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricati 278 esempi da validation/retain.jsonl\n",
      "Caricati 254 esempi da validation/forget.jsonl\n",
      "DataLoader val creati: retain=139 batch, forget=127 batch\n"
     ]
    }
   ],
   "source": [
    "# Creare dataset e dataloader per validation dataset\n",
    "val_retain_dataset = UnlearningDataset('validation/retain.jsonl', tokenizer)\n",
    "val_forget_dataset = UnlearningDataset('validation/forget.jsonl', tokenizer)\n",
    "\n",
    "batch_size = 2\n",
    "val_retain_loader = DataLoader(val_retain_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_forget_loader = DataLoader(val_forget_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "print(f\"DataLoader val creati: retain={len(val_retain_loader)} batch, forget={len(val_forget_loader)} batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. KL divergence con batch combinato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T08:58:57.184678Z",
     "iopub.status.busy": "2025-08-13T08:58:57.183889Z",
     "iopub.status.idle": "2025-08-13T08:58:57.204742Z",
     "shell.execute_reply": "2025-08-13T08:58:57.204232Z",
     "shell.execute_reply.started": "2025-08-13T08:58:57.184653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_combined_batch(retain_batch, forget_batch):\n",
    "    \"\"\"Combina batch retain e forget con etichette split\"\"\"\n",
    "    \n",
    "    # Retain (split = 0)\n",
    "    retain_data = {\n",
    "        'input_ids': retain_batch['input_ids'],\n",
    "        'attention_mask': retain_batch['attention_mask'],\n",
    "        'labels': retain_batch['labels'],\n",
    "        'split': torch.zeros(retain_batch['input_ids'].size(0))\n",
    "    }\n",
    "    \n",
    "    # Forget (split = 1)\n",
    "    forget_data = {\n",
    "        'input_ids': forget_batch['input_ids'],\n",
    "        'attention_mask': forget_batch['attention_mask'],\n",
    "        'labels': forget_batch['labels'],\n",
    "        'split': torch.ones(forget_batch['input_ids'].size(0))\n",
    "    }\n",
    "    \n",
    "    # Combinare\n",
    "    combined_batch = {\n",
    "        'input_ids': torch.cat([retain_data['input_ids'], forget_data['input_ids']], dim=0),\n",
    "        'attention_mask': torch.cat([retain_data['attention_mask'], forget_data['attention_mask']], dim=0),\n",
    "        'labels': torch.cat([retain_data['labels'], forget_data['labels']], dim=0),\n",
    "        'split': torch.cat([retain_data['split'], forget_data['split']], dim=0)\n",
    "    }\n",
    "    \n",
    "    return combined_batch\n",
    "\n",
    "def compute_balanced_kl_loss(student_model, good_teacher, bad_teacher, batch):\n",
    "      \"\"\"Loss bilanciato: out_teacher = (1-split)*good + split*bad\"\"\"\n",
    "\n",
    "      input_ids = batch['input_ids'].to('cuda:0')\n",
    "      attention_mask = batch['attention_mask'].to('cuda:0')\n",
    "      split = batch['split'].to('cuda:0')\n",
    "\n",
    "      # Forward pass studente\n",
    "      student_outputs = student_model(input_ids=input_ids, attention_mask=attention_mask)     \n",
    "      student_logits = student_outputs.logits\n",
    "\n",
    "      # Forward pass insegnanti\n",
    "      with torch.no_grad():\n",
    "          input_ids_gpu1 = input_ids.to('cuda:1')\n",
    "          attention_mask_gpu1 = attention_mask.to('cuda:1')\n",
    "\n",
    "          good_outputs = good_teacher(input_ids=input_ids_gpu1, attention_mask=attention_mask_gpu1)\n",
    "          good_logits = good_outputs.logits.to('cuda:0')\n",
    "\n",
    "          bad_outputs = bad_teacher(input_ids=input_ids_gpu1,attention_mask=attention_mask_gpu1)\n",
    "          bad_logits = bad_outputs.logits.to('cuda:0')\n",
    "\n",
    "      # CORREZIONE: Broadcasting corretto per split\n",
    "      split_expanded = split.unsqueeze(-1).unsqueeze(-1)\n",
    "      combined_teacher_logits = (1 - split_expanded) * good_logits + split_expanded * bad_logits\n",
    "\n",
    "      # KL divergence\n",
    "      student_probs = F.log_softmax(student_logits, dim=-1)\n",
    "      teacher_probs = F.softmax(combined_teacher_logits, dim=-1)\n",
    "      kl_loss = F.kl_div(student_probs, teacher_probs, reduction='batchmean')\n",
    "\n",
    "      return kl_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop Bilanciato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T08:58:57.206208Z",
     "iopub.status.busy": "2025-08-13T08:58:57.205508Z",
     "iopub.status.idle": "2025-08-13T08:59:00.196119Z",
     "shell.execute_reply": "2025-08-13T08:59:00.195295Z",
     "shell.execute_reply.started": "2025-08-13T08:58:57.206170Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_balanced_unlearning_with_validation(\n",
    "    student_model, good_teacher, bad_teacher,\n",
    "    retain_loader, forget_loader, val_retain_loader, val_forget_loader,\n",
    "    optimizer, tokenizer, num_epochs=3, device=\"cuda:0\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Loop di training unificato:\n",
    "    - Batch bilanciati retain/forget\n",
    "    - Loss smussata (probabilità combinate)\n",
    "    - Validazione ad ogni epoca\n",
    "    - Salvataggio modello e tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    student_model.to(device)\n",
    "    student_model.train()\n",
    "    \n",
    "    good_teacher.eval()\n",
    "    bad_teacher.eval()\n",
    "    \n",
    "    print(\"🚀 TRAINING BILANCIATO con VALIDAZIONE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n📅 EPOCA {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        epoch_train_losses = []\n",
    "        \n",
    "        # Iteratori per retain e forget\n",
    "        retain_iter = iter(retain_loader)\n",
    "        forget_iter = iter(forget_loader)\n",
    "        max_steps = max(len(retain_loader), len(forget_loader))\n",
    "        \n",
    "        # --- Training ---\n",
    "        with tqdm(total=max_steps, desc=f\"Epoca {epoch+1} (Train)\") as pbar:\n",
    "            for _ in range(max_steps):\n",
    "                try:\n",
    "                    retain_batch = next(retain_iter)\n",
    "                except StopIteration:\n",
    "                    retain_iter = iter(retain_loader)\n",
    "                    retain_batch = next(retain_iter)\n",
    "                \n",
    "                try:\n",
    "                    forget_batch = next(forget_iter)\n",
    "                except StopIteration:\n",
    "                    forget_iter = iter(forget_loader)\n",
    "                    forget_batch = next(forget_iter)\n",
    "                \n",
    "                # Batch combinato\n",
    "                combined_batch = create_combined_batch(retain_batch, forget_batch)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss = compute_balanced_kl_loss(\n",
    "                    student_model, good_teacher, bad_teacher, combined_batch\n",
    "                )\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # print({\"Train Loss\": loss.item()})\n",
    "                epoch_train_losses.append(loss.item())\n",
    "                pbar.update(1)\n",
    "        \n",
    "        avg_train_loss = np.mean(epoch_train_losses)\n",
    "        \n",
    "        # --- Validazione ---\n",
    "        val_losses = []\n",
    "        val_retain_iter = iter(val_retain_loader)\n",
    "        val_forget_iter = iter(val_forget_loader)\n",
    "        max_val_steps = max(len(val_retain_loader), len(val_forget_loader))\n",
    "        \n",
    "        student_model.eval()\n",
    "        with torch.no_grad():\n",
    "            with tqdm(total=max_val_steps, desc=f\"Epoca {epoch+1} (Val)\") as pbar:\n",
    "                for _ in range(max_val_steps):\n",
    "                    try:\n",
    "                        retain_batch = next(val_retain_iter)\n",
    "                    except StopIteration:\n",
    "                        val_retain_iter = iter(val_retain_loader)\n",
    "                        retain_batch = next(val_retain_iter)\n",
    "                    \n",
    "                    try:\n",
    "                        forget_batch = next(val_forget_iter)\n",
    "                    except StopIteration:\n",
    "                        val_forget_iter = iter(val_forget_loader)\n",
    "                        forget_batch = next(val_forget_iter)\n",
    "                    \n",
    "                    combined_batch = create_combined_batch(retain_batch, forget_batch)\n",
    "                    val_loss = compute_balanced_kl_loss(\n",
    "                        student_model, good_teacher, bad_teacher, combined_batch\n",
    "                    )\n",
    "                    # print({\"Val Loss\": val_loss.item()})\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        print(f\"📊 Epoca {epoch+1} - Train Loss medio: {avg_train_loss:.4f} | Val Loss medio: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # --- Salvataggio ---\n",
    "        save_path = f\"studentmodel_epoch_{epoch+1}\"\n",
    "        student_model.save_pretrained(save_path)\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "        student_model.train()\n",
    "    \n",
    "    print(\"\\n✅ TRAINING COMPLETATO!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Configurazione Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T08:59:00.197289Z",
     "iopub.status.busy": "2025-08-13T08:59:00.197038Z",
     "iopub.status.idle": "2025-08-13T08:59:00.211878Z",
     "shell.execute_reply": "2025-08-13T08:59:00.211329Z",
     "shell.execute_reply.started": "2025-08-13T08:59:00.197271Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurazione completata:\n",
      "- Epoche: 4\n",
      "- Learning rate: 0.0001\n",
      "- Parametri LoRA salvati: 128\n"
     ]
    }
   ],
   "source": [
    "# Parametri training\n",
    "num_epochs = 4\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(student_model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "# Salvare stato iniziale per task vector\n",
    "initial_state_dict = {}\n",
    "for name, param in student_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        initial_state_dict[name] = param.data.clone()\n",
    "\n",
    "print(f\"Configurazione completata:\")\n",
    "print(f\"- Epoche: {num_epochs}\")\n",
    "print(f\"- Learning rate: {learning_rate}\")\n",
    "print(f\"- Parametri LoRA salvati: {len(initial_state_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Esecuzione Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T08:59:00.213869Z",
     "iopub.status.busy": "2025-08-13T08:59:00.213365Z",
     "iopub.status.idle": "2025-08-13T10:31:54.945593Z",
     "shell.execute_reply": "2025-08-13T10:31:54.944763Z",
     "shell.execute_reply.started": "2025-08-13T08:59:00.213842Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 TRAINING BILANCIATO con VALIDAZIONE\n",
      "======================================================================\n",
      "\n",
      "📅 EPOCA 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 1 (Train): 100%|██████████| 568/568 [19:54<00:00,  2.10s/it]\n",
      "Epoca 1 (Val): 100%|██████████| 139/139 [03:11<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoca 1 - Train Loss medio: 225.9901 | Val Loss medio: 351.0510\n",
      "\n",
      "📅 EPOCA 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 2 (Train): 100%|██████████| 568/568 [20:04<00:00,  2.12s/it]\n",
      "Epoca 2 (Val): 100%|██████████| 139/139 [03:11<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoca 2 - Train Loss medio: 118.3526 | Val Loss medio: 488.7881\n",
      "\n",
      "📅 EPOCA 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 3 (Train): 100%|██████████| 568/568 [20:04<00:00,  2.12s/it]\n",
      "Epoca 3 (Val): 100%|██████████| 139/139 [03:11<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoca 3 - Train Loss medio: 82.7263 | Val Loss medio: 551.1994\n",
      "\n",
      "📅 EPOCA 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoca 4 (Train): 100%|██████████| 568/568 [20:04<00:00,  2.12s/it]\n",
      "Epoca 4 (Val): 100%|██████████| 139/139 [03:11<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoca 4 - Train Loss medio: 58.3913 | Val Loss medio: 531.3211\n",
      "\n",
      "✅ TRAINING COMPLETATO!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Eseguire training bilanciato\n",
    "training_metrics = train_balanced_unlearning_with_validation(\n",
    "    student_model, good_teacher, bad_teacher,\n",
    "    retain_loader, forget_loader, val_retain_loader, val_forget_loader,\n",
    "    optimizer, tokenizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Salvataggio Risultati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T10:32:24.320441Z",
     "iopub.status.busy": "2025-08-13T10:32:24.320147Z",
     "iopub.status.idle": "2025-08-13T10:32:24.453594Z",
     "shell.execute_reply": "2025-08-13T10:32:24.452847Z",
     "shell.execute_reply.started": "2025-08-13T10:32:24.320419Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Risultati salvati in balanced_results/\n",
      "- balanced_model/: Modello con unlearning bilanciato\n",
      "- task_vector.pt: Task vector per applicazioni future\n",
      "- training_metrics.pt: Metriche di training\n"
     ]
    }
   ],
   "source": [
    "# Creare directory\n",
    "os.makedirs('balanced_results', exist_ok=True)\n",
    "\n",
    "# Salvare modello\n",
    "student_model.save_pretrained('balanced_results/balanced_model')\n",
    "\n",
    "# Calcolare task vector\n",
    "task_vector = {}\n",
    "for name, param in student_model.named_parameters():\n",
    "    if param.requires_grad and name in initial_state_dict:\n",
    "        task_vector[name] = param.data - initial_state_dict[name]\n",
    "\n",
    "# Salvare task vector e metriche\n",
    "torch.save(task_vector, 'balanced_results/task_vector.pt')\n",
    "torch.save(training_metrics, 'balanced_results/training_metrics.pt')\n",
    "\n",
    "print(\"✅ Risultati salvati in balanced_results/\")\n",
    "print(\"- balanced_model/: Modello con unlearning bilanciato\")\n",
    "print(\"- task_vector.pt: Task vector per applicazioni future\")\n",
    "print(\"- training_metrics.pt: Metriche di training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Valutazione Veloce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-13T08:53:27.372Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def quick_evaluation(model, tokenizer, retain_path, forget_path, max_examples=30):\n",
    "    \"\"\"Valutazione veloce per confronto\"\"\"\n",
    "    \n",
    "    def eval_perplexity(path, label):\n",
    "        perplexities = []\n",
    "        with open(path, \"r\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= max_examples:\n",
    "                    break\n",
    "                try:\n",
    "                    item = json.loads(line)\n",
    "                    text = str(list(item.values())[0])\n",
    "                    \n",
    "                    encoding = tokenizer(text, return_tensors=\"pt\", max_length=256, truncation=True)\n",
    "                    encoding = {k: v.to(model.device) for k, v in encoding.items()}\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**encoding, labels=encoding['input_ids'])\n",
    "                        if outputs.loss is not None:\n",
    "                            perplexities.append(torch.exp(outputs.loss).item())\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        avg_ppl = np.mean(perplexities) if perplexities else float('inf')\n",
    "        print(f\"  {label}: {avg_ppl:.2f} (su {len(perplexities)} esempi)\")\n",
    "        return avg_ppl\n",
    "    \n",
    "    print(f\"📊 Valutazione veloce (max {max_examples} esempi):\")\n",
    "    retain_ppl = eval_perplexity(retain_path, \"Retain\")\n",
    "    forget_ppl = eval_perplexity(forget_path, \"Forget\")\n",
    "    \n",
    "    ratio = forget_ppl / retain_ppl if retain_ppl != float('inf') else float('inf')\n",
    "    print(f\"  Unlearning Ratio: {ratio:.2f} (>1 = buono)\")\n",
    "    \n",
    "    return {\"retain_ppl\": retain_ppl, \"forget_ppl\": forget_ppl, \"ratio\": ratio}\n",
    "\n",
    "# Valutare modello finale\n",
    "student_model.eval()\n",
    "results = quick_evaluation(student_model, tokenizer, \n",
    "                          \"validation/retain.jsonl\", \"validation/forget.jsonl\")\n",
    "\n",
    "print(\"\\n RISULTATO: Retain basso + Forget alto = Unlearning efficace!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-13T10:32:31.188730Z",
     "iopub.status.busy": "2025-08-13T10:32:31.188449Z",
     "iopub.status.idle": "2025-08-13T10:35:07.869094Z",
     "shell.execute_reply": "2025-08-13T10:35:07.868442Z",
     "shell.execute_reply.started": "2025-08-13T10:32:31.188710Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 VALUTAZIONE FINALE\n",
      "========================================\n",
      "  Processati 1/100 esempi...\n",
      "  Processati 26/100 esempi...\n",
      "  Processati 51/100 esempi...\n",
      "  Processati 76/100 esempi...\n",
      "\n",
      "📈 RETAIN:\n",
      "  Perplexity: 116.42 (su 100 esempi)\n",
      "  ROUGE-L: 0.004 (su 100 esempi)\n",
      "  Processati 1/100 esempi...\n",
      "  Processati 26/100 esempi...\n",
      "  Processati 51/100 esempi...\n",
      "  Processati 76/100 esempi...\n",
      "\n",
      "📈 FORGET:\n",
      "  Perplexity: 167.07 (su 100 esempi)\n",
      "  ROUGE-L: 0.002 (su 100 esempi)\n",
      "\n",
      "========================================\n",
      "🎯 ANALISI UNLEARNING\n",
      "========================================\n",
      "\n",
      "📊 Unlearning Ratio: 1.435\n",
      "✅ OTTIMO: Unlearning efficace\n",
      "\n",
      "📝 Qualità Generazione:\n",
      "  Retain ROUGE-L: 0.004\n",
      "  Forget ROUGE-L: 0.002\n",
      "✅ CORRETTO: Mantiene qualità su retain, degrada su forget\n",
      "\n",
      "💾 Risultati valutazione salvati in: balanced_results/evaluation_results.pt\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_final(model, tokenizer, max_examples=100):\n",
    "    \"\"\"Valutazione completa del modello\"\"\"\n",
    "    \n",
    "    print(\"VALUTAZIONE FINALE\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    from rouge_score import rouge_scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    def eval_dataset(path, name):\n",
    "        perplexities, rouge_scores = [], []\n",
    "        \n",
    "        with open(path, \"r\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= max_examples: \n",
    "                    break\n",
    "                try:\n",
    "                    item = json.loads(line)\n",
    "                    keys = list(item.keys())\n",
    "                    question = str(item[keys[0]])\n",
    "                    expected = str(item[keys[1]]) if len(keys) > 1 else \"\"\n",
    "                    \n",
    "                    # Perplexity\n",
    "                    full_text = question + expected\n",
    "                    enc = tokenizer(full_text, return_tensors=\"pt\", max_length=256, truncation=True)\n",
    "                    enc = {k: v.to(model.device) for k, v in enc.items()}\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**enc, labels=enc['input_ids'])\n",
    "                        if outputs.loss is not None:\n",
    "                            perplexities.append(torch.exp(outputs.loss).item())\n",
    "                    \n",
    "                    # Generazione per ROUGE\n",
    "                    input_enc = tokenizer(question, return_tensors=\"pt\", max_length=256, truncation=True)\n",
    "                    input_enc = {k: v.to(model.device) for k, v in input_enc.items()}\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        gen_outputs = model.generate(\n",
    "                            input_enc['input_ids'],\n",
    "                            attention_mask=input_enc['attention_mask'],\n",
    "                            max_new_tokens=64,\n",
    "                            do_sample=False,\n",
    "                            pad_token_id=tokenizer.eos_token_id\n",
    "                        )\n",
    "                        \n",
    "                        new_tokens = gen_outputs[:, input_enc['input_ids'].shape[-1]:]\n",
    "                        generated_text = tokenizer.decode(new_tokens[0], skip_special_tokens=True)\n",
    "                    \n",
    "                    # ROUGE score\n",
    "                    rouge_result = scorer.score(expected, generated_text)\n",
    "                    rouge_scores.append(rouge_result['rougeL'].fmeasure)\n",
    "                    \n",
    "                    if i % 25 == 0:\n",
    "                        print(f\"  Processati {i+1}/{max_examples} esempi...\")\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        avg_ppl = np.mean(perplexities) if perplexities else float('inf')\n",
    "        avg_rouge = np.mean(rouge_scores) if rouge_scores else 0.0\n",
    "        \n",
    "        print(f\"\\n📈 {name}:\")\n",
    "        print(f\"  Perplexity: {avg_ppl:.2f} (su {len(perplexities)} esempi)\")\n",
    "        print(f\"  ROUGE-L: {avg_rouge:.3f} (su {len(rouge_scores)} esempi)\")\n",
    "        \n",
    "        return avg_ppl, avg_rouge\n",
    "    \n",
    "    retain_ppl, retain_rouge = eval_dataset(\"validation/retain.jsonl\", \"RETAIN\")\n",
    "    forget_ppl, forget_rouge = eval_dataset(\"validation/forget.jsonl\", \"FORGET\")\n",
    "    \n",
    "    # Analisi finale\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\" ANALISI UNLEARNING\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    ratio = forget_ppl / retain_ppl if retain_ppl > 0 else 0\n",
    "    print(f\"\\n📊 Unlearning Ratio: {ratio:.3f}\")\n",
    "    \n",
    "    if ratio > 1.2:\n",
    "        print(\"✅ OTTIMO: Unlearning efficace\")\n",
    "    elif ratio > 1.0:\n",
    "        print(\"⚠️ BUONO: Unlearning parziale\")\n",
    "    else:\n",
    "        print(\"❌ Insufficiente unlearning\")\n",
    "    \n",
    "    print(f\"\\n Qualità Generazione:\")\n",
    "    print(f\"  Retain ROUGE-L: {retain_rouge:.3f}\")\n",
    "    print(f\"  Forget ROUGE-L: {forget_rouge:.3f}\")\n",
    "    \n",
    "    if retain_rouge > forget_rouge:\n",
    "        print(\"✅ CORRETTO: Mantiene qualità su retain, degrada su forget\")\n",
    "    else:\n",
    "        print(\"⚠️ ATTENZIONE: Qualità simile su entrambi i set\")\n",
    "    \n",
    "    return {\n",
    "        \"retain_ppl\": retain_ppl, \n",
    "        \"forget_ppl\": forget_ppl, \n",
    "        \"ratio\": ratio,\n",
    "        \"retain_rouge\": retain_rouge,\n",
    "        \"forget_rouge\": forget_rouge\n",
    "    }\n",
    "\n",
    "# Eseguire valutazione\n",
    "student_model.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "final_results = evaluate_model_final(student_model, tokenizer, max_examples=100)\n",
    "\n",
    "# Salvare risultati valutazione\n",
    "torch.save(final_results, 'balanced_results/evaluation_results.pt')\n",
    "print(\"\\n Risultati valutazione salvati in: balanced_results/evaluation_results.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Mostra memoria prima\n",
    "!nvidia-smi\n",
    "\n",
    "# Elimina tutte le variabili definite (tranne quelle di sistema)\n",
    "for name in dir():\n",
    "    if not name.startswith('_'):\n",
    "        del globals()[name]\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "# Garbage collector per pulire RAM Python\n",
    "gc.collect()\n",
    "\n",
    "# Libera la cache PyTorch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "# Mostra memoria dopo\n",
    "!nvidia-smi\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8051727,
     "sourceId": 12737770,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
