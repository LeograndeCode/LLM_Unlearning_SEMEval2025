{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearning DualTeacher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T08:37:29.448844Z",
     "iopub.status.busy": "2025-09-07T08:37:29.448419Z",
     "iopub.status.idle": "2025-09-07T08:38:03.989740Z",
     "shell.execute_reply": "2025-09-07T08:38:03.988902Z",
     "shell.execute_reply.started": "2025-09-07T08:37:29.448814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages for Kaggle environment (safe if already installed)\n",
    "%pip install -q rouge-score\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "\n",
    "# Paths and configuration for Kaggle environment\n",
    "MODEL_PATH = \"/kaggle/input/olmo-model/semeval25-unlearning-1B-model\"\n",
    "DATA_PATH = \"/kaggle/input/olmo-model/semeval25-unlearning-data\"\n",
    "MIA_VAL_PATH = \"/kaggle/input/mia-dataset-val\"\n",
    "MIA_TRAIN_PATH = \"/kaggle/input/mia-dataset\"\n",
    "GOOD_TEACHER_PATH = \"/kaggle/input/good-teacher\"\n",
    "\n",
    "STUDENT_TRAINED = \"/kaggle/input/student-trained\"\n",
    "STUDENT_PATH = \"/kaggle/working/studentmodel_final\"\n",
    "Path(STUDENT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy any pre-trained student artifacts from input to working dir (idempotent)\n",
    "dir_path = Path(STUDENT_TRAINED)\n",
    "for file in dir_path.iterdir():\n",
    "    shutil.copyfile(f\"{STUDENT_TRAINED}/{file.name}\", f\"{STUDENT_PATH}/{file.name}\")\n",
    "\n",
    "# Report visible GPUs in Kaggle runtime\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T08:38:03.991948Z",
     "iopub.status.busy": "2025-09-07T08:38:03.991372Z",
     "iopub.status.idle": "2025-09-07T08:38:06.746068Z",
     "shell.execute_reply": "2025-09-07T08:38:06.745324Z",
     "shell.execute_reply.started": "2025-09-07T08:38:03.991902Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load parquet datasets from Kaggle inputs\n",
    "retain_train_df = pd.read_parquet(f\"{DATA_PATH}/data/retain_train-00000-of-00001.parquet\", engine='pyarrow')\n",
    "retain_validation_df = pd.read_parquet(f\"{DATA_PATH}/data/retain_validation-00000-of-00001.parquet\", engine='pyarrow')\n",
    "forget_train_df = pd.read_parquet(f\"{DATA_PATH}/data/forget_train-00000-of-00001.parquet\", engine='pyarrow')\n",
    "forget_validation_df = pd.read_parquet(f\"{DATA_PATH}/data/forget_validation-00000-of-00001.parquet\", engine='pyarrow')\n",
    "\n",
    "# Save as JSONL for evaluation scripts (portable without shell commands)\n",
    "Path('train').mkdir(parents=True, exist_ok=True)\n",
    "Path('validation').mkdir(parents=True, exist_ok=True)\n",
    "retain_train_df.to_json('train/retain.jsonl', orient='records', lines=True)\n",
    "forget_train_df.to_json('train/forget.jsonl', orient='records', lines=True)\n",
    "retain_validation_df.to_json('validation/retain.jsonl', orient='records', lines=True)\n",
    "forget_validation_df.to_json('validation/forget.jsonl', orient='records', lines=True)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Datasets saved and tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T08:38:06.747061Z",
     "iopub.status.busy": "2025-09-07T08:38:06.746831Z",
     "iopub.status.idle": "2025-09-07T08:38:06.754981Z",
     "shell.execute_reply": "2025-09-07T08:38:06.754208Z",
     "shell.execute_reply.started": "2025-09-07T08:38:06.747037Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UnlearningDataset(Dataset):\n",
    "    \"\"\"Dataset for unlearning with combined input and output text.\n",
    "\n",
    "    - Expects items with keys: 'input', 'output', optional 'split' ('retain' or 'forget').\n",
    "    - Returns tokenized tensors and the start index of the output (start_locs).\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if isinstance(data_source, pd.DataFrame):\n",
    "            self.data = data_source\n",
    "            print(f\"Loaded {len(self.data)} samples from DataFrame\")\n",
    "        elif isinstance(data_source, str):\n",
    "            data_list = []\n",
    "            with open(data_source, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    item = json.loads(line.strip())\n",
    "                    data_list.append(item)\n",
    "            self.data = pd.DataFrame(data_list)\n",
    "            print(f\"Loaded {len(self.data)} samples from {data_source}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        input_text = item[\"input\"]\n",
    "        output_text = item[\"output\"]\n",
    "\n",
    "        # Single tokenization of concatenated input and output\n",
    "        combined = f\"{input_text} {output_text}\"\n",
    "        tokenized = self.tokenizer(\n",
    "            combined,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Tokenize only the input to compute the start index of the output\n",
    "        input_ids = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n",
    "            \"start_locs\": input_ids.size(0),  # index where output begins\n",
    "            \"labels\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"split\": 1 if item.get(\"split\", \"retain\") == \"forget\" else 0\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T08:38:06.757217Z",
     "iopub.status.busy": "2025-09-07T08:38:06.756977Z",
     "iopub.status.idle": "2025-09-07T08:38:06.813667Z",
     "shell.execute_reply": "2025-09-07T08:38:06.812941Z",
     "shell.execute_reply.started": "2025-09-07T08:38:06.757200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "batch_size = 4\n",
    "# Build separate datasets for retain/forget splits\n",
    "retain_train_dataset = UnlearningDataset(retain_train_df, tokenizer)\n",
    "forget_train_dataset = UnlearningDataset(forget_train_df, tokenizer)\n",
    "\n",
    "retain_train_dataloader = DataLoader(retain_train_dataset, batch_size, shuffle=True)\n",
    "forget_train_dataloader = DataLoader(forget_train_dataset, batch_size, shuffle=True)\n",
    "\n",
    "# Validation loaders (no shuffling)\n",
    "retain_val_dataset = UnlearningDataset(retain_validation_df, tokenizer)\n",
    "forget_val_dataset = UnlearningDataset(forget_validation_df, tokenizer)\n",
    "retain_val_dataloader = DataLoader(retain_val_dataset, batch_size, shuffle=False)\n",
    "forget_val_dataloader = DataLoader(forget_val_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DualTeacher Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T15:36:45.707061Z",
     "iopub.status.busy": "2025-09-07T15:36:45.706832Z",
     "iopub.status.idle": "2025-09-07T15:36:45.761260Z",
     "shell.execute_reply": "2025-09-07T15:36:45.760581Z",
     "shell.execute_reply.started": "2025-09-07T15:36:45.707035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DualTeacherTrainer:\n",
    "    def __init__(self, model_path, tokenizer, teacher_lora_config, student_lora_config, device_map=None):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.teacher_lora_config = teacher_lora_config\n",
    "        self.student_lora_config = student_lora_config\n",
    "        self.device_map = device_map or {\"student\": \"cuda:0\", \"teacher\": \"cuda:1\"}\n",
    "        \n",
    "        self.good_teacher = None\n",
    "        self.student_model = None\n",
    "        self.initial_state_dict = {}\n",
    "        \n",
    "        # Validation tracking\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "        # Sequential training parameters\n",
    "        self.forget_weight_multiplier = 0.5  # Reduce forget influence to prevent catastrophic forgetting\n",
    "        \n",
    "        \n",
    "    def setup_models(self, skip_teacher_setup=False):\n",
    "        \"\"\"Initialize and setup both teacher and student models\"\"\"\n",
    "        # Ensure two GPUs are available (Kaggle dual GPU runtime)\n",
    "        if torch.cuda.device_count() < 2:\n",
    "            raise RuntimeError(\"This notebook expects 2 GPUs. Please enable a 2-GPU accelerator in Kaggle settings.\")\n",
    "        print(\"Setting up models...\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(self.model_path, local_files_only=True)\n",
    "        \n",
    "        if skip_teacher_setup is False:\n",
    "            # Setup good teacher with LoRA (for training)\n",
    "            self.good_teacher = get_peft_model(base_model, self.teacher_lora_config)\n",
    "            self.good_teacher = self.good_teacher.to(self.device_map[\"teacher\"])\n",
    "            self.good_teacher.print_trainable_parameters()\n",
    "            \n",
    "        # Setup student model with LoRA\n",
    "        self.student_model = get_peft_model(base_model, self.student_lora_config)\n",
    "        self.student_model = self.student_model.to(self.device_map[\"student\"])\n",
    "        self.student_model.print_trainable_parameters()\n",
    "        \n",
    "        # Save initial state for task vector calculation\n",
    "        for name, param in self.student_model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.initial_state_dict[name] = param.data.clone()\n",
    "        \n",
    "        print(\"Models setup completed\")\n",
    "        \n",
    "    \n",
    "    def create_bad_teacher_logits(self, good_teacher_logits):\n",
    "        \"\"\"Create bad-teacher logits as noisy values around a uniform distribution.\"\"\"\n",
    "        # Uniform logits (pre-softmax)\n",
    "        uniform_logits = torch.zeros_like(good_teacher_logits)\n",
    "        # Add Gaussian noise\n",
    "        noisy_logits = uniform_logits + 0.1 * torch.randn_like(good_teacher_logits)\n",
    "        return noisy_logits\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def compute_retain_only_loss(self, batch):\n",
    "        \"\"\"Compute loss only for retain samples\"\"\"\n",
    "        # Devices\n",
    "        student_device = self.device_map[\"student\"]\n",
    "        teacher_device = self.device_map[\"teacher\"]\n",
    "    \n",
    "        # Inputs\n",
    "        input_ids_student = batch[\"input_ids\"].to(student_device)\n",
    "        attention_mask_student = batch[\"attention_mask\"].to(student_device)\n",
    "    \n",
    "        # Student forward\n",
    "        student_logits = self.student_model(input_ids_student, attention_mask=attention_mask_student).logits\n",
    "        student_log_probs = torch.nn.functional.log_softmax(student_logits, dim=-1).to(teacher_device)\n",
    "    \n",
    "        # Teacher forward (no grad)\n",
    "        input_ids_teacher = batch[\"input_ids\"].to(teacher_device)\n",
    "        attention_mask_teacher = batch[\"attention_mask\"].to(teacher_device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            good_teacher_logits = self.good_teacher(input_ids_teacher, attention_mask=attention_mask_teacher).logits\n",
    "            good_teacher_probs = torch.nn.functional.softmax(good_teacher_logits, dim=-1)\n",
    "    \n",
    "        # KL divergence for retain samples\n",
    "        retain_kl = torch.nn.functional.kl_div(\n",
    "            student_log_probs,\n",
    "            good_teacher_probs,\n",
    "            reduction=\"batchmean\",  # averaged over batch and tokens\n",
    "            log_target=False\n",
    "        )\n",
    "\n",
    "        # Student entropy regularization (encourages confidence)\n",
    "        entropy_loss = -(student_log_probs.exp() * student_log_probs).sum(-1).mean()\n",
    "        \n",
    "        return 3.0 * retain_kl - 0.1 * entropy_loss    # Negative entropy to encourage confidence\n",
    "    \n",
    "    def compute_forget_only_loss(self, batch):\n",
    "        \"\"\"Compute loss only for forget samples\"\"\"\n",
    "        # Devices\n",
    "        student_device = self.device_map[\"student\"]\n",
    "        teacher_device = self.device_map[\"teacher\"]\n",
    "    \n",
    "        # Inputs\n",
    "        input_ids_student = batch[\"input_ids\"].to(student_device)\n",
    "        attention_mask_student = batch[\"attention_mask\"].to(student_device)\n",
    "    \n",
    "        # Student forward\n",
    "        student_logits = self.student_model(input_ids_student, attention_mask=attention_mask_student).logits\n",
    "        student_log_probs = torch.nn.functional.log_softmax(student_logits, dim=-1).to(teacher_device)\n",
    "    \n",
    "        # Teacher forward (no grad)\n",
    "        input_ids_teacher = batch[\"input_ids\"].to(teacher_device)\n",
    "        attention_mask_teacher = batch[\"attention_mask\"].to(teacher_device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            good_teacher_logits = self.good_teacher(input_ids_teacher, attention_mask=attention_mask_teacher).logits\n",
    "            bad_teacher_logits = self.create_bad_teacher_logits(good_teacher_logits)\n",
    "            bad_teacher_probs = torch.nn.functional.softmax(bad_teacher_logits, dim=-1)\n",
    "    \n",
    "        # KL divergence for forget samples (towards bad teacher)\n",
    "        forget_kl = torch.nn.functional.kl_div(\n",
    "            student_log_probs,\n",
    "            bad_teacher_probs,\n",
    "            reduction=\"batchmean\",\n",
    "            log_target=False\n",
    "        )\n",
    "\n",
    "        # Student entropy regularization (encourages uncertainty)\n",
    "        entropy_loss = -(student_log_probs.exp() * student_log_probs).sum(-1).mean()\n",
    "        \n",
    "        return 2.0 * forget_kl + 0.1 * entropy_loss  # Positive entropy to encourage uncertainty\n",
    "\n",
    "    def compute_kl_divergence(self, batch):\n",
    "        \"\"\"Original mixed training method\"\"\"\n",
    "        # Devices\n",
    "        student_device = self.device_map[\"student\"]\n",
    "        teacher_device = self.device_map[\"teacher\"]\n",
    "    \n",
    "        # Inputs\n",
    "        input_ids_student = batch[\"input_ids\"].to(student_device)\n",
    "        attention_mask_student = batch[\"attention_mask\"].to(student_device)\n",
    "        labels_student = batch[\"labels\"].to(student_device)\n",
    "        split = batch[\"split\"].float().to(student_device)\n",
    "    \n",
    "        # Student forward\n",
    "        student_logits = self.student_model(input_ids_student, attention_mask=attention_mask_student).logits\n",
    "        student_log_probs = torch.nn.functional.log_softmax(student_logits, dim=-1).to(teacher_device)\n",
    "    \n",
    "        # Teacher forward (no grad)\n",
    "        input_ids_teacher = batch[\"input_ids\"].to(teacher_device)\n",
    "        attention_mask_teacher = batch[\"attention_mask\"].to(teacher_device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            good_teacher_logits = self.good_teacher(input_ids_teacher, attention_mask=attention_mask_teacher).logits\n",
    "            bad_teacher_logits = self.create_bad_teacher_logits(good_teacher_logits)\n",
    "            good_teacher_probs = torch.nn.functional.softmax(good_teacher_logits, dim=-1)\n",
    "            bad_teacher_probs = torch.nn.functional.softmax(bad_teacher_logits, dim=-1)\n",
    "    \n",
    "        # Masks retain/forget\n",
    "        retain_mask = (split <= 0.5).to(teacher_device)\n",
    "        forget_mask = (split > 0.5).to(teacher_device)\n",
    "    \n",
    "        total_loss = 0.0\n",
    "        if retain_mask.any():\n",
    "            retain_kl = torch.nn.functional.kl_div(\n",
    "                student_log_probs[retain_mask],\n",
    "                good_teacher_probs[retain_mask.bool()],\n",
    "                reduction=\"none\",\n",
    "                log_target=False\n",
    "            ).sum(dim=-1)  # sum over vocabulary\n",
    "            retain_kl = retain_kl.mean()\n",
    "            total_loss += 3.0 * retain_kl  # retain_weight\n",
    "    \n",
    "        if forget_mask.any():\n",
    "            forget_kl = torch.nn.functional.kl_div(\n",
    "                student_log_probs[forget_mask],\n",
    "                bad_teacher_probs[forget_mask.bool()],\n",
    "                reduction=\"none\",\n",
    "                log_target=False\n",
    "            ).sum(dim=-1)\n",
    "            forget_kl = forget_kl.mean()\n",
    "            total_loss +=  1.5 * forget_kl  # forget_weight\n",
    "    \n",
    "        # Student entropy regularization\n",
    "        entropy_loss = -(student_log_probs.exp() * student_log_probs).sum(-1).mean()\n",
    "    \n",
    "        return total_loss + 0.2 * entropy_loss\n",
    "\n",
    "    def validate_retain_only(self, retain_val_dataloader):\n",
    "        \"\"\"Validate only on retain samples\"\"\"\n",
    "        self.student_model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in retain_val_dataloader:\n",
    "                loss = self.compute_retain_only_loss(batch)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        self.student_model.train()\n",
    "        return np.mean(val_losses) if val_losses else float('inf')\n",
    "\n",
    "    def validate_forget_only(self, forget_val_dataloader):\n",
    "        \"\"\"Validate only on forget samples\"\"\"\n",
    "        self.student_model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in forget_val_dataloader:\n",
    "                loss = self.compute_forget_only_loss(batch)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        self.student_model.train()\n",
    "        return np.mean(val_losses) if val_losses else float('inf')\n",
    "      \n",
    "        \n",
    "    def train_student_sequential(self, retain_dataloader, forget_dataloader, \n",
    "                               retain_val_dataloader=None, forget_val_dataloader=None, \n",
    "                               num_epochs=6, lr=1e-5, val_freq=1, patience=3):\n",
    "        \"\"\"Train student model with sequential retain/forget training\"\"\"\n",
    "        print(\"Training student with SEQUENTIAL dual-teacher approach...\")\n",
    "        \n",
    "        self.student_model.train()\n",
    "        optimizer = torch.optim.AdamW(self.student_model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        \n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            retain_losses = []\n",
    "            forget_losses = []\n",
    "            \n",
    "            # Calculate total batches for progress bar\n",
    "            total_batches = len(retain_dataloader) + len(forget_dataloader)\n",
    "            \n",
    "            with tqdm(total=total_batches, desc=f\"Student Epoch {epoch+1} (Sequential)\") as pbar:\n",
    "                \n",
    "                # Phase 1: Train on retain batches\n",
    "                pbar.set_description(f\"Epoch {epoch+1} - Phase 1: Retain\")\n",
    "                for batch in retain_dataloader:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.compute_retain_only_loss(batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    retain_losses.append(loss.item())\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix({\"Retain Loss\": f\"{loss.item():.4f}\"})\n",
    "                \n",
    "                # Phase 2: Pure forget training (with reduced weight)\n",
    "                pbar.set_description(f\"Epoch {epoch+1} - Phase 2: Forget\")\n",
    "                for batch in forget_dataloader:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.compute_forget_only_loss(batch) * self.forget_weight_multiplier\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    forget_losses.append(loss.item() / self.forget_weight_multiplier)  # Store unweighted for logging\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix({\"Forget Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "                \n",
    "               \n",
    "            # Logging\n",
    "            avg_retain_loss = np.mean(retain_losses) if retain_losses else 0.0\n",
    "            avg_forget_loss = np.mean(forget_losses) if forget_losses else 0.0\n",
    "            \n",
    "            print(f\"Student Epoch {epoch+1}\")\n",
    "            print(f\"   └─ Retain: {avg_retain_loss:.4f}, Forget: {avg_forget_loss:.4f} (×{self.forget_weight_multiplier})\")\n",
    "            \n",
    "            # Validation if validation dataloaders are provided\n",
    "            if retain_val_dataloader is not None and forget_val_dataloader is not None and (epoch + 1) % val_freq == 0:\n",
    "                print(\"Running validation...\")\n",
    "                retain_val_loss = self.validate_retain_only(retain_val_dataloader)\n",
    "                forget_val_loss = self.validate_forget_only(forget_val_dataloader)\n",
    "                \n",
    "                # Combine with weights for early stopping\n",
    "                # For forget loss, we use the inverse because we want it to be high (more confusion = better)\n",
    "                # Normalize to get a consistent combined metric\n",
    "                retain_score = retain_val_loss                   # lower = better\n",
    "                forget_score = 1.0 / (1.0 + forget_val_loss)     # lower = better if forget loss is high\n",
    "            \n",
    "                combined_val_loss = 0.7 * retain_score + 0.3 * forget_score\n",
    "                print(f\"Validation - Retain: {retain_val_loss:.4f}, Forget: {forget_val_loss:.4f}, Combined: {combined_val_loss:.4f}\")\n",
    "                \n",
    "                # Check if this is the best model so far\n",
    "                if combined_val_loss < self.best_val_loss:\n",
    "                    self.best_val_loss = combined_val_loss\n",
    "                    self.best_epoch = epoch + 1\n",
    "                    patience_counter = 0\n",
    "                    print(f\"New best validation loss: {combined_val_loss:.4f}\")\n",
    "                    \n",
    "                    # Save best model\n",
    "                    self.save_model(f\"studentmodel_best_val_sequential\")\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    print(f\"No improvement for {patience_counter} validation checks\")\n",
    "                \n",
    "                # Early stopping\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1} (no improvement for {patience} validations)\")\n",
    "                    break\n",
    "            \n",
    "            # Save model after each epoch\n",
    "            self.save_model(f\"studentmodel_epoch_{epoch+1}_sequential\")\n",
    "        \n",
    "        print(\"Student sequential training completed\")\n",
    "        print(f\"Best validation loss: {self.best_val_loss:.4f} at epoch {self.best_epoch}\")\n",
    "\n",
    "    def train_good_teacher(self, dataloader, val_dataloader=None, num_epochs=2, lr=1e-4, save_path=\"good_teacher_adapter\", val_freq=1):\n",
    "        \"\"\"Train the good teacher on retain samples using LoRA with optional validation\"\"\"\n",
    "        print(\"Training good teacher with LoRA...\")\n",
    "\n",
    "        self.good_teacher.to(self.device_map[\"teacher\"])\n",
    "        self.good_teacher.train()\n",
    "        optimizer = torch.optim.AdamW(self.good_teacher.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs} - Good Teacher Training\")\n",
    "            \n",
    "            epoch_losses = []\n",
    "            retain_batches_processed = 0\n",
    "            \n",
    "            with tqdm(total=len(dataloader), desc=f\"Good Teacher Epoch {epoch+1}\") as pbar:\n",
    "                for batch in dataloader:\n",
    "                    # Filter retain samples only\n",
    "                    split = batch['split']\n",
    "                    retain_mask = (split == 0)\n",
    "                    \n",
    "                    if not retain_mask.any():\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract retain samples\n",
    "                    input_ids = batch['input_ids'][retain_mask].to(self.device_map[\"teacher\"])\n",
    "                    attention_mask = batch['attention_mask'][retain_mask].to(self.device_map[\"teacher\"])\n",
    "                    labels = batch['labels'][retain_mask].to(self.device_map[\"teacher\"])\n",
    "                    \n",
    "                    if input_ids.size(0) == 0:\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = self.good_teacher(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    epoch_losses.append(loss.item())\n",
    "                    retain_batches_processed += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                    if retain_batches_processed % 100 == 0:\n",
    "                        pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "            if epoch_losses:\n",
    "                avg_loss = np.mean(epoch_losses)\n",
    "                print(f\"Good Teacher Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n",
    "                \n",
    "                # Validation if validation dataloader is provided\n",
    "                if val_dataloader is not None and (epoch + 1) % val_freq == 0:\n",
    "                    print(\"Running validation...\")\n",
    "                    val_loss = self.validate_good_teacher(val_dataloader)\n",
    "                    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        print(\"Good teacher training completed\")\n",
    "        \n",
    "        # Save only LoRA adapter\n",
    "        self.save_good_teacher(save_path)\n",
    "        print(f\"Good teacher adapter saved to {save_path}\")\n",
    "        \n",
    "    def save_good_teacher(self, save_path):\n",
    "        \"\"\"Save only the LoRA adapter of the good teacher\"\"\"\n",
    "        self.good_teacher.save_pretrained(save_path)\n",
    "        \n",
    "    def save_model(self, save_path):\n",
    "        \"\"\"Save student model and tokenizer\"\"\"\n",
    "        self.student_model.save_pretrained(save_path)\n",
    "        self.tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    def load_teacher(self, GOOD_TEACHER_PATH):\n",
    "        \"\"\"Load a pre-trained good teacher model\"\"\"\n",
    "        self.good_teacher = AutoModelForCausalLM.from_pretrained(GOOD_TEACHER_PATH)\n",
    "        self.good_teacher.eval()  # freeze it\n",
    "        for param in self.good_teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.good_teacher.to(self.device_map[\"teacher\"])\n",
    "        print(\"Good teacher loaded and frozen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Trainer and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T08:38:06.856124Z",
     "iopub.status.busy": "2025-09-07T08:38:06.855856Z",
     "iopub.status.idle": "2025-09-07T08:38:06.872388Z",
     "shell.execute_reply": "2025-09-07T08:38:06.871770Z",
     "shell.execute_reply.started": "2025-09-07T08:38:06.856107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_good_teacher = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T08:38:06.873257Z",
     "iopub.status.busy": "2025-09-07T08:38:06.873052Z",
     "iopub.status.idle": "2025-09-07T08:38:06.887127Z",
     "shell.execute_reply": "2025-09-07T08:38:06.886445Z",
     "shell.execute_reply.started": "2025-09-07T08:38:06.873242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configure LoRA for teacher and student\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = DualTeacherTrainer(\n",
    "    model_path=MODEL_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    teacher_lora_config=lora_config,\n",
    "    student_lora_config=lora_config,\n",
    "    device_map={\"student\": \"cuda:0\", \"teacher\": \"cuda:1\"}\n",
    ")\n",
    "# # Setup models\n",
    "\n",
    "if train_good_teacher == True:\n",
    "    trainer.setup_models()\n",
    "    # Train good teacher (will be converted to base model after training)\n",
    "    trainer.train_good_teacher(train_dataloader, val_dataloader, num_epochs=5) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T08:46:15.369037Z",
     "iopub.status.busy": "2025-09-07T08:46:15.368737Z",
     "iopub.status.idle": "2025-09-07T08:46:19.840435Z",
     "shell.execute_reply": "2025-09-07T08:46:19.839323Z",
     "shell.execute_reply.started": "2025-09-07T08:46:15.369014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup models (student is initialized regardless)\n",
    "trainer.setup_models(skip_teacher_setup=True)\n",
    "\n",
    "# Load a pre-trained good teacher base model and freeze it\n",
    "trainer.load_teacher(GOOD_TEACHER_PATH)\n",
    "\n",
    "# Now train the student directly\n",
    "trainer.train_student_sequential(\n",
    "    retain_train_dataloader, \n",
    "    forget_train_dataloader,\n",
    "    retain_val_dataloader,\n",
    "    forget_val_dataloader,\n",
    "    num_epochs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-07T08:39:17.887920Z",
     "iopub.status.idle": "2025-09-07T08:39:17.888148Z",
     "shell.execute_reply": "2025-09-07T08:39:17.888050Z",
     "shell.execute_reply.started": "2025-09-07T08:39:17.888040Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"evaluation.py\", \"w\") as f:\n",
    "    f.write(r\"\"\"\n",
    "# Official evaluation script provided by the task organizers\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import shutil\n",
    "import argparse\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from accelerate import Accelerator\n",
    "from collections import defaultdict\n",
    "from statistics import mean, harmonic_mean\n",
    "from rouge_score import rouge_scorer\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "def get_args_and_verify():\n",
    "    parser = argparse.ArgumentParser(description=\"Script to run inference and evaluation\")\n",
    "    parser.add_argument('--data_path', help=\"Path to unlearning dataset containing jsonl files\")\n",
    "    parser.add_argument('--checkpoint_path', help=\"Path to model checkpoint\")\n",
    "    parser.add_argument('--output_dir', required=False, default=None, help=\"Path to store inference files and evaluation results\")\n",
    "    parser.add_argument('--mia_data_path', required=False, default=None, help=\"Path to member and nonmember jsonl files for MIA attack\")\n",
    "    parser.add_argument('--mmlu_metrics_file_path', required=False, default=None, help=\"Path to metrics.json file generated by MMLU\")\n",
    "    parser.add_argument('--max_new_tokens', required=False, type=int, default=256, help='Maximum number of tokens to generate')\n",
    "    parser.add_argument('--batch_size', required=False, type=int, default=25, help='Batch size for inference')\n",
    "    parser.add_argument('--debug', required=False, default=False, action='store_true', help='Print detailed messages')\n",
    "    parser.add_argument('--compute_metrics_only', required=False, default=False, action='store_true', help='Skip inference and compute metrics from inference files')\n",
    "    parser.add_argument('--seed', required=False, default=42, help='Random seed for experiments')\n",
    "    parser.add_argument('--keep_files', required=False, default=False, action='store_true', help='Retain intermediate files')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.compute_metrics_only:\n",
    "        args.keep_files = True\n",
    "\n",
    "    if args.output_dir is None:\n",
    "        args.output_dir = os.getcwd()\n",
    "    else:\n",
    "        args.output_dir = args.output_dir.rstrip('/')\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Verify data files exist\n",
    "    assert(os.path.exists(args.data_path))\n",
    "    assert(os.path.exists(os.path.join(args.data_path, 'forget.jsonl')))\n",
    "    assert(os.path.exists(os.path.join(args.data_path, 'retain.jsonl')))\n",
    "\n",
    "    # If specified, verify data files exist\n",
    "    if args.mia_data_path is not None:\n",
    "        assert(os.path.exists(args.mia_data_path))\n",
    "        assert(os.path.exists(os.path.join(args.mia_data_path, 'member.jsonl')))\n",
    "        assert(os.path.exists(os.path.join(args.mia_data_path, 'nonmember.jsonl')))\n",
    "    else: # Add warning if MIA path not provided\n",
    "        print(\"WARNING: MIA data path not provided. Final evaluation includes MIA; please rerun with this option for accurate performance. Proceeding for now.\")\n",
    "        \n",
    "    if args.mmlu_metrics_file_path is not None:\n",
    "        assert(os.path.exists(args.mmlu_metrics_file_path))\n",
    "    else: # Add warning if MMLU file not provided\n",
    "        print(\"WARNING: MMLU metrics file not provided, your final evaluation metric includes MMLU aggregate performance so please run this test to get the accurate performance. Proceeding for now\")\n",
    "\n",
    "    return args\n",
    "\n",
    "def inference(args, model, tokenizer):\n",
    "    forget_file = args.data_path + 'forget.jsonl'\n",
    "    retain_file = args.data_path + 'retain.jsonl'\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    model.to(accelerator.device)\n",
    "\n",
    "    for split, train_file in [('retain', retain_file), ('forget', forget_file)]:\n",
    "        data_files = {}\n",
    "        dataset_args = {}\n",
    "        if train_file is not None:\n",
    "            data_files[\"train\"] = train_file\n",
    "        raw_datasets = datasets.load_dataset(\n",
    "            \"json\",\n",
    "            data_files=data_files,\n",
    "            **dataset_args,\n",
    "        )\n",
    "        train_dataset = raw_datasets[\"train\"]\n",
    "\n",
    "        output_dic = defaultdict(lambda :{'id': [], 'task': [], 'input': [], 'expected_output': [], 'model_output': [], 'nll': []})\n",
    "\n",
    "        with accelerator.split_between_processes(train_dataset, apply_padding=True) as data:\n",
    "            for idx in tqdm(range(len(data['input']))):\n",
    "                question, answer = data[\"input\"][idx], data[\"output\"][idx]\n",
    "                output_dic[accelerator.process_index]['id'].append(data[\"id\"][idx])\n",
    "                output_dic[accelerator.process_index]['task'].append(data[\"task\"][idx])\n",
    "                output_dic[accelerator.process_index]['input'].append(data[\"input\"][idx])\n",
    "                output_dic[accelerator.process_index]['expected_output'].append(data[\"output\"][idx])\n",
    "                input_ids = tokenizer(\n",
    "                    question,\n",
    "                    return_tensors='pt'\n",
    "                ).input_ids.to(model.device)\n",
    "\n",
    "                combined_input_ids = tokenizer(\n",
    "                    question+answer,\n",
    "                    return_tensors='pt'\n",
    "                ).input_ids.to(model.device)\n",
    "                combined_target_ids = combined_input_ids.clone()\n",
    "                combined_target_ids[:,:len(input_ids[0])] = -100\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Create attention mask to avoid warnings\n",
    "                    attention_mask = torch.ones_like(input_ids)\n",
    "                    out = model.generate(\n",
    "                        input_ids, \n",
    "                        attention_mask=attention_mask,\n",
    "                        max_new_tokens=args.max_new_tokens, \n",
    "                        do_sample=False, \n",
    "                        use_cache=True, \n",
    "                        pad_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "                    output_ids = out[:, len(input_ids[0]):]\n",
    "                    output = tokenizer.batch_decode(\n",
    "                        output_ids,\n",
    "                        skip_special_tokens=True,\n",
    "                        clean_up_tokenization_spaces=True)[0]\n",
    "                    output_dic[accelerator.process_index]['model_output'].append(output)\n",
    "\n",
    "                    # For Perplexity\n",
    "                    out = model(combined_input_ids, labels=combined_target_ids)\n",
    "                    if args.debug:\n",
    "                        print(tokenizer.batch_decode(\n",
    "                            torch.argmax(\n",
    "                                torch.nn.functional.softmax(\n",
    "                                    out.logits.clone().detach(),\n",
    "                                    dim=2),\n",
    "                                dim=2)[:, len(input_ids[0]):],\n",
    "                            skip_special_tokens=True,\n",
    "                            clean_up_tokenization_spaces=True)[0])\n",
    "                    neg_log_likelihood = out.loss.item()\n",
    "                    output_dic[accelerator.process_index]['nll'].append(neg_log_likelihood)\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        \n",
    "        if args.debug:\n",
    "            print([len(value) for value in output_dic[accelerator.process_index].values()])\n",
    "        output_df = pd.DataFrame.from_dict(output_dic[accelerator.process_index])\n",
    "        \n",
    "        output_file_name = f\"{args.output_dir}/{split}_{accelerator.process_index}.csv\"\n",
    "        if args.debug:\n",
    "            print('Saving to: ', output_file_name)\n",
    "        output_df.to_csv(output_file_name, index=False)\n",
    "\n",
    "def mia_attacks(args, model, tokenizer):\n",
    "    member_file = args.mia_data_path + 'member.jsonl'\n",
    "    nonmember_file = args.mia_data_path + 'nonmember.jsonl'\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    model.to(accelerator.device)\n",
    "\n",
    "    for dataset, train_file in [('member', member_file), ('nonmember', nonmember_file)]:\n",
    "        data_files = {}\n",
    "        dataset_args = {}\n",
    "        if train_file is not None:\n",
    "            data_files[\"train\"] = train_file\n",
    "        raw_datasets = datasets.load_dataset(\n",
    "            \"json\",\n",
    "            data_files=data_files,\n",
    "            **dataset_args,\n",
    "        )\n",
    "        train_dataset = raw_datasets[\"train\"]\n",
    "\n",
    "        output_dic = defaultdict(lambda :{'id': [], 'nll': []})\n",
    "\n",
    "        with accelerator.split_between_processes(train_dataset, apply_padding=True) as data:\n",
    "            for idx in tqdm(range(len(data['document']))):\n",
    "                document = data[\"document\"][idx]\n",
    "                output_dic[accelerator.process_index]['id'].append(data[\"id\"][idx])\n",
    "                input_ids = tokenizer(\n",
    "                    document,\n",
    "                    return_tensors='pt'\n",
    "                ).input_ids.to(model.device)\n",
    "\n",
    "                target_ids = input_ids.clone()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    out = model(input_ids, labels=target_ids)\n",
    "                    neg_log_likelihood = out.loss.item()\n",
    "                    output_dic[accelerator.process_index]['nll'].append(neg_log_likelihood)\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        \n",
    "        output_df = pd.DataFrame.from_dict(output_dic[accelerator.process_index])\n",
    "        \n",
    "        results_dir = os.path.join(args.output_dir, 'mia_results')\n",
    "        Path(results_dir).mkdir(parents=True, exist_ok=True)\n",
    "        output_file_name = f\"{results_dir}/{dataset}_{accelerator.process_index}.csv\"\n",
    "        if args.debug:\n",
    "            print('Saving to: ', output_file_name)\n",
    "        output_df.to_csv(output_file_name, index=False)\n",
    "\n",
    "def compute_auc(member_loss, nonmember_loss):\n",
    "    assert not np.any(np.isnan(member_loss))\n",
    "    assert not np.any(np.isnan(nonmember_loss))\n",
    "    combined_loss = member_loss + nonmember_loss \n",
    "    combined_loss = -1 * np.array(combined_loss)\n",
    "    combined_labels = len(member_loss) * [1] + len(nonmember_loss) * [0]\n",
    "    fp, tp, _ = roc_curve(combined_labels, combined_loss)\n",
    "\n",
    "    auc_score = float(auc(fp, tp))\n",
    "\n",
    "    return auc_score\n",
    "\n",
    "def compute_metrics(args):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    results = {}\n",
    "    aggregate_scores_list = []\n",
    "    for split in ['forget', 'retain']:\n",
    "        files = glob.glob(args.output_dir + '/{}_*.csv'.format(split))\n",
    "        if len(files) == 0:\n",
    "            print(\"[ERROR] Missing inference files, rerun script with inference first\")\n",
    "            return  # sys.exit(1) throws a long traceback so just return for now\n",
    "        df_list = [pd.read_csv(f) for f in files]\n",
    "        if not args.keep_files:\n",
    "            _ = [os.remove(f) for f in files]\n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "        df['regurgitation-score-rouge-1'] = None\n",
    "        df['regurgitation-score'] = None\n",
    "        df['knowledge-score'] = None\n",
    "        ground_truths = df['expected_output'].tolist()\n",
    "        gen_outputs = df['model_output'].tolist()\n",
    "\n",
    "        for i, (gen, gt) in enumerate(zip(gen_outputs, ground_truths)):\n",
    "            if df.loc[i, 'id'][:-1].endswith('sc'):\n",
    "                rouge_scores = scorer.score(str(gt), str(gen))\n",
    "                df.loc[i, 'regurgitation-score-rouge-1'] = rouge_scores['rouge1'].recall\n",
    "                df.loc[i, 'regurgitation-score'] = rouge_scores['rougeL'].recall\n",
    "            elif df.loc[i, 'id'][:-1].endswith('qa'):\n",
    "                 df.loc[i, 'knowledge-score'] = int(str(gt).strip().lower() == str(gen).strip().lower())\n",
    "\n",
    "        results[split+'-set'] = {'overall-regurgitation-score': np.mean(df['regurgitation-score']), 'overall-knowledge-score': np.mean(df['knowledge-score'])}\n",
    "        split_aggregate_scores_dict = df.groupby('task')[['regurgitation-score', 'knowledge-score']].mean().to_dict(orient='index')\n",
    "        results[split+'-set'].update(split_aggregate_scores_dict)\n",
    "        split_aggregate_score_values = [float(val) for inner in split_aggregate_scores_dict.values() for val in inner.values()]\n",
    "        if split == 'forget':\n",
    "            split_aggregate_score_values = [(1 - val) for val in split_aggregate_score_values]\n",
    "\n",
    "        aggregate_scores_list.extend(split_aggregate_score_values)\n",
    "\n",
    "    if args.mia_data_path is not None:\n",
    "        mia_results_dir = os.path.join(args.output_dir, 'mia_results')\n",
    "        mia_results = {}\n",
    "        for dataset in ['member', 'nonmember']:\n",
    "            files = glob.glob(mia_results_dir + '/{}_*.csv'.format(dataset))\n",
    "            if len(files) == 0:\n",
    "                print(\"[ERROR] Missing mia files, rerun script with inference first\")\n",
    "                return  # sys.exit(1) throws a long traceback so just return for no\n",
    "            df_list = [pd.read_csv(f) for f in files]\n",
    "            df = pd.concat(df_list, ignore_index=True)\n",
    "            mia_results[dataset] = df['nll'].tolist()\n",
    "        \n",
    "        if not args.keep_files:\n",
    "            shutil.rmtree(mia_results_dir)\n",
    "\n",
    "        auc = compute_auc(mia_results['member'], mia_results['nonmember'])\n",
    "        # Best MIA rates we can get are ~0.5. \n",
    "        # Scores close to 1 suggest under-unlearning\n",
    "        # Scores close to 0 suggest over-unlearning\n",
    "        results['mia_loss_acc'] = auc\n",
    "#        aggregate_scores_list.append(1 - auc) \n",
    "\n",
    "    if args.mmlu_metrics_file_path is not None:\n",
    "        with open(args.mmlu_metrics_file_path) as inptr:\n",
    "            mmlu_scores = json.loads(inptr.read())\n",
    "        results['mmlu_average'] = mmlu_scores['average_acc']\n",
    "#        aggregate_scores_list.append(mmlu_scores['average_acc'])\n",
    "    \n",
    "    results['aggregated-terms'] = aggregate_scores_list\n",
    "\n",
    "    task_aggregate = harmonic_mean(aggregate_scores_list)\n",
    "    results['aggregate-score'] = -1\n",
    "\n",
    "    results['harmonic-mean-task-aggregate'] = task_aggregate\n",
    "\n",
    "    # Need MMLU and MIA scores to compute the aggregate\n",
    "    if 'mmlu_average' in results and 'mia_loss_acc' in results:\n",
    "        if results['mmlu_average'] < 0.371:\n",
    "            # MMLU score should not drop below 75% of pre-unlearning preformance\n",
    "            print(f\"[WARNING] The MMLU average for the provided checkpoint is below threshold. If this happens your model may not be considered in final challenge ranking.\")\n",
    "\n",
    "        mia_final_score = 1 - abs(results['mia_loss_acc'] - 0.5)*2\n",
    "        results['mia_final_score'] = mia_final_score\n",
    "        results['aggregate-score'] = mean([task_aggregate, results['mmlu_average'], mia_final_score])\n",
    "\n",
    "    metrics_file = os.path.join(args.output_dir, 'evaluation_results.jsonl')\n",
    "    with open(metrics_file, 'w') as outptr:\n",
    "        outptr.write(json.dumps(results))\n",
    "\n",
    "def main():\n",
    "    args = get_args_and_verify()\n",
    "\n",
    "    # Set random seed\n",
    "    random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    if args.debug:\n",
    "        print('Evaluating Checkpoint at {}'.format(args.checkpoint_path))\n",
    "\n",
    "    checkpoint_path = args.checkpoint_path\n",
    "\n",
    "    # Set up accelerator\n",
    "    accelerator = Accelerator()\n",
    "    if not args.compute_metrics_only:\n",
    "        model = AutoModelForCausalLM.from_pretrained(checkpoint_path, torch_dtype=torch.bfloat16, trust_remote_code = True) # .to('cuda')\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "        inference(args, model, tokenizer)\n",
    "\n",
    "        if args.mia_data_path is not None:\n",
    "            mia_attacks(args, model, tokenizer)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        compute_metrics(args)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\"\"\")\n",
    "import sys\n",
    "import types\n",
    "\n",
    "try:\n",
    "    import evaluation\n",
    "    import importlib\n",
    "    importlib.reload(evaluation)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def run_evaluation(\n",
    "    data_path,\n",
    "    checkpoint_path,\n",
    "    output_dir=\"eval_results\",\n",
    "    mia_data_path=None, #MIA_TRAIN_PATH\n",
    "    mia_data_val_path=MIA_VAL_PATH,\n",
    "    mmlu_metrics_file_path=None,\n",
    "    max_new_tokens=256,\n",
    "    batch_size=25,\n",
    "    debug=False,\n",
    "    compute_metrics_only=False,\n",
    "    seed=42,\n",
    "    keep_files=True,\n",
    "):\n",
    "    try:\n",
    "        # Build an argparse-like args object\n",
    "        args = types.SimpleNamespace(\n",
    "            data_path=data_path,\n",
    "            checkpoint_path=checkpoint_path,\n",
    "            output_dir=output_dir,\n",
    "            mia_data_path=mia_data_path,\n",
    "            mia_data_val_path=mia_data_val_path,\n",
    "            mmlu_metrics_file_path=mmlu_metrics_file_path,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            batch_size=batch_size,\n",
    "            debug=debug,\n",
    "            compute_metrics_only=compute_metrics_only,\n",
    "            seed=seed,\n",
    "            keep_files=keep_files,\n",
    "        )\n",
    "\n",
    "        # Verify file paths exist\n",
    "        print(\"Verifying paths...\")\n",
    "        print(f\"  Data path: {data_path}\")\n",
    "        print(f\"  Checkpoint path: {checkpoint_path}\")\n",
    "        print(f\"  Output dir: {output_dir}\")\n",
    "        \n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"Data path not found: {data_path}\")\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Checkpoint path not found: {checkpoint_path}\")\n",
    "        if not os.path.exists(os.path.join(data_path, 'forget.jsonl')):\n",
    "            raise FileNotFoundError(f\"forget.jsonl not found in {data_path}\")\n",
    "        if not os.path.exists(os.path.join(data_path, 'retain.jsonl')):\n",
    "            raise FileNotFoundError(f\"retain.jsonl not found in {data_path}\")\n",
    "\n",
    "        # Normalize paths (as in the original script)\n",
    "        from pathlib import Path\n",
    "        if args.output_dir is None:\n",
    "            args.output_dir = os.getcwd()\n",
    "        else:\n",
    "            args.output_dir = args.output_dir.rstrip('/')\n",
    "            Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Run functions directly\n",
    "        import random, torch, numpy as np\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "\n",
    "        from accelerate import Accelerator\n",
    "        accelerator = Accelerator()\n",
    "\n",
    "        if not args.compute_metrics_only:\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "            from peft import PeftModel, LoraConfig\n",
    "            \n",
    "            print(f\"Loading model from {args.checkpoint_path}...\")\n",
    "            \n",
    "            # Load PEFT (LoRA) model if saved as adapter\n",
    "            try:\n",
    "                # Try to load as PEFT model first\n",
    "                base_model_path = MODEL_PATH  # Use the base model path\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    base_model_path, \n",
    "                    local_files_only=True,\n",
    "                    torch_dtype=torch.bfloat16\n",
    "                )\n",
    "                model = PeftModel.from_pretrained(base_model, args.checkpoint_path)\n",
    "                print(\"Loaded as PEFT model\")\n",
    "            except:\n",
    "                # If that fails, load as a regular model\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    args.checkpoint_path,\n",
    "                    torch_dtype=torch.bfloat16,\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "                print(\"Loaded as regular model\")\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_path)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            print(\"Starting inference...\")\n",
    "            evaluation.inference(args, model, tokenizer)\n",
    "            \n",
    "            # if args.mia_data_path is not None:\n",
    "                # print(\"🔍 Starting MIA attacks...\")\n",
    "                # evaluation.mia_attacks(args, model, tokenizer)\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            print(\"Computing metrics...\")\n",
    "            evaluation.compute_metrics(args)\n",
    "            print(\"Evaluation completed!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during evaluation:\", str(e))\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# === Step 4: Run evaluation ===\n",
    "print(\"Starting evaluation process...\")\n",
    "\n",
    "# Check files exist before starting\n",
    "if os.path.exists(\"validation/forget.jsonl\") and os.path.exists(\"validation/retain.jsonl\"):\n",
    "    if os.path.exists(\"studentmodel_best_val_sequential/\"):\n",
    "        run_evaluation(\n",
    "            data_path=\"validation/\",  # relative folder with forget.jsonl and retain.jsonl\n",
    "            checkpoint_path=\"studentmodel_best_val_sequential/\",  # cartella relativa con i pesi del modello\n",
    "            output_dir=\"eval_results\",\n",
    "            debug=False  # Attiva debug per vedere cosa succede\n",
    "        )\n",
    "    else:\n",
    "        print(\"Model checkpoint not found at balanced_results/balanced_model/\")\n",
    "        print(\"   Make sure the training completed successfully\")\n",
    "else:\n",
    "    print(\"Validation files not found\")\n",
    "    print(\"   Expected: validation/forget.jsonl and validation/retain.jsonl\")\n",
    "    print(\"   Make sure the data processing completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8051727,
     "sourceId": 12737770,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8153355,
     "sourceId": 12886974,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8155392,
     "sourceId": 12890024,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8155743,
     "sourceId": 12890625,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8159200,
     "sourceId": 12895681,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8162113,
     "sourceId": 12900193,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
