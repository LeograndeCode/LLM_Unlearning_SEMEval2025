{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selective Knowledge Negation Unlearning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:54:08.478341Z",
     "iopub.status.busy": "2025-08-16T15:54:08.478082Z",
     "iopub.status.idle": "2025-08-16T15:54:53.925665Z",
     "shell.execute_reply": "2025-08-16T15:54:53.924076Z",
     "shell.execute_reply.started": "2025-08-16T15:54:08.478323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install rouge-score transformers peft huggingface_hub pyarrow\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Configurazioni\n",
    "MODEL_PATH = \"semeval25-unlearning-1B-model\"\n",
    "\n",
    "print(f\"GPUs disponibili: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Caricamento Dati e Modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:54:53.928809Z",
     "iopub.status.busy": "2025-08-16T15:54:53.927770Z",
     "iopub.status.idle": "2025-08-16T15:54:55.468476Z",
     "shell.execute_reply": "2025-08-16T15:54:55.467556Z",
     "shell.execute_reply.started": "2025-08-16T15:54:53.928773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Helpers to load datasets from either JSONL or Parquet\n",
    "\n",
    "def load_table(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File non trovato: {path}\")\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in [\".jsonl\", \".json\"]:\n",
    "        return pd.read_json(path, lines=True)\n",
    "    if ext in [\".parquet\"]:\n",
    "        # requires pyarrow or fastparquet\n",
    "        return pd.read_parquet(path)\n",
    "    raise ValueError(f\"Formato file non supportato: {ext}\")\n",
    "\n",
    "# Canonical schema\n",
    "schema_cols = [\"input\", \"output\", \"split\"]\n",
    "\n",
    "# Try to coerce to expected schema\n",
    "def ensure_schema(df: pd.DataFrame, path: str) -> pd.DataFrame:\n",
    "    # map common alt column names\n",
    "    input_candidates = [\"input\", \"prompt\", \"question\", \"instruction\", \"query\", \"source\", \"text\"]\n",
    "    output_candidates = [\"output\", \"answer\", \"response\", \"target\", \"completion\"]\n",
    "\n",
    "    # choose first available\n",
    "    in_col = next((c for c in input_candidates if c in df.columns), None)\n",
    "    out_col = next((c for c in output_candidates if c in df.columns), None)\n",
    "\n",
    "    df2 = pd.DataFrame()\n",
    "    df2[\"input\"] = df[in_col] if in_col else np.nan\n",
    "    df2[\"output\"] = df[out_col] if out_col else np.nan\n",
    "\n",
    "    # split: keep if exists, else infer from filename, else retain\n",
    "    if \"split\" in df.columns:\n",
    "        df2[\"split\"] = df[\"split\"]\n",
    "    else:\n",
    "        base = os.path.basename(path).lower()\n",
    "        if \"forget\" in base:\n",
    "            df2[\"split\"] = \"forget\"\n",
    "        else:\n",
    "            df2[\"split\"] = \"retain\"\n",
    "\n",
    "    # ensure types are strings where relevant\n",
    "    df2[\"input\"] = df2[\"input\"].astype(str)\n",
    "    df2[\"output\"] = df2[\"output\"].astype(str)\n",
    "    return df2[schema_cols]\n",
    "\n",
    "# Caricamento dati\n",
    "retain_train_path = 'train/retain.jsonl'  # se esiste\n",
    "forget_train_path = 'train/forget_train-00000-of-00001.parquet'  # nuovo formato\n",
    "retain_val_path = 'validation/retain.jsonl'\n",
    "forget_val_path = 'validation/forget.jsonl'\n",
    "\n",
    "# Carica con fallback: se un file non esiste, usa un DF vuoto coerente\n",
    "\n",
    "def safe_load(path):\n",
    "    try:\n",
    "        df = load_table(path)\n",
    "        df = ensure_schema(df, path)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame(columns=schema_cols)\n",
    "\n",
    "retain_train_df = safe_load(retain_train_path)\n",
    "forget_train_df = safe_load(forget_train_path)\n",
    "retain_validation_df = safe_load(retain_val_path)\n",
    "forget_validation_df = safe_load(forget_val_path)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Prefer right padding for causal LM efficiency\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "snapshot_download(repo_id='llmunlearningsemeval2025organization/olmo-1B-model-semeval25-unlearning', local_dir='semeval25-unlearning-1B-model')\n",
    "\n",
    "print(\"Dataset salvati e tokenizer caricato\")\n",
    "print({\n",
    "    'retain_train': len(retain_train_df),\n",
    "    'forget_train': len(forget_train_df),\n",
    "    'retain_validation': len(retain_validation_df),\n",
    "    'forget_validation': len(forget_validation_df),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check lettura\n",
    "print(\"Train (retain, forget) sizes:\", len(retain_train_df), len(forget_train_df))\n",
    "print(\"Columns:\", list(train_data.columns))\n",
    "print(train_data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:54:55.469690Z",
     "iopub.status.busy": "2025-08-16T15:54:55.469467Z",
     "iopub.status.idle": "2025-08-16T15:54:55.478117Z",
     "shell.execute_reply": "2025-08-16T15:54:55.477328Z",
     "shell.execute_reply.started": "2025-08-16T15:54:55.469667Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UnlearningDataset(Dataset):\n",
    "    def __init__(self, data_source, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if isinstance(data_source, pd.DataFrame):\n",
    "            self.data = data_source\n",
    "            print(f\"Caricati {len(self.data)} esempi dal DataFrame\")\n",
    "        elif isinstance(data_source, str):\n",
    "            # Supporto a file .jsonl e .parquet\n",
    "            ext = os.path.splitext(data_source)[1].lower()\n",
    "            if ext in [\".jsonl\", \".json\"]:\n",
    "                data_list = []\n",
    "                with open(data_source, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        item = json.loads(line.strip())\n",
    "                        data_list.append(item)\n",
    "                self.data = pd.DataFrame(data_list)\n",
    "            elif ext in [\".parquet\"]:\n",
    "                self.data = pd.read_parquet(data_source)\n",
    "            else:\n",
    "                raise ValueError(f\"Formato file non supportato: {ext}\")\n",
    "            # Normalizza colonne necessarie\n",
    "            for col in [\"input\", \"output\", \"split\"]:\n",
    "                if col not in self.data.columns:\n",
    "                    self.data[col] = np.nan\n",
    "            print(f\"Caricati {len(self.data)} esempi da {data_source}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        prompt_text = str(self.data.iloc[idx][\"input\"])  # coerce to string\n",
    "        answer_raw = self.data.iloc[idx][\"output\"]\n",
    "        answer_text = str(answer_raw)\n",
    "\n",
    "        # Tokenize prompt and answer separately to get a robust boundary\n",
    "        prompt_tok = self.tokenizer(\n",
    "            prompt_text,\n",
    "            add_special_tokens=False,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        # Ensure a leading space for the answer for stable tokenization after the prompt\n",
    "        answer_text_sp = answer_text if answer_text.startswith(\" \") else (\" \" + answer_text)\n",
    "        answer_tok = self.tokenizer(\n",
    "            answer_text_sp,\n",
    "            add_special_tokens=False,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "\n",
    "        prompt_ids = prompt_tok[\"input_ids\"]\n",
    "        answer_ids = answer_tok[\"input_ids\"]\n",
    "\n",
    "        # Compute how many answer tokens survive after truncation\n",
    "        prompt_len = len(prompt_ids)\n",
    "        ans_len = len(answer_ids)\n",
    "        if prompt_len >= self.max_length:\n",
    "            # No room for answer tokens\n",
    "            input_ids = prompt_ids[: self.max_length]\n",
    "            answer_len_kept = 0\n",
    "        else:\n",
    "            available = self.max_length - prompt_len\n",
    "            answer_len_kept = min(ans_len, max(0, available))\n",
    "            input_ids = prompt_ids + answer_ids[:answer_len_kept]\n",
    "        \n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        labels = list(input_ids)\n",
    "\n",
    "        # Start index of the first answer token (before shift)\n",
    "        ans_start = prompt_len\n",
    "        start_locs = min(ans_start, len(input_ids) - 1)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"start_locs\": start_locs,\n",
    "            \"answer_len_kept\": int(answer_len_kept),\n",
    "            \"labels\": labels,\n",
    "            \"split\": 1 if self.data.iloc[idx][\"split\"] == \"forget\" else 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:54:55.480221Z",
     "iopub.status.busy": "2025-08-16T15:54:55.480021Z",
     "iopub.status.idle": "2025-08-16T15:54:55.520835Z",
     "shell.execute_reply": "2025-08-16T15:54:55.520099Z",
     "shell.execute_reply.started": "2025-08-16T15:54:55.480205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "# Reduce max_length slightly to lower memory footprint\n",
    "batch_size = 2\n",
    "\n",
    "# Concatena train retain + forget (Parquet)\n",
    "train_data = pd.concat([retain_train_df, forget_train_df], ignore_index=True)\n",
    "\n",
    "# Filtra righe prive di input/output\n",
    "train_data = train_data.dropna(subset=[\"input\", \"output\"]).reset_index(drop=True)\n",
    "\n",
    "dataset = UnlearningDataset(train_data, tokenizer, max_length=384)\n",
    "\n",
    "# Dynamic padding collate_fn\n",
    "def sku_collate_fn(batch, pad_id, max_length=384):\n",
    "    bs = len(batch)\n",
    "    lengths = [min(len(item['input_ids']), max_length) for item in batch]\n",
    "    max_len = max(lengths) if lengths else 1\n",
    "    input_ids = torch.full((bs, max_len), pad_id, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((bs, max_len), dtype=torch.long)\n",
    "    labels = torch.full((bs, max_len), -100, dtype=torch.long)\n",
    "    start_locs = []\n",
    "    answer_lens = []\n",
    "    splits = []\n",
    "    for i, item in enumerate(batch):\n",
    "        ids = item['input_ids'][:max_length]\n",
    "        am = item['attention_mask'][:max_length]\n",
    "        input_ids[i, :len(ids)] = torch.tensor(ids)\n",
    "        attention_mask[i, :len(am)] = torch.tensor(am)\n",
    "        labels[i, :len(ids)] = torch.tensor(item['labels'][:max_length])\n",
    "        start_locs.append(min(item['start_locs'], max_length - 1))\n",
    "        answer_lens.append(min(item['answer_len_kept'], max_length))\n",
    "        splits.append(item['split'])\n",
    "    batch_out = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "        'start_locs': torch.tensor(start_locs, dtype=torch.long),\n",
    "        'answer_len_kept': torch.tensor(answer_lens, dtype=torch.long),\n",
    "        'split': torch.tensor(splits, dtype=torch.long)\n",
    "    }\n",
    "    return batch_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selective Knowledge Negation Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectiveKnowledgeNegationTrainer:\n",
    "    \"\"\"\n",
    "    Trainer implementing Selective Knowledge Negation Unlearning (SKU).\n",
    "\n",
    "    Core idea:\n",
    "    - For retain samples: optimize the standard language modeling loss (cross-entropy) to preserve knowledge.\n",
    "    - For forget samples: minimize the probability of producing the forbidden answer tokens via token-level\n",
    "      unlikelihood loss on the answer span while still keeping CE on the prompt context to stabilize training.\n",
    "\n",
    "    Added enhancements:\n",
    "    - L2 anchoring of trainable weights to their initial values (helps preserve general knowledge).\n",
    "    - Entropy regularization on the forget answer span (makes distribution flat to reduce memorization).\n",
    "    - Optional refusal-target CE on forget answer span to steer toward a safe response template.\n",
    "    - Cosine LR scheduler with warmup and unlikelihood ramp-up for stable training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path, tokenizer, lora_config, device=\"cuda:0\", refusal_text: str = \" I cannot comply with that request.\"):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lora_config = lora_config\n",
    "        self.device = device\n",
    "        self.refusal_text = refusal_text\n",
    "\n",
    "        self.model = None\n",
    "        self.initial_state_dict = {}\n",
    "\n",
    "    def _count_trainable(self, model):\n",
    "        total = sum(p.numel() for p in model.parameters())\n",
    "        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        return trainable, total\n",
    "\n",
    "    def setup_model(self):\n",
    "        print(\"üîß Setting up model (LoRA)...\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_path,\n",
    "            local_files_only=True\n",
    "        )\n",
    "\n",
    "        model = get_peft_model(base_model, self.lora_config).to(self.device)\n",
    "        \n",
    "        # Disable KV cache during training to save memory\n",
    "        try:\n",
    "            model.config.use_cache = False\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "        # Report trainables and snapshot initial trainable params\n",
    "        try:\n",
    "            self.model.print_trainable_parameters()\n",
    "        except Exception:\n",
    "            trainable, total = self._count_trainable(self.model)\n",
    "            print(f\"Trainable params: {trainable} / {total}\")\n",
    "        # Warn if still zero\n",
    "        tcount, _ = self._count_trainable(self.model)\n",
    "        if tcount == 0:\n",
    "            print(\"‚ùóNo trainable parameters detected. Training will be a no-op and backward will be skipped. Check target_modules.\")\n",
    "        self.initial_state_dict.clear()\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.initial_state_dict[name] = p.data.clone()\n",
    "        print(\"‚úÖ Model setup completed\")\n",
    "\n",
    "    def _compute_span_masks_targets_precise(self, attention_mask, start_locs, answer_len_kept):\n",
    "        B, T = attention_mask.shape\n",
    "        device = attention_mask.device\n",
    "        prompt_mask_tgt = torch.zeros((B, T), dtype=torch.bool, device=device)\n",
    "        answer_mask_tgt = torch.zeros((B, T), dtype=torch.bool, device=device)\n",
    "        for i in range(B):\n",
    "            s = int(start_locs[i].item()) if torch.is_tensor(start_locs[i]) else int(start_locs[i])\n",
    "            s = max(0, min(s, T - 1))\n",
    "            L = int(answer_len_kept[i].item()) if torch.is_tensor(answer_len_kept[i]) else int(answer_len_kept[i])\n",
    "            # shift-to-target alignment: target indices correspond to positions 1..T-1\n",
    "            split_t = s - 1\n",
    "            if split_t >= 0:\n",
    "                # Prompt is [0 .. split_t-1]\n",
    "                if split_t > 0:\n",
    "                    prompt_mask_tgt[i, :split_t] = True\n",
    "                # Answer is [split_t .. split_t+L-1], clipped to T\n",
    "                if L > 0:\n",
    "                    end_pos = min(T - 1, split_t + L - 1)\n",
    "                    answer_mask_tgt[i, split_t : end_pos + 1] = True\n",
    "            else:\n",
    "                # No prompt tokens; all start as answer, but limit to L\n",
    "                if L > 0:\n",
    "                    end_pos = min(T - 1, L - 1)\n",
    "                    answer_mask_tgt[i, : end_pos + 1] = True\n",
    "        return prompt_mask_tgt, answer_mask_tgt\n",
    "\n",
    "    def _cross_entropy_loss(self, logits, labels, loss_mask):\n",
    "        vocab = logits.size(-1)\n",
    "        # Use reshape instead of view to safely handle non-contiguous tensors\n",
    "        logits_flat = logits.reshape(-1, vocab)\n",
    "        labels_flat = labels.reshape(-1)\n",
    "        mask_flat = loss_mask.reshape(-1)\n",
    "        if mask_flat.sum() == 0:\n",
    "            # Return a zero that's attached to the current graph if logits require grad; else plain zero\n",
    "            return (logits.sum() * 0.0) if logits.requires_grad else torch.tensor(0.0, device=logits.device)\n",
    "        return F.cross_entropy(\n",
    "            logits_flat[mask_flat],\n",
    "            labels_flat[mask_flat],\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "\n",
    "    def _unlikelihood_loss(self, logits, labels, loss_mask):\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        y = labels.unsqueeze(-1)\n",
    "        p_y = torch.gather(probs, dim=-1, index=y).squeeze(-1)\n",
    "        eps = 1e-6\n",
    "        p_y = p_y.clamp_min(eps).clamp_max(1 - eps)\n",
    "        ul = -torch.log(1.0 - p_y)\n",
    "        # Exclude pad/eos targets from UL (noisy gradients)\n",
    "        pad_id = self.tokenizer.pad_token_id or self.tokenizer.eos_token_id\n",
    "        eos_id = self.tokenizer.eos_token_id\n",
    "        valid_tokens = (labels != pad_id) & (labels != -100)\n",
    "        if eos_id is not None:\n",
    "            valid_tokens = valid_tokens & (labels != eos_id)\n",
    "        ul = ul[loss_mask & valid_tokens]\n",
    "        if ul.numel() == 0:\n",
    "            # Return a zero that's attached to the current graph if logits require grad; else plain zero\n",
    "            return (logits.sum() * 0.0) if logits.requires_grad else torch.tensor(0.0, device=logits.device)\n",
    "        return ul.mean()\n",
    "\n",
    "    def _entropy_on_mask(self, logits, loss_mask):\n",
    "        # Computes mean entropy H(p) over masked positions\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        eps = 1e-8\n",
    "        ent = -(probs * (probs.clamp_min(eps).log())).sum(dim=-1)\n",
    "        ent = ent[loss_mask]\n",
    "        if ent.numel() == 0:\n",
    "            return (logits.sum() * 0.0) if logits.requires_grad else torch.tensor(0.0, device=logits.device)\n",
    "        return ent.mean()\n",
    "\n",
    "    def _l2_anchor(self):\n",
    "        if not self.initial_state_dict:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "        total = None\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if p.requires_grad and name in self.initial_state_dict:\n",
    "                diff = p - self.initial_state_dict[name].to(p.device, dtype=p.dtype)\n",
    "                term = (diff * diff).sum()\n",
    "                total = term if total is None else total + term\n",
    "        if total is None:\n",
    "            total = torch.tensor(0.0, device=self.device)\n",
    "        return total / max(1, len(self.initial_state_dict))\n",
    "\n",
    "    def _build_refusal_targets(self, answer_mask_tgt, forget_mask, Tm1):\n",
    "        # answer_mask_tgt, forget_mask: [B, T-1] bool\n",
    "        B = answer_mask_tgt.size(0)\n",
    "        device = answer_mask_tgt.device\n",
    "        # Tokenize refusal template once\n",
    "        refusal_ids = self.tokenizer(\n",
    "            self.refusal_text,\n",
    "            add_special_tokens=False,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=Tm1,\n",
    "            return_tensors=None,\n",
    "        )[\"input_ids\"]\n",
    "        if len(refusal_ids) == 0:\n",
    "            # Fallback to EOS\n",
    "            rid = self.tokenizer.eos_token_id\n",
    "            refusal_ids = [rid if rid is not None else 0]\n",
    "        R = len(refusal_ids)\n",
    "        # Prepare targets of shape [B, T-1] filled with -100\n",
    "        target_ref = torch.full((B, Tm1), -100, dtype=torch.long, device=device)\n",
    "        # For each sample, fill the answer span positions with repeated refusal ids\n",
    "        idxs = torch.arange(Tm1, device=device)\n",
    "        for i in range(B):\n",
    "            mask = (answer_mask_tgt[i] & forget_mask[i])\n",
    "            L = int(mask.sum().item())\n",
    "            if L <= 0:\n",
    "                continue\n",
    "            # Repeat/trim the refusal ids to L\n",
    "            seq = (refusal_ids * ((L + R - 1) // R))[:L]\n",
    "            target_positions = idxs[mask]\n",
    "            target_ref[i, target_positions] = torch.tensor(seq, dtype=torch.long, device=device)\n",
    "        return target_ref\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        dataloader,\n",
    "        num_epochs=4,\n",
    "        lr=1e-4,\n",
    "        ce_weight_prompt=1.0,\n",
    "        ul_weight_answer=1.0,\n",
    "        ce_weight_retain=1.0,\n",
    "        l2_anchor_weight=1e-5,\n",
    "        entropy_weight_answer=0.0,\n",
    "        refusal_weight=0.0,\n",
    "        grad_clip=1.0,\n",
    "        grad_accum_steps=1,\n",
    "        use_mixed_precision=True,\n",
    "        warmup_ratio=0.1,\n",
    "        ul_ramp_ratio=0.2,\n",
    "    ):\n",
    "        assert self.model is not None, \"Call setup_model() first\"\n",
    "        self.model.train()\n",
    "        # Optimizer: prefer 8-bit if available\n",
    "        optimizer = None\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "            optimizer = bnb.optim.PagedAdamW8bit(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "            print(\"üßÆ Using 8-bit PagedAdamW optimizer (bitsandbytes)\")\n",
    "        except Exception:\n",
    "            try:\n",
    "                optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01, fused=True)\n",
    "                print(\"üßÆ Using fused AdamW optimizer\")\n",
    "            except Exception:\n",
    "                optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "                print(\"üßÆ Using standard AdamW optimizer\")\n",
    "\n",
    "        # Scheduler with warmup and cosine decay\n",
    "        steps_per_epoch = max(1, (len(dataloader) + grad_accum_steps - 1) // grad_accum_steps)\n",
    "        total_steps = num_epochs * steps_per_epoch\n",
    "        warmup_steps = max(1, int(warmup_ratio * total_steps))\n",
    "        ul_ramp_steps = max(1, int(ul_ramp_ratio * total_steps))\n",
    "        try:\n",
    "            from transformers import get_cosine_schedule_with_warmup\n",
    "            scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "            print(f\"üìà Using cosine scheduler with warmup ({warmup_steps}/{total_steps})\")\n",
    "        except Exception:\n",
    "            def lr_lambda(step):\n",
    "                if step < warmup_steps:\n",
    "                    return float(step) / float(max(1, warmup_steps))\n",
    "                progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "                # Cosine from 1 to 0\n",
    "                return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "            print(f\"üìà Using LambdaLR scheduler with warmup ({warmup_steps}/{total_steps})\")\n",
    "\n",
    "        # Mixed precision setup\n",
    "        from contextlib import nullcontext\n",
    "        device_type = 'cuda' if ('cuda' in str(self.device) and torch.cuda.is_available()) else ('mps' if ('mps' in str(self.device) and torch.backends.mps.is_available()) else None)\n",
    "        param_dtype = next(self.model.parameters()).dtype if any(p.requires_grad for p in self.model.parameters()) else torch.float32\n",
    "        use_amp = use_mixed_precision and device_type is not None\n",
    "        amp_dtype = torch.bfloat16 if (device_type == 'cuda' and param_dtype == torch.bfloat16) else torch.float16\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=(device_type == 'cuda' and amp_dtype == torch.float16 and use_amp))\n",
    "        \n",
    "        global_step = 0\n",
    "        no_grad_batches = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_losses = []\n",
    "            with tqdm(total=len(dataloader), desc=f\"SKU Epoch {epoch+1}\") as pbar:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                for step, batch in enumerate(dataloader):\n",
    "                    input_ids = batch[\"input_ids\"].to(self.device, non_blocking=True)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(self.device, non_blocking=True)\n",
    "                    labels_full = batch[\"labels\"].to(self.device, non_blocking=True)\n",
    "                    start_locs = batch[\"start_locs\"].to(self.device, non_blocking=True)\n",
    "                    answer_len_kept = batch[\"answer_len_kept\"].to(self.device, non_blocking=True)\n",
    "                    split = batch[\"split\"].to(self.device, non_blocking=True)\n",
    "\n",
    "                    with (torch.autocast(device_type=device_type, dtype=amp_dtype) if use_amp else nullcontext()):\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            use_cache=False,\n",
    "                            return_dict=True,\n",
    "                        )\n",
    "                        logits_full = outputs.logits  # [B, T, V]\n",
    "                        logits = logits_full[:, :-1, :]\n",
    "                        target = labels_full[:, 1:]\n",
    "                        attn_tgt = attention_mask[:, 1:].bool()\n",
    "                        # Precise masks using kept answer length\n",
    "                        prompt_mask_tgt, answer_mask_tgt = self._compute_span_masks_targets_precise(attention_mask, start_locs, answer_len_kept)\n",
    "                        prompt_mask_tgt = prompt_mask_tgt[:, :-1]\n",
    "                        answer_mask_tgt = answer_mask_tgt[:, :-1]\n",
    "                        retain_mask = (split == 0).unsqueeze(-1).expand_as(prompt_mask_tgt)\n",
    "                        forget_mask = (split == 1).unsqueeze(-1).expand_as(prompt_mask_tgt)\n",
    "                        valid_tgt = attn_tgt\n",
    "\n",
    "                        # Core losses\n",
    "                        retain_loss = self._cross_entropy_loss(\n",
    "                            logits, target, loss_mask=(valid_tgt & retain_mask)\n",
    "                        ) * ce_weight_retain\n",
    "                        forget_prompt_loss = self._cross_entropy_loss(\n",
    "                            logits, target, loss_mask=(valid_tgt & forget_mask & prompt_mask_tgt)\n",
    "                        ) * ce_weight_prompt\n",
    "\n",
    "                        # Ramp-up UL weight\n",
    "                        ul_scale = min(1.0, float(global_step + 1) / float(max(1, ul_ramp_steps)))\n",
    "                        forget_ul_loss = self._unlikelihood_loss(\n",
    "                            logits, target, loss_mask=(valid_tgt & forget_mask & answer_mask_tgt)\n",
    "                        ) * (ul_weight_answer * ul_scale)\n",
    "\n",
    "                        # Entropy regularization on forget answer span (maximize entropy)\n",
    "                        ent_loss = self._entropy_on_mask(\n",
    "                            logits, loss_mask=(valid_tgt & forget_mask & answer_mask_tgt)\n",
    "                        )\n",
    "                        entropy_term = -entropy_weight_answer * ent_loss\n",
    "\n",
    "                        # Optional refusal-target CE on forget answer span\n",
    "                        refusal_term = torch.tensor(0.0, device=logits.device)\n",
    "                        if refusal_weight > 0.0:\n",
    "                            Tm1 = logits.size(1)\n",
    "                            target_ref = self._build_refusal_targets(answer_mask_tgt, forget_mask, Tm1)\n",
    "                            refusal_term = self._cross_entropy_loss(\n",
    "                                logits, target_ref, loss_mask=(valid_tgt & forget_mask & answer_mask_tgt)\n",
    "                            ) * refusal_weight\n",
    "\n",
    "                        # L2 anchor on weights\n",
    "                        l2_term = self._l2_anchor() * l2_anchor_weight\n",
    "\n",
    "                        loss = retain_loss + forget_prompt_loss + forget_ul_loss + entropy_term + refusal_term + l2_term\n",
    "                        loss_for_backward = loss / max(1, int(grad_accum_steps))\n",
    "\n",
    "                    if not loss_for_backward.requires_grad:\n",
    "                        no_grad_batches += 1\n",
    "                        pbar.set_postfix({\"Loss\": f\"{float(loss.detach().cpu()):.4f}\", \"note\": \"no-grad-batch\"})\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                    if scaler.is_enabled():\n",
    "                        scaler.scale(loss_for_backward).backward()\n",
    "                    else:\n",
    "                        loss_for_backward.backward()\n",
    "\n",
    "                    do_step = ((step + 1) % grad_accum_steps == 0) or (step + 1 == len(dataloader))\n",
    "                    if do_step:\n",
    "                        if scaler.is_enabled():\n",
    "                            scaler.unscale_(optimizer)\n",
    "                        if grad_clip is not None:\n",
    "                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n",
    "                        if scaler.is_enabled():\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                        else:\n",
    "                            optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad(set_to_none=True)\n",
    "                        global_step += 1\n",
    "\n",
    "                    epoch_losses.append(float(loss.detach().cpu()))\n",
    "                    pbar.set_postfix({\n",
    "                        \"Loss\": f\"{float(loss.detach().cpu()):.4f}\",\n",
    "                        \"RetCE\": f\"{float(retain_loss.detach().cpu()):.3f}\",\n",
    "                        \"FgtCE\": f\"{float(forget_prompt_loss.detach().cpu()):.3f}\",\n",
    "                        \"FgtUL\": f\"{float(forget_ul_loss.detach().cpu()):.3f}\",\n",
    "                        \"Ent\": f\"{float(entropy_term.detach().cpu()):.3f}\",\n",
    "                        \"Ref\": f\"{float(refusal_term.detach().cpu()):.3f}\",\n",
    "                        \"L2\": f\"{float(l2_term.detach().cpu()):.3f}\",\n",
    "                    })\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    # Proactive cleanup to prevent fragmentation\n",
    "                    del outputs, logits_full, logits, target, attn_tgt, prompt_mask_tgt, answer_mask_tgt, retain_mask, forget_mask, valid_tgt\n",
    "                    if device_type == 'cuda' and ((step + 1) % 50 == 0):\n",
    "                        torch.cuda.empty_cache()\n",
    "            if device_type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            avg_loss = float(np.mean(epoch_losses)) if epoch_losses else 0.0\n",
    "            if no_grad_batches:\n",
    "                print(f\"‚ÑπÔ∏è Epoch {epoch+1}: skipped {no_grad_batches} batches with no grad signal (check trainable params)\")\n",
    "                no_grad_batches = 0\n",
    "            print(f\"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    def save_model(self, output_dir: str):\n",
    "        \"\"\"Save LoRA adapters (preferred) or fallback to saving base model weights.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        try:\n",
    "            # If using PEFT, this saves the adapter weights\n",
    "            self.model.save_pretrained(output_dir)\n",
    "            print(f\"üíæ Saved PEFT adapters to {output_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not save PEFT adapters directly: {e}\")\n",
    "            try:\n",
    "                # Fallback: try saving base model\n",
    "                if hasattr(self.model, \"base_model\"):\n",
    "                    self.model.base_model.save_pretrained(output_dir)\n",
    "                    print(f\"üíæ Saved base model to {output_dir}\")\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Failed to save model: {e2}\")\n",
    "\n",
    "    def calculate_task_vector(self):\n",
    "        \"\"\"Compute delta between current trainable params and their initial snapshot.\"\"\"\n",
    "        delta = {}\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if p.requires_grad and name in self.initial_state_dict:\n",
    "                delta[name] = (p.data - self.initial_state_dict[name]).detach().cpu()\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Trainer and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:54:55.548798Z",
     "iopub.status.busy": "2025-08-16T15:54:55.548454Z",
     "iopub.status.idle": "2025-08-16T15:55:48.528242Z",
     "shell.execute_reply": "2025-08-16T15:55:48.527344Z",
     "shell.execute_reply.started": "2025-08-16T15:54:55.548774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configure LoRA (single model)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    # Use a safe default to attach adapters to all Linear layers\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "\n",
    "# Initialize SKU trainer\n",
    "device_sel = \"cuda:0\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "sku_trainer = SelectiveKnowledgeNegationTrainer(\n",
    "    model_path=MODEL_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    lora_config=lora_config,\n",
    "    device=device_sel,\n",
    ")\n",
    "\n",
    "# Setup model\n",
    "sku_trainer.setup_model()\n",
    "\n",
    "# Guard: ensure there are trainable parameters\n",
    "trainable_params = sum(p.numel() for p in sku_trainer.model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameter count: {trainable_params}\")\n",
    "assert trainable_params > 0, \"No trainable parameters found; LoRA did not attach.\"\n",
    "\n",
    "# Optional: improve stability/perf\n",
    "if hasattr(sku_trainer.model, \"gradient_checkpointing_enable\"):\n",
    "    try:\n",
    "        sku_trainer.model.gradient_checkpointing_enable()\n",
    "    except Exception:\n",
    "        pass\n",
    "try:\n",
    "    sku_trainer.model = sku_trainer.model.to(dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:55:48.529500Z",
     "iopub.status.busy": "2025-08-16T15:55:48.529203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train SKU model\n",
    "sku_trainer.train(\n",
    "    dataloader=dataloader,\n",
    "    num_epochs=6,\n",
    "    lr=1e-4,\n",
    "    ce_weight_prompt=0.5,        # reduce to avoid reinforcing harmful answers\n",
    "    ul_weight_answer=7.0,        # stronger UL\n",
    "    ce_weight_retain=1.0,        # preserve retain knowledge\n",
    "    l2_anchor_weight=0,       # gentle anchor\n",
    "    entropy_weight_answer=0.05,  # push to higher entropy on forbidden span\n",
    "    refusal_weight=0.3,          # steer toward refusal template\n",
    "    grad_clip=1.0,\n",
    "    grad_accum_steps=4,          # keep effective batch size\n",
    "    use_mixed_precision=True,    # enable autocast\n",
    "    warmup_ratio=0.08,\n",
    "    ul_ramp_ratio=0.15,\n",
    ")\n",
    "\n",
    "# Quick delta check: task-vector L2 norm\n",
    "with torch.no_grad():\n",
    "    delta = sku_trainer.calculate_task_vector()\n",
    "    total_norm = 0.0\n",
    "    for t in delta.values():\n",
    "        total_norm += float(t.float().pow(2).sum().sqrt())\n",
    "    print(f\"Œî (task-vector) total L2 norm: {total_norm:.4f}\")\n",
    "\n",
    "# Save an intermediate checkpoint\n",
    "sku_trainer.save_model(\"sku_model_epoch_last\")\n",
    "\n",
    "# Optional quick A/B: generate one forget sample before vs after adapters to verify effect\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    from peft import PeftModel\n",
    "    prompt = forget_validation_df.iloc[0]['input']\n",
    "    base_tok = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "    if base_tok.pad_token is None:\n",
    "        base_tok.pad_token = base_tok.eos_token\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "    inputs = base_tok(prompt, return_tensors='pt').to(base_model.device)\n",
    "    with torch.no_grad():\n",
    "        out_base = base_model.generate(**inputs, max_new_tokens=64)\n",
    "    txt_base = base_tok.decode(out_base[0], skip_special_tokens=True)\n",
    "\n",
    "    adapted = PeftModel.from_pretrained(base_model, \"sku_model_epoch_last\")\n",
    "    with torch.no_grad():\n",
    "        out_adapt = adapted.generate(**inputs, max_new_tokens=64)\n",
    "    txt_adapt = base_tok.decode(out_adapt[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"--- A/B Quick Check ---\")\n",
    "    print(\"Prompt:\", prompt)\n",
    "    print(\"Base  :\", txt_base)\n",
    "    print(\"Adapt :\", txt_adapt)\n",
    "except Exception as e:\n",
    "    print(f\"A/B quick check skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results and Task Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "os.makedirs('balanced_results', exist_ok=True)\n",
    "\n",
    "# Save SKU model\n",
    "sku_trainer.save_model('balanced_results/balanced_model')\n",
    "\n",
    "# Calculate and save task vector\n",
    "task_vector = sku_trainer.calculate_task_vector()\n",
    "torch.save(task_vector, 'balanced_results/task_vector.pt')\n",
    "\n",
    "print(\"‚úÖ Results saved in balanced_results/\")\n",
    "print(\"- balanced_model/: SKU-trained model\")\n",
    "print(\"- task_vector.pt: Task vector for future applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "try:\n",
    "    import evaluation\n",
    "    import importlib\n",
    "    importlib.reload(evaluation)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def run_evaluation(\n",
    "    data_path,\n",
    "    checkpoint_path,\n",
    "    output_dir=\"eval_results\",\n",
    "    mia_data_path=None,\n",
    "    mmlu_metrics_file_path=None,\n",
    "    max_new_tokens=256,\n",
    "    batch_size=25,\n",
    "    debug=False,\n",
    "    compute_metrics_only=False,\n",
    "    seed=42,\n",
    "    keep_files=False,\n",
    "):\n",
    "    try:\n",
    "        # Costruiamo un oggetto args simile a quello di argparse\n",
    "        args = types.SimpleNamespace(\n",
    "            data_path=data_path,\n",
    "            checkpoint_path=checkpoint_path,\n",
    "            output_dir=output_dir,\n",
    "            mia_data_path=mia_data_path,\n",
    "            mmlu_metrics_file_path=mmlu_metrics_file_path,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            batch_size=batch_size,\n",
    "            debug=debug,\n",
    "            compute_metrics_only=compute_metrics_only,\n",
    "            seed=seed,\n",
    "            keep_files=keep_files,\n",
    "        )\n",
    "\n",
    "        # Verifica che i file esistano\n",
    "        print(f\"üîç Verificando paths...\")\n",
    "        print(f\"  Data path: {data_path}\")\n",
    "        print(f\"  Checkpoint path: {checkpoint_path}\")\n",
    "        print(f\"  Output dir: {output_dir}\")\n",
    "        \n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"Data path not found: {data_path}\")\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Checkpoint path not found: {checkpoint_path}\")\n",
    "        if not os.path.exists(os.path.join(data_path, 'forget.jsonl')):\n",
    "            raise FileNotFoundError(f\"forget.jsonl not found in {data_path}\")\n",
    "        if not os.path.exists(os.path.join(data_path, 'retain.jsonl')):\n",
    "            raise FileNotFoundError(f\"retain.jsonl not found in {data_path}\")\n",
    "\n",
    "        # Normalizza i path (come nello script originale)\n",
    "        from pathlib import Path\n",
    "        if args.output_dir is None:\n",
    "            args.output_dir = os.getcwd()\n",
    "        else:\n",
    "            args.output_dir = args.output_dir.rstrip('/')\n",
    "            Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Lancia direttamente le funzioni\n",
    "        import random, torch, numpy as np\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "\n",
    "        from accelerate import Accelerator\n",
    "        accelerator = Accelerator()\n",
    "\n",
    "        if not args.compute_metrics_only:\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "            from peft import PeftModel\n",
    "            \n",
    "            print(f\"üì• Loading base model from {MODEL_PATH} and adapters from {args.checkpoint_path}...\")\n",
    "            \n",
    "            # Always build tokenizer from the base model to keep special tokens consistent\n",
    "            base_tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "            if base_tokenizer.pad_token is None:\n",
    "                base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "            # Load base model and then PEFT adapters\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_PATH, \n",
    "                local_files_only=True,\n",
    "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "            )\n",
    "            try:\n",
    "                model = PeftModel.from_pretrained(base_model, args.checkpoint_path)\n",
    "                print(\"‚úÖ Loaded base + PEFT adapters\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è PEFT load failed ({e}); trying to load as a regular model\")\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    args.checkpoint_path,\n",
    "                    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "\n",
    "            # Sanity: check adapters are present and active\n",
    "            has_peft = hasattr(model, 'peft_config') and len(getattr(model, 'peft_config', {})) > 0\n",
    "            print(f\"PEFT active: {has_peft}\")\n",
    "            if has_peft:\n",
    "                print(f\"Adapters: {list(model.peft_config.keys())}\")\n",
    "            model.eval()\n",
    "\n",
    "            print(\"üöÄ Starting inference...\")\n",
    "            evaluation.inference(args, model, base_tokenizer)\n",
    "            \n",
    "            if args.mia_data_path is not None:\n",
    "                print(\"üîç Starting MIA attacks...\")\n",
    "                evaluation.mia_attacks(args, model, base_tokenizer)\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            print(\"üìä Computing metrics...\")\n",
    "            evaluation.compute_metrics(args)\n",
    "            print(\"‚úÖ Evaluation completed!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# === Step 4: Esegui evaluation ===\n",
    "print(\"üéØ Starting evaluation process...\")\n",
    "\n",
    "# Verifica che i file esistano prima di iniziare\n",
    "if os.path.exists(\"validation/forget.jsonl\") and os.path.exists(\"validation/retain.jsonl\"):\n",
    "    if os.path.exists(\"balanced_results/balanced_model/\"):\n",
    "        run_evaluation(\n",
    "            data_path=\"validation/\",  # cartella relativa con forget.jsonl e retain.jsonl\n",
    "            checkpoint_path=\"balanced_results/balanced_model/\",  # cartella relativa con i pesi del modello\n",
    "            output_dir=\"eval_results\",\n",
    "            debug=True  # Attiva debug per vedere cosa succede\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚ùå Model checkpoint not found at balanced_results/balanced_model/\")\n",
    "        print(\"   Make sure the training completed successfully\")\n",
    "else:\n",
    "    print(\"‚ùå Validation files not found\")\n",
    "    print(\"   Expected: validation/forget.jsonl and validation/retain.jsonl\")\n",
    "    print(\"   Make sure the data processing completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch: safe unlikelihood loss to avoid CUDA index OOB in gather\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# This assumes the class SelectiveKnowledgeNegationTrainer is already defined above\n",
    "\n",
    "def _ul_safe(self, logits, labels, loss_mask):\n",
    "    \"\"\"Unlikelihood loss with safe indexing: ignore invalid targets and prevent OOB gather.\"\"\"\n",
    "    V = logits.size(-1)\n",
    "    device = logits.device\n",
    "    # Build validity mask for target indices\n",
    "    pad_id = self.tokenizer.pad_token_id or self.tokenizer.eos_token_id\n",
    "    eos_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    labels_long = labels.long()\n",
    "    valid_tokens = (labels_long >= 0) & (labels_long < V) & (labels_long != -100)\n",
    "    if pad_id is not None:\n",
    "        valid_tokens = valid_tokens & (labels_long != pad_id)\n",
    "    if eos_id is not None:\n",
    "        valid_tokens = valid_tokens & (labels_long != eos_id)\n",
    "\n",
    "    # Replace invalid indices with 0 to keep gather in-bounds; they will be masked out later\n",
    "    safe_idx = labels_long.clamp(min=0, max=V - 1)\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    p_y_all = torch.gather(probs, dim=-1, index=safe_idx.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    mask = (loss_mask & valid_tokens)\n",
    "    if mask.sum().item() == 0:\n",
    "        return (logits.sum() * 0.0) if logits.requires_grad else torch.tensor(0.0, device=device)\n",
    "\n",
    "    p_y = p_y_all[mask].clamp(1e-6, 1 - 1e-6)\n",
    "    ul = -torch.log(1.0 - p_y)\n",
    "    return ul.mean()\n",
    "\n",
    "# Monkey-patch the method on the class if present\n",
    "try:\n",
    "    SelectiveKnowledgeNegationTrainer._unlikelihood_loss = _ul_safe\n",
    "    print(\"‚úÖ Patched SelectiveKnowledgeNegationTrainer._unlikelihood_loss with safe implementation\")\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è Could not find SelectiveKnowledgeNegationTrainer to patch. Define the class first and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe LoRA re-init to ensure trainable adapters attach\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config_safe = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "\n",
    "# Recreate trainer to use the safe config\n",
    "sku_trainer = SelectiveKnowledgeNegationTrainer(\n",
    "    model_path=MODEL_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    lora_config=lora_config_safe,\n",
    "    device=(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")),\n",
    ")\n",
    "\n",
    "sku_trainer.setup_model()\n",
    "\n",
    "# Optional training stability tweaks\n",
    "if hasattr(sku_trainer.model, \"gradient_checkpointing_enable\"):\n",
    "    try:\n",
    "        sku_trainer.model.gradient_checkpointing_enable()\n",
    "    except Exception:\n",
    "        pass\n",
    "try:\n",
    "    sku_trainer.model = sku_trainer.model.to(dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"‚úÖ Trainer re-initialized with 'all-linear' LoRA target. Proceed to run the training cell again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8. Diagnostics & Debugging for SKU Effectiveness ===\n",
    "import torch, math\n",
    "from collections import Counter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"[Diagnostics] Starting SKU debugging block...\")\n",
    "\n",
    "# 1. Dataset split distribution & basic length stats\n",
    "if 'dataset' in globals():\n",
    "    split_counts = Counter(dataset.data['split']) if hasattr(dataset, 'data') else {}\n",
    "    print(\"Split counts:\", split_counts)\n",
    "    # Approx prompt / answer token stats (first 200 samples)\n",
    "    prompt_lens = []\n",
    "    answer_lens = []\n",
    "    for i in range(min(200, len(dataset))):\n",
    "        item = dataset[i]\n",
    "        prompt_lens.append(int(item['start_locs']))\n",
    "        answer_lens.append(int(item['answer_len_kept']))\n",
    "    if prompt_lens:\n",
    "        print(f\"Avg prompt tokens: {sum(prompt_lens)/len(prompt_lens):.1f} | Avg kept answer tokens: {sum(answer_lens)/len(answer_lens):.1f}\")\n",
    "        zero_ans = sum(1 for x in answer_lens if x == 0)\n",
    "        print(f\"Samples with 0 answer tokens kept: {zero_ans}/{len(answer_lens)} ({100*zero_ans/len(answer_lens):.1f}%)\")\n",
    "else:\n",
    "    print(\"Dataset not found in globals().\")\n",
    "\n",
    "# 2. Inspect one batch to see UL active positions\n",
    "if 'dataloader' in globals():\n",
    "    first_batch = next(iter(dataloader))\n",
    "    # Move minimal tensors\n",
    "    input_ids = first_batch['input_ids']\n",
    "    attention_mask = first_batch['attention_mask']\n",
    "    start_locs = first_batch['start_locs']\n",
    "    answer_len_kept = first_batch['answer_len_kept']\n",
    "    split = first_batch['split']\n",
    "    if 'sku_trainer' in globals():\n",
    "        with torch.no_grad():\n",
    "            prompt_mask_tgt, answer_mask_tgt = sku_trainer._compute_span_masks_targets_precise(attention_mask, start_locs, answer_len_kept)\n",
    "            # Align with target length (T-1)\n",
    "            prompt_mask_tgt = prompt_mask_tgt[:, :-1]\n",
    "            answer_mask_tgt = answer_mask_tgt[:, :-1]\n",
    "            attn_tgt = attention_mask[:, 1:].bool()\n",
    "            retain_mask = (split == 0).unsqueeze(-1).expand_as(prompt_mask_tgt)\n",
    "            forget_mask = (split == 1).unsqueeze(-1).expand_as(prompt_mask_tgt)\n",
    "            ul_active = (attn_tgt & forget_mask & answer_mask_tgt).sum().item()\n",
    "            forget_answer_tokens = (forget_mask & answer_mask_tgt).sum().item()\n",
    "            print(f\"UL active positions in sample batch: {ul_active}\")\n",
    "            print(f\"Total forget answer target positions in sample batch: {forget_answer_tokens}\")\n",
    "            if forget_answer_tokens == 0:\n",
    "                print(\"WARNING: No forget answer tokens available; unlikelihood loss will be zero. Consider increasing max_length or shortening prompts.\")\n",
    "    else:\n",
    "        print(\"sku_trainer not defined.\")\n",
    "else:\n",
    "    print(\"Dataloader not found.\")\n",
    "\n",
    "# 3. Function to compute log-prob of a sensitive answer span before & after adapters\n",
    "\n",
    "def compute_span_logprob(model, tokenizer, prompt, span_text, device=None):\n",
    "    device = device or (next(model.parameters()).device if any(p.requires_grad for p in model.parameters()) else 'cpu')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tok = tokenizer(prompt + span_text, return_tensors='pt')\n",
    "        for k in tok: tok[k] = tok[k].to(device)\n",
    "        outputs = model(**tok, use_cache=False, return_dict=True)\n",
    "        logits = outputs.logits # [1, T, V]\n",
    "        input_ids = tok['input_ids']\n",
    "        # We want log P(span | prompt). Identify boundary.\n",
    "        prompt_ids = tokenizer(prompt, add_special_tokens=False)['input_ids']\n",
    "        plen = len(prompt_ids)\n",
    "        # Shift for causal LM\n",
    "        target_ids = input_ids[:, 1:]  # next-token targets\n",
    "        logits_shifted = logits[:, :-1, :]\n",
    "        # Positions corresponding to span tokens\n",
    "        span_positions = list(range(plen, input_ids.size(1)-1))  # exclude last because of shift alignment\n",
    "        if not span_positions:\n",
    "            return float('nan'), 0\n",
    "        log_probs = torch.log_softmax(logits_shifted[0, span_positions, :], dim=-1)\n",
    "        tgt_tokens = target_ids[0, span_positions]\n",
    "        gathered = log_probs[range(len(span_positions)), tgt_tokens]\n",
    "        return gathered.sum().item(), len(span_positions)\n",
    "\n",
    "# 4. Compare probability of original answer phrase (greedy sensitive segment)\n",
    "try:\n",
    "    sensitive_prompt = forget_validation_df.iloc[0]['input']\n",
    "    # Use ground-truth output field as sensitive answer to suppress\n",
    "    sensitive_answer = forget_validation_df.iloc[0]['output']\n",
    "    base_tok = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "    if base_tok.pad_token is None: base_tok.pad_token = base_tok.eos_token\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, local_files_only=True, torch_dtype=torch.float32)\n",
    "    lp_base, Lspan = compute_span_logprob(base_model, base_tok, sensitive_prompt, sensitive_answer)\n",
    "    print(f\"Base span log-prob (sum over {Lspan} tokens): {lp_base:.2f}\")\n",
    "    adapted = None\n",
    "    from peft import PeftModel\n",
    "    if os.path.exists('sku_model_epoch_last'):\n",
    "        try:\n",
    "            adapted = PeftModel.from_pretrained(base_model, 'sku_model_epoch_last')\n",
    "        except Exception as e:\n",
    "            print(\"Could not load adapters from sku_model_epoch_last:\", e)\n",
    "    elif os.path.exists('balanced_results/balanced_model'):\n",
    "        try:\n",
    "            adapted = PeftModel.from_pretrained(base_model, 'balanced_results/balanced_model')\n",
    "        except Exception as e:\n",
    "            print(\"Could not load adapters from balanced_results/balanced_model:\", e)\n",
    "    if adapted is not None:\n",
    "        lp_adapt, _ = compute_span_logprob(adapted, base_tok, sensitive_prompt, sensitive_answer)\n",
    "        print(f\"Adapted span log-prob: {lp_adapt:.2f}\")\n",
    "        if math.isfinite(lp_base) and math.isfinite(lp_adapt):\n",
    "            delta = lp_adapt - lp_base\n",
    "            print(f\"Œî log-prob (adapted - base): {delta:.2f} (negative desired for forgetting)\")\n",
    "            if delta > -0.5:\n",
    "                print(\"Span probability not sufficiently reduced. Consider stronger ul_weight_answer, higher lr, or upsampling forget examples.\")\n",
    "    else:\n",
    "        print(\"No adapted model directory found for probability comparison.\")\n",
    "except Exception as e:\n",
    "    print(\"Span log-prob comparison skipped:\", e)\n",
    "\n",
    "# 5. Recommendations print based on quick heuristics\n",
    "print(\"\\n[Heuristic Recommendations]\")\n",
    "if 'split_counts' in locals() and split_counts:\n",
    "    total = sum(split_counts.values())\n",
    "    fgt = split_counts.get('forget', 0)\n",
    "    if fgt / max(1,total) < 0.2:\n",
    "        print(\"- Forget examples <20%: upsample forget or increase ul_weight_answer/refusal_weight.\")\n",
    "if 'answer_lens' in locals() and answer_lens:\n",
    "    if sum(1 for x in answer_lens if x==0) / len(answer_lens) > 0.3:\n",
    "        print(\"- Many samples lose answer tokens (truncation). Increase max_length or shorten prompts.\")\n",
    "print(\"- If Œî log-prob ~0, raise lr (e.g., 1e-4), set ul_weight_answer 6-8, set refusal_weight 0.3, temporarily disable l2_anchor.\")\n",
    "print(\"- Use sampling (top_p=0.9, temperature=0.8) for qualitative A/B instead of greedy only.\")\n",
    "print(\"[Diagnostics] Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8.1 Enhanced LoRA Re-init with Auto-Detected Linear Modules ===\n",
    "import torch, types\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "print(\"[Enhanced LoRA] Detecting linear submodules for targeted adaptation...\")\n",
    "base_tmp = AutoModelForCausalLM.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "linear_names = []\n",
    "for name, module in base_tmp.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        # Use the leaf module name\n",
    "        leaf = name.split('.')[-1]\n",
    "        linear_names.append(leaf)\n",
    "unique_linear = sorted(set(linear_names))\n",
    "print(f\"Found {len(unique_linear)} unique linear leaf names (showing first 25): {unique_linear[:25]}\")\n",
    "\n",
    "# Heuristic filter: keep typical projection/feed-forward names if they exist\n",
    "preferred = [n for n in unique_linear if any(k in n for k in [\"q\", \"k\", \"v\", \"o\", \"proj\", \"gate\", \"up\", \"down\", \"w1\", \"w2\", \"fc\", \"linear\"])]\n",
    "# Fallback to all unique linear names if filter becomes too small\n",
    "if len(preferred) < 4:\n",
    "    preferred = unique_linear\n",
    "print(f\"Using {len(preferred)} target module names for LoRA: {preferred}\")\n",
    "\n",
    "auto_lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,                 # larger rank for stronger capacity\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=preferred,\n",
    ")\n",
    "\n",
    "# Rebuild trainer with new config\n",
    "sku_trainer = SelectiveKnowledgeNegationTrainer(\n",
    "    model_path=MODEL_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    lora_config=auto_lora_config,\n",
    "    device=(\"cuda:0\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")),\n",
    ")\n",
    "sku_trainer.setup_model()\n",
    "\n",
    "# Force LoRA params to fp32 for better small-gradient resolution\n",
    "for n, p in sku_trainer.model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        p.data = p.data.float()\n",
    "\n",
    "print(\"[Enhanced LoRA] Trainable parameter count:\", sum(p.numel() for p in sku_trainer.model.parameters() if p.requires_grad))\n",
    "print(\"Proceed to run the enhanced training cell (8.2).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8.2 Enhanced Training Pass (Stronger Forgetting) ===\n",
    "from math import ceil\n",
    "import types\n",
    "\n",
    "print(\"[Enhanced Training] Ready.\")\n",
    "\n",
    "# Hyperparameters tuned for stronger unlearning pressure\n",
    "EPOCHS = 5\n",
    "LR = 1e-4\n",
    "UL_WEIGHT = 7.0\n",
    "REFUSAL_WEIGHT = 0.3\n",
    "CE_PROMPT_WEIGHT = 0.5\n",
    "CE_RETAIN_WEIGHT = 1.0\n",
    "L2_ANCHOR = 0.0\n",
    "ENTROPY_WEIGHT = 0.05\n",
    "GRAD_ACCUM = 4\n",
    "WARMUP = 0.08\n",
    "UL_RAMP = 0.15\n",
    "CURRICULUM_SWITCH = 2\n",
    "L2_AFTER = 5e-6\n",
    "FORGET_UPSAMPLE = 1.5\n",
    "\n",
    "if 'dataset' in globals():\n",
    "    import pandas as pd\n",
    "    base_df = dataset.data\n",
    "    forget_df = base_df[base_df['split'] == 'forget']\n",
    "    retain_df = base_df[base_df['split'] == 'retain']\n",
    "    if FORGET_UPSAMPLE > 1 and len(forget_df) > 0:\n",
    "        reps_int = int(FORGET_UPSAMPLE)\n",
    "        frac_part = FORGET_UPSAMPLE - reps_int\n",
    "        replicated = [forget_df]*reps_int\n",
    "        if frac_part > 1e-6:\n",
    "            replicated.append(forget_df.sample(frac=frac_part, replace=True, random_state=42))\n",
    "        extra_forget = pd.concat(replicated, ignore_index=True)\n",
    "        aug_df = pd.concat([retain_df, extra_forget], ignore_index=True)\n",
    "        print(f\"[Enhanced Training] Upsampled forget examples: {len(extra_forget)}\")\n",
    "    else:\n",
    "        aug_df = base_df\n",
    "    dataset = UnlearningDataset(aug_df, tokenizer, max_length=384)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: sku_collate_fn(b, pad_id, max_length=384),\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        num_workers=2 if torch.cuda.is_available() else 0,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    print(f\"[Enhanced Training] New dataset size {len(dataset)} | forget {sum(dataset.data['split']=='forget')} | retain {sum(dataset.data['split']=='retain')}\")\n",
    "else:\n",
    "    print(\"[Enhanced Training] Dataset object not found; skipping upsample step.\")\n",
    "\n",
    "orig_train_method = sku_trainer.train\n",
    "\n",
    "def train_with_curriculum(self):\n",
    "    for epoch in range(EPOCHS):\n",
    "        cur_l2 = L2_AFTER if epoch >= CURRICULUM_SWITCH else L2_ANCHOR\n",
    "        print(f\"\\n[Epoch {epoch+1}/{EPOCHS}] L2 anchor weight: {cur_l2}\")\n",
    "        orig_train_method(\n",
    "            dataloader=dataloader,\n",
    "            num_epochs=1,\n",
    "            lr=LR,\n",
    "            ce_weight_prompt=CE_PROMPT_WEIGHT,\n",
    "            ul_weight_answer=UL_WEIGHT,\n",
    "            ce_weight_retain=CE_RETAIN_WEIGHT,\n",
    "            l2_anchor_weight=cur_l2,\n",
    "            entropy_weight_answer=ENTROPY_WEIGHT,\n",
    "            refusal_weight=REFUSAL_WEIGHT,\n",
    "            grad_clip=1.0,\n",
    "            grad_accum_steps=GRAD_ACCUM,\n",
    "            use_mixed_precision=True,\n",
    "            warmup_ratio=WARMUP if epoch == 0 else 0.0,\n",
    "            ul_ramp_ratio=UL_RAMP,\n",
    "        )\n",
    "        # Save epoch checkpoint\n",
    "        ckpt_dir = f\"enhanced_ckpts/epoch_{epoch+1}\"\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        self.save_model(ckpt_dir)\n",
    "        # Span probability diagnostic\n",
    "        try:\n",
    "            from peft import PeftModel\n",
    "            base_tok = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "            if base_tok.pad_token is None: base_tok.pad_token = base_tok.eos_token\n",
    "            base_model_local = AutoModelForCausalLM.from_pretrained(MODEL_PATH, local_files_only=True, torch_dtype=torch.float32)\n",
    "            adapted_local = PeftModel.from_pretrained(base_model_local, ckpt_dir)\n",
    "            sensitive_prompt = forget_validation_df.iloc[0]['input']\n",
    "            sensitive_answer = forget_validation_df.iloc[0]['output']\n",
    "            lp_base, Lspan = compute_span_logprob(base_model_local, base_tok, sensitive_prompt, sensitive_answer)\n",
    "            lp_adapt, _ = compute_span_logprob(adapted_local, base_tok, sensitive_prompt, sensitive_answer)\n",
    "            print(f\"[Epoch {epoch+1}] Span log-prob base {lp_base:.2f} -> adapted {lp_adapt:.2f} Œî {lp_adapt-lp_base:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Epoch {epoch+1}] Span diagnostic skipped: {e}\")\n",
    "\n",
    "sku_trainer.train_curriculum = types.MethodType(train_with_curriculum, sku_trainer)\n",
    "print(\"Call sku_trainer.train_curriculum() to launch enhanced training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8.3 Deep Debug: Adapter Change & Gradient Inspection ===\n",
    "import torch, math\n",
    "\n",
    "if 'sku_trainer' not in globals():\n",
    "    print(\"sku_trainer not found. Run the setup cells first.\")\n",
    "else:\n",
    "    model = sku_trainer.model\n",
    "    changed = []\n",
    "    total_l2 = 0.0\n",
    "    total_params = 0\n",
    "    # Identify LoRA matrices (common naming: lora_A, lora_B)\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.requires_grad and ('lora_A' in name or 'lora_B' in name):\n",
    "            init = sku_trainer.initial_state_dict.get(name)\n",
    "            if init is None:\n",
    "                continue\n",
    "            diff = (p.detach() - init.to(p.device, dtype=p.dtype))\n",
    "            l2 = diff.pow(2).sum().sqrt().item()\n",
    "            mean_abs = diff.abs().mean().item()\n",
    "            changed.append((name, p.numel(), l2, mean_abs))\n",
    "            total_l2 += diff.pow(2).sum().item()\n",
    "            total_params += p.numel()\n",
    "    if not changed:\n",
    "        print(\"No LoRA parameter deltas detected (initial snapshot may refer to different model instance).\")\n",
    "    else:\n",
    "        changed.sort(key=lambda x: x[2], reverse=True)\n",
    "        print(f\"Top 5 adapter deltas (L2 norm):\")\n",
    "        for name, n, l2, mean_abs in changed[:5]:\n",
    "            print(f\"  {name}: L2={l2:.4e} | mean|Œî|={mean_abs:.4e} | elems={n}\")\n",
    "        global_l2 = math.sqrt(total_l2) if total_l2>0 else 0.0\n",
    "        print(f\"Global adapter Œî L2: {global_l2:.4e} over {total_params} params\")\n",
    "\n",
    "    # If global_l2 ~ 0, training produced effectively no update -> verify gradient flow on one batch\n",
    "    if total_l2 < 1e-6:\n",
    "        print(\"Adapter updates near zero; sampling one batch to check gradient flow...\")\n",
    "        b = next(iter(dataloader))\n",
    "        for k in b: b[k] = b[k].to(sku_trainer.device)\n",
    "        model.train()\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad = None\n",
    "        out = model(input_ids=b['input_ids'], attention_mask=b['attention_mask'], use_cache=False, return_dict=True)\n",
    "        logits = out.logits[:, :-1]\n",
    "        target = b['labels'][:, 1:]\n",
    "        attn_tgt = b['attention_mask'][:, 1:].bool()\n",
    "        # Simple CE to probe gradients\n",
    "        vocab = logits.size(-1)\n",
    "        loss_probe = torch.nn.functional.cross_entropy(logits.reshape(-1, vocab)[attn_tgt.reshape(-1)], target.reshape(-1)[attn_tgt.reshape(-1)])\n",
    "        loss_probe.backward()\n",
    "        grad_sum = 0.0\n",
    "        grad_ct = 0\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad and ('lora_A' in name or 'lora_B' in name):\n",
    "                if p.grad is not None:\n",
    "                    gnorm = p.grad.detach().pow(2).sum().sqrt().item()\n",
    "                    if gnorm > 0:\n",
    "                        grad_sum += gnorm\n",
    "                        grad_ct += 1\n",
    "        print(f\"Adapter grad probes: {grad_ct} matrices with cumulative grad L2 {grad_sum:.4e}\")\n",
    "        if grad_ct == 0 or grad_sum < 1e-6:\n",
    "            print(\"No gradient reaching LoRA layers. Possible causes: target_modules mismatch, model wrapped after snapshot, or parameters frozen.\")\n",
    "        else:\n",
    "            print(\"Gradients flow, but saved deltas were zero -> likely saving/loading different adapter instance.\")\n",
    "\n",
    "    # Sanity: list adapter directory files for last epoch if present\n",
    "    import os\n",
    "    last_ckpt = None\n",
    "    if os.path.isdir('enhanced_ckpts'):\n",
    "        epochs = [d for d in os.listdir('enhanced_ckpts') if d.startswith('epoch_')]\n",
    "        if epochs:\n",
    "            epochs.sort()\n",
    "            last_ckpt = f\"enhanced_ckpts/{epochs[-1]}\"\n",
    "    if last_ckpt:\n",
    "        print(f\"Listing adapter files in {last_ckpt}:\")\n",
    "        try:\n",
    "            for f in os.listdir(last_ckpt):\n",
    "                print(\"  -\", f, os.path.getsize(os.path.join(last_ckpt,f)), \"bytes\")\n",
    "        except Exception as e:\n",
    "            print(\"Could not list files:\", e)\n",
    "    else:\n",
    "        print(\"No enhanced_ckpts directory found.\")\n",
    "\n",
    "print(\"[Deep Debug Complete]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8.4 Multi-Sample Forget Span Probability Evaluation ===\n",
    "import torch, math, statistics\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"[8.4] Evaluating average forget-answer log-prob before vs after adapters (first 50 samples)...\")\n",
    "\n",
    "N_SAMPLES = 50\n",
    "if 'forget_validation_df' not in globals():\n",
    "    print(\"forget_validation_df missing.\")\n",
    "else:\n",
    "    base_tok = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "    if base_tok.pad_token is None: base_tok.pad_token = base_tok.eos_token\n",
    "    base_model_eval = AutoModelForCausalLM.from_pretrained(MODEL_PATH, local_files_only=True, torch_dtype=torch.float32)\n",
    "    adapted_model_eval = None\n",
    "    # Prefer last enhanced checkpoint\n",
    "    pref_ckpt = 'enhanced_ckpts/epoch_5'\n",
    "    if not os.path.isdir(pref_ckpt):\n",
    "        # fallback to any epoch dir\n",
    "        import glob\n",
    "        cands = sorted(glob.glob('enhanced_ckpts/epoch_*'))\n",
    "        if cands:\n",
    "            pref_ckpt = cands[-1]\n",
    "    if os.path.isdir(pref_ckpt):\n",
    "        try:\n",
    "            adapted_model_eval = PeftModel.from_pretrained(base_model_eval, pref_ckpt)\n",
    "            adapted_model_eval.eval()\n",
    "            print(f\"Loaded adapters from {pref_ckpt}\")\n",
    "        except Exception as e:\n",
    "            print(\"Adapter load failed:\", e)\n",
    "    else:\n",
    "        print(\"No adapter checkpoint directory found for evaluation.\")\n",
    "\n",
    "    def span_lp(model, prompt, answer):\n",
    "        with torch.no_grad():\n",
    "            toks_prompt = base_tok(prompt, add_special_tokens=False)\n",
    "            plen = len(toks_prompt['input_ids'])\n",
    "            toks_full = base_tok(prompt + answer, return_tensors='pt')\n",
    "            out = model(**toks_full, use_cache=False, return_dict=True)\n",
    "            logits = out.logits[:, :-1]\n",
    "            targets = toks_full['input_ids'][:, 1:]\n",
    "            # answer positions start at plen (input index), so target indices start at plen-1\n",
    "            start = max(plen-1, 0)\n",
    "            end = targets.size(1)\n",
    "            if start >= end:\n",
    "                return float('nan'), 0\n",
    "            lps = torch.log_softmax(logits[0, start:end, :], dim=-1)\n",
    "            tgt = targets[0, start:end]\n",
    "            gather = lps[range(end-start), tgt]\n",
    "            return gather.sum().item(), end-start\n",
    "    base_scores = []\n",
    "    adapt_scores = []\n",
    "    for i in range(min(N_SAMPLES, len(forget_validation_df))):\n",
    "        row = forget_validation_df.iloc[i]\n",
    "        lp_b, Lb = span_lp(base_model_eval, row['input'], row['output'])\n",
    "        if math.isfinite(lp_b):\n",
    "            base_scores.append(lp_b)\n",
    "        if adapted_model_eval is not None:\n",
    "            lp_a, La = span_lp(adapted_model_eval, row['input'], row['output'])\n",
    "            if math.isfinite(lp_a):\n",
    "                adapt_scores.append(lp_a)\n",
    "    if base_scores and adapt_scores and len(base_scores)==len(adapt_scores):\n",
    "        deltas = [a-b for a,b in zip(adapt_scores, base_scores)]\n",
    "        print(f\"Samples compared: {len(deltas)}\")\n",
    "        print(f\"Mean base span log-prob: {statistics.mean(base_scores):.2f}\")\n",
    "        print(f\"Mean adapted span log-prob: {statistics.mean(adapt_scores):.2f}\")\n",
    "        print(f\"Mean Œî (adapt-base): {statistics.mean(deltas):.2f}\")\n",
    "        print(f\"Median Œî: {statistics.median(deltas):.2f}\")\n",
    "        worse = sum(1 for d in deltas if d>0)\n",
    "        better = sum(1 for d in deltas if d<0)\n",
    "        print(f\"Count Œî<0 (desired): {better} | Œî>0: {worse}\")\n",
    "        if statistics.mean(deltas) > -0.5:\n",
    "            print(\"=> Forgetting signal weak. Consider raising UL_WEIGHT, reducing CE_PROMPT_WEIGHT further, or adding KL to refusal.\")\n",
    "    else:\n",
    "        print(\"Insufficient comparable scores.\")\n",
    "\n",
    "print(\"[8.4] Done.\")\n",
    "\n",
    "# === 8.5 Instrumented Training Wrapper (Logs UL loss & Answer Mask Coverage) ===\n",
    "import time, types\n",
    "\n",
    "if 'sku_trainer' in globals():\n",
    "    original_train = sku_trainer.train\n",
    "    def train_instrumented(self, *args, **kwargs):\n",
    "        print(\"[Instrumented] Starting instrumented pass...\")\n",
    "        log_every = 50\n",
    "        batch_stats = []\n",
    "        from contextlib import nullcontext\n",
    "        # Wrap original dataloader to capture per-batch masks\n",
    "        dl = kwargs.get('dataloader')\n",
    "        if dl is None:\n",
    "            raise ValueError('Provide dataloader explicitly to train_instrumented')\n",
    "        # Monkey patch internal methods to intercept UL\n",
    "        orig_ul = self._unlikelihood_loss\n",
    "        def tracked_ul(logits, labels, loss_mask):\n",
    "            ul_val = orig_ul(logits, labels, loss_mask)\n",
    "            batch_stats.append(('ul', float(ul_val.detach().cpu()), int(loss_mask.sum().item())))\n",
    "            return ul_val\n",
    "        self._unlikelihood_loss = tracked_ul\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            original_train(self, *args, **kwargs)\n",
    "        finally:\n",
    "            self._unlikelihood_loss = orig_ul\n",
    "        # Summaries\n",
    "        ul_vals = [v for tag,v,_ in batch_stats if tag=='ul']\n",
    "        ul_tokens = [c for tag,_,c in batch_stats if tag=='ul']\n",
    "        if ul_vals:\n",
    "            print(f\"[Instrumented] Avg UL loss: {sum(ul_vals)/len(ul_vals):.4f} | Avg UL active tokens: {sum(ul_tokens)/len(ul_tokens):.1f}\")\n",
    "        print(f\"[Instrumented] Duration: {time.time()-start_time:.1f}s\")\n",
    "    sku_trainer.train_instrumented = types.MethodType(train_instrumented, sku_trainer)\n",
    "    print(\"[8.5] sku_trainer.train_instrumented available. Use it with the same args as sku_trainer.train().\")\n",
    "else:\n",
    "    print(\"sku_trainer not found; skip instrumented wrapper.\")\n",
    "\n",
    "print(\"[8.5] Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8.4b Corrected Isolated Span Probability Evaluation ===\n",
    "\"\"\"\n",
    "The previous 8.4 evaluation loaded adapters into (or alongside) the same base model instance prior to computing\n",
    "\"base\" scores, making base and adapted probabilities identical (Œî = 0). This cell:\n",
    " 1. Loads a FRESH base model (base_model_plain) for baseline scores.\n",
    " 2. Loads a SEPARATE fresh base model and then applies adapters (base_model_with_adapters -> adapted_model).\n",
    " 3. Computes log P(answer | prompt) over answer tokens only, for N samples.\n",
    " 4. Reports mean / median Œî (adapt - base). Negative values indicate successful forgetting.\n",
    "\n",
    "Memory note: Loads the 1B model twice; if OOM, set LOAD_TWICE=False to reuse one instance (but must score base BEFORE attaching adapters) .\n",
    "\"\"\"\n",
    "import math, statistics, torch, gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "N_SAMPLES = 50\n",
    "LOAD_TWICE = True   # set False if memory constrained\n",
    "CKPT_DIR = 'enhanced_ckpts/epoch_5'\n",
    "\n",
    "def load_base():\n",
    "    return AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH, local_files_only=True, torch_dtype=torch.float32\n",
    "    )\n",
    "\n",
    "print('[8.4b] Starting isolated evaluation ...')\n",
    "if 'forget_validation_df' not in globals():\n",
    "    print('Missing forget_validation_df.')\n",
    "else:\n",
    "    base_tok = AutoTokenizer.from_pretrained('allenai/OLMo-1B-0724-hf')\n",
    "    if base_tok.pad_token is None:\n",
    "        base_tok.pad_token = base_tok.eos_token\n",
    "\n",
    "    if not LOAD_TWICE:\n",
    "        # One-pass strategy: compute base scores, then attach adapters and recompute\n",
    "        base_model_plain = load_base().eval()\n",
    "        # Collect base scores\n",
    "        base_scores = []\n",
    "        answers_lens = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(min(N_SAMPLES, len(forget_validation_df))):\n",
    "                row = forget_validation_df.iloc[i]\n",
    "                prompt = row['input']\n",
    "                answer = row['output']\n",
    "                toks_full = base_tok(prompt + answer, return_tensors='pt')\n",
    "                plen = len(base_tok(prompt, add_special_tokens=False)['input_ids'])\n",
    "                for k in toks_full: toks_full[k] = toks_full[k]\n",
    "                out = base_model_plain(**toks_full, use_cache=False, return_dict=True)\n",
    "                logits = out.logits[:, :-1]\n",
    "                targets = toks_full['input_ids'][:, 1:]\n",
    "                start = plen  # answer starts at prompt length for target alignment\n",
    "                end = targets.size(1)\n",
    "                if start < end:\n",
    "                    lps = torch.log_softmax(logits[0, start-1:end-1, :], dim=-1)  # shift back one due to causal alignment\n",
    "                    span_ids = targets[0, start-1:end-1]\n",
    "                    gather = lps[range(span_ids.size(0)), span_ids]\n",
    "                    base_scores.append(gather.sum().item())\n",
    "                    answers_lens.append(span_ids.size(0))\n",
    "        # Attach adapters\n",
    "        adapted_model = PeftModel.from_pretrained(base_model_plain, CKPT_DIR).eval()\n",
    "        adapt_scores = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(min(N_SAMPLES, len(forget_validation_df))):\n",
    "                row = forget_validation_df.iloc[i]\n",
    "                prompt = row['input']\n",
    "                answer = row['output']\n",
    "                toks_full = base_tok(prompt + answer, return_tensors='pt')\n",
    "                plen = len(base_tok(prompt, add_special_tokens=False)['input_ids'])\n",
    "                out = adapted_model(**toks_full, use_cache=False, return_dict=True)\n",
    "                logits = out.logits[:, :-1]\n",
    "                targets = toks_full['input_ids'][:, 1:]\n",
    "                start = plen\n",
    "                end = targets.size(1)\n",
    "                if start < end:\n",
    "                    lps = torch.log_softmax(logits[0, start-1:end-1, :], dim=-1)\n",
    "                    span_ids = targets[0, start-1:end-1]\n",
    "                    gather = lps[range(span_ids.size(0)), span_ids]\n",
    "                    adapt_scores.append(gather.sum().item())\n",
    "    else:\n",
    "        # Two independent loads\n",
    "        base_model_plain = load_base().eval()\n",
    "        base_scores = []\n",
    "        answers_lens = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(min(N_SAMPLES, len(forget_validation_df))):\n",
    "                row = forget_validation_df.iloc[i]\n",
    "                prompt = row['input']\n",
    "                answer = row['output']\n",
    "                toks_full = base_tok(prompt + answer, return_tensors='pt')\n",
    "                plen = len(base_tok(prompt, add_special_tokens=False)['input_ids'])\n",
    "                out = base_model_plain(**toks_full, use_cache=False, return_dict=True)\n",
    "                logits = out.logits[:, :-1]\n",
    "                targets = toks_full['input_ids'][:, 1:]\n",
    "                start = plen\n",
    "                end = targets.size(1)\n",
    "                if start < end:\n",
    "                    lps = torch.log_softmax(logits[0, start-1:end-1, :], dim=-1)\n",
    "                    span_ids = targets[0, start-1:end-1]\n",
    "                    gather = lps[range(span_ids.size(0)), span_ids]\n",
    "                    base_scores.append(gather.sum().item())\n",
    "                    answers_lens.append(span_ids.size(0))\n",
    "        # Free first instance if memory tight\n",
    "        del base_model_plain\n",
    "        gc.collect()\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Load second base and apply adapters\n",
    "        base_model_with_adapters = load_base()\n",
    "        adapted_model = PeftModel.from_pretrained(base_model_with_adapters, CKPT_DIR).eval()\n",
    "        adapt_scores = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(min(N_SAMPLES, len(forget_validation_df))):\n",
    "                row = forget_validation_df.iloc[i]\n",
    "                prompt = row['input']\n",
    "                answer = row['output']\n",
    "                toks_full = base_tok(prompt + answer, return_tensors='pt')\n",
    "                plen = len(base_tok(prompt, add_special_tokens=False)['input_ids'])\n",
    "                out = adapted_model(**toks_full, use_cache=False, return_dict=True)\n",
    "                logits = out.logits[:, :-1]\n",
    "                targets = toks_full['input_ids'][:, 1:]\n",
    "                start = plen\n",
    "                end = targets.size(1)\n",
    "                if start < end:\n",
    "                    lps = torch.log_softmax(logits[0, start-1:end-1, :], dim=-1)\n",
    "                    span_ids = targets[0, start-1:end-1]\n",
    "                    gather = lps[range(span_ids.size(0)), span_ids]\n",
    "                    adapt_scores.append(gather.sum().item())\n",
    "\n",
    "    if base_scores and adapt_scores and len(base_scores)==len(adapt_scores):\n",
    "        deltas = [a-b for a,b in zip(adapt_scores, base_scores)]\n",
    "        print(f\"Samples compared: {len(deltas)}\")\n",
    "        print(f\"Mean base span log-prob: {statistics.mean(base_scores):.2f}\")\n",
    "        print(f\"Mean adapted span log-prob: {statistics.mean(adapt_scores):.2f}\")\n",
    "        print(f\"Mean Œî (adapt-base): {statistics.mean(deltas):.2f}\")\n",
    "        print(f\"Median Œî: {statistics.median(deltas):.2f}\")\n",
    "        neg = sum(1 for d in deltas if d < 0)\n",
    "        pos = sum(1 for d in deltas if d > 0)\n",
    "        print(f\"Count Œî<0 (desired): {neg} | Œî>0: {pos}\")\n",
    "        if statistics.mean(deltas) > -0.5:\n",
    "            print('=> Forgetting still weak OR sensitive spans not strongly represented; consider: higher UL_WEIGHT (8-10), add KL to refusal, extra epochs focused only on forget batch sampling.')\n",
    "    else:\n",
    "        print('Insufficient comparable scores or mismatch.')\n",
    "\n",
    "print('[8.4b] Done.')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8051727,
     "sourceId": 12737770,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
