{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlearning with TOFU (Inverted Hinge Loss + Fisher Information + FILA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T08:37:29.448844Z",
     "iopub.status.busy": "2025-09-07T08:37:29.448419Z",
     "iopub.status.idle": "2025-09-07T08:38:03.989740Z",
     "shell.execute_reply": "2025-09-07T08:38:03.988902Z",
     "shell.execute_reply.started": "2025-09-07T08:37:29.448814Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q rouge-score torchmetrics transformers huggingface_hub\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Paths and configuration for local environment\n",
    "MODEL_PATH = \"semeval25-unlearning-1B-model\"\n",
    "DATA_PATH = \"semeval25-unlearning-data\"\n",
    "STUDENT_PATH = \"studentmodel_final\"\n",
    "Path(STUDENT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# HuggingFace token (replace with your actual token)\n",
    "# Get your token from: https://huggingface.co/settings/tokens\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "# Download model and data if not already present\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(\"Downloading model...\")\n",
    "    snapshot_download(\n",
    "        repo_id='llmunlearningsemeval2025organization/olmo-1B-model-semeval25-unlearning', \n",
    "        token=HF_TOKEN, \n",
    "        local_dir=MODEL_PATH\n",
    "    )\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(\"Downloading dataset...\")\n",
    "    snapshot_download(\n",
    "        repo_id='llmunlearningsemeval2025organization/semeval25-unlearning-dataset-public', \n",
    "        token=HF_TOKEN, \n",
    "        local_dir=DATA_PATH, \n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "\n",
    "# Report visible GPUs\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T08:38:03.991948Z",
     "iopub.status.busy": "2025-09-07T08:38:03.991372Z",
     "iopub.status.idle": "2025-09-07T08:38:06.746068Z",
     "shell.execute_reply": "2025-09-07T08:38:06.745324Z",
     "shell.execute_reply.started": "2025-09-07T08:38:03.991902Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load parquet datasets from local directory\n",
    "retain_train_df = pd.read_parquet(f\"{DATA_PATH}/data/retain_train-00000-of-00001.parquet\", engine='pyarrow')\n",
    "retain_validation_df = pd.read_parquet(f\"{DATA_PATH}/data/retain_validation-00000-of-00001.parquet\", engine='pyarrow')\n",
    "forget_train_df = pd.read_parquet(f\"{DATA_PATH}/data/forget_train-00000-of-00001.parquet\", engine='pyarrow')\n",
    "forget_validation_df = pd.read_parquet(f\"{DATA_PATH}/data/forget_validation-00000-of-00001.parquet\", engine='pyarrow')\n",
    "\n",
    "# Save as JSONL for evaluation scripts (portable without shell commands)\n",
    "Path('train').mkdir(parents=True, exist_ok=True)\n",
    "Path('validation').mkdir(parents=True, exist_ok=True)\n",
    "retain_train_df.to_json('train/retain.jsonl', orient='records', lines=True)\n",
    "forget_train_df.to_json('train/forget.jsonl', orient='records', lines=True)\n",
    "retain_validation_df.to_json('validation/retain.jsonl', orient='records', lines=True)\n",
    "forget_validation_df.to_json('validation/forget.jsonl', orient='records', lines=True)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Datasets saved and tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T08:38:06.747061Z",
     "iopub.status.busy": "2025-09-07T08:38:06.746831Z",
     "iopub.status.idle": "2025-09-07T08:38:06.754981Z",
     "shell.execute_reply": "2025-09-07T08:38:06.754208Z",
     "shell.execute_reply.started": "2025-09-07T08:38:06.747037Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UnlearningDataset(Dataset):\n",
    "    \"\"\"Dataset for unlearning with combined input and output text.\n",
    "\n",
    "    - Expects items with keys: 'input', 'output', optional 'split' ('retain' or 'forget').\n",
    "    - Returns tokenized tensors and the start index of the output (start_locs).\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if isinstance(data_source, pd.DataFrame):\n",
    "            self.data = data_source\n",
    "            print(f\"Loaded {len(self.data)} samples from DataFrame\")\n",
    "        elif isinstance(data_source, str):\n",
    "            data_list = []\n",
    "            with open(data_source, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    item = json.loads(line.strip())\n",
    "                    data_list.append(item)\n",
    "            self.data = pd.DataFrame(data_list)\n",
    "            print(f\"Loaded {len(self.data)} samples from {data_source}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        input_text = item[\"input\"]\n",
    "        output_text = item[\"output\"]\n",
    "\n",
    "        # Single tokenization of concatenated input and output\n",
    "        combined = f\"{input_text} {output_text}\"\n",
    "        tokenized = self.tokenizer(\n",
    "            combined,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Tokenize only the input to compute the start index of the output\n",
    "        input_ids = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n",
    "            \"start_locs\": input_ids.size(0),  # index where output begins\n",
    "            \"labels\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"split\": 1 if item.get(\"split\", \"retain\") == \"forget\" else 0\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T08:38:06.757217Z",
     "iopub.status.busy": "2025-09-07T08:38:06.756977Z",
     "iopub.status.idle": "2025-09-07T08:38:06.813667Z",
     "shell.execute_reply": "2025-09-07T08:38:06.812941Z",
     "shell.execute_reply.started": "2025-09-07T08:38:06.757200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "batch_size = 4\n",
    "# Build separate datasets for retain/forget splits\n",
    "retain_train_dataset = UnlearningDataset(retain_train_df, tokenizer)\n",
    "forget_train_dataset = UnlearningDataset(forget_train_df, tokenizer)\n",
    "\n",
    "retain_train_dataloader = DataLoader(retain_train_dataset, batch_size, shuffle=True)\n",
    "forget_train_dataloader = DataLoader(forget_train_dataset, batch_size, shuffle=True)\n",
    "\n",
    "# Validation loaders (no shuffling)\n",
    "retain_val_dataset = UnlearningDataset(retain_validation_df, tokenizer)\n",
    "forget_val_dataset = UnlearningDataset(forget_validation_df, tokenizer)\n",
    "retain_val_dataloader = DataLoader(retain_val_dataset, batch_size, shuffle=False)\n",
    "forget_val_dataloader = DataLoader(forget_val_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DualTeacher Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T15:36:45.707061Z",
     "iopub.status.busy": "2025-09-07T15:36:45.706832Z",
     "iopub.status.idle": "2025-09-07T15:36:45.761260Z",
     "shell.execute_reply": "2025-09-07T15:36:45.760581Z",
     "shell.execute_reply.started": "2025-09-07T15:36:45.707035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from functools import reduce\n",
    "from torchmetrics.classification import MulticlassHingeLoss\n",
    "from torchmetrics.utilities.data import to_onehot\n",
    "from torchmetrics.metric import Metric\n",
    "from torchmetrics.functional.classification.confusion_matrix import _multiclass_confusion_matrix_format\n",
    "from torchmetrics.functional.classification.hinge import (\n",
    "    _multiclass_hinge_loss_arg_validation, \n",
    "    _multiclass_hinge_loss_tensor_validation,\n",
    "    _hinge_loss_compute\n",
    ")\n",
    "\n",
    "def _custom_multiclass_hinge_loss_update(\n",
    "    preds,\n",
    "    target,\n",
    "    alpha,\n",
    "    squared,\n",
    "    multiclass_mode = \"crammer-singer\"\n",
    "):\n",
    "    if not torch.all((preds >= 0) * (preds <= 1)):\n",
    "        preds = preds.softmax(1)\n",
    "\n",
    "    target = to_onehot(target, max(2, preds.shape[1])).bool()\n",
    "    if multiclass_mode == \"crammer-singer\":\n",
    "        margin = preds[target]\n",
    "        margin -= torch.max(preds[~target].view(preds.shape[0], -1), dim=1)[0]\n",
    "    else:\n",
    "        target = target.bool()\n",
    "        margin = torch.zeros_like(preds)\n",
    "        margin[target] = preds[target]\n",
    "        margin[~target] = -preds[~target]\n",
    "\n",
    "    measures = alpha + margin\n",
    "    measures = torch.clamp(measures, 0)\n",
    "\n",
    "    if squared:\n",
    "        measures = measures.pow(2)\n",
    "\n",
    "    total = torch.tensor(target.shape[0], device=target.device)\n",
    "    return measures.sum(dim=0), total\n",
    "\n",
    "def multiclass_hinge_loss(\n",
    "    preds,\n",
    "    target,\n",
    "    num_classes,\n",
    "    alpha = 1.0,\n",
    "    squared = False,\n",
    "    multiclass_mode = \"crammer-singer\",\n",
    "    ignore_index = None,\n",
    "    validate_args = True,\n",
    "):\n",
    "    if validate_args:\n",
    "        _multiclass_hinge_loss_arg_validation(num_classes, squared, multiclass_mode, ignore_index)\n",
    "        _multiclass_hinge_loss_tensor_validation(preds, target, num_classes, ignore_index)\n",
    "    preds, target = _multiclass_confusion_matrix_format(preds, target, ignore_index, convert_to_labels=False)\n",
    "    measures, total = _custom_multiclass_hinge_loss_update(\n",
    "        preds, \n",
    "        target, \n",
    "        alpha,\n",
    "        squared, \n",
    "        multiclass_mode,\n",
    "    )\n",
    "    return _hinge_loss_compute(measures, total)\n",
    "\n",
    "class TOFUTrainer:\n",
    "    def __init__(self, model_path, tokenizer, lora_config, device=\"cuda:0\", importance_file=None):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lora_config = lora_config\n",
    "        self.device = device\n",
    "        self.importance_file = importance_file\n",
    "        \n",
    "        self.model = None\n",
    "        self.initial_state_dict = {}\n",
    "        \n",
    "        # Validation tracking\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "        # Loss weights\n",
    "        self.forget_weight = 1.0\n",
    "        self.retain_weight = 1.0\n",
    "        \n",
    "    def get_module_by_name(self, module, access_string):\n",
    "        \"\"\"Helper function to get module by name\"\"\"\n",
    "        names = access_string.split(sep='.')\n",
    "        return reduce(getattr, names, module)\n",
    "        \n",
    "    def setup_model(self):\n",
    "        \"\"\"Initialize and setup model with LoRA and optional FILA\"\"\"\n",
    "        print(\"Setting up model...\")\n",
    "        \n",
    "        # Load base model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(self.model_path, local_files_only=True)\n",
    "        \n",
    "        # Setup model with LoRA\n",
    "        self.model = get_peft_model(base_model, self.lora_config)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.print_trainable_parameters()\n",
    "        \n",
    "        # Apply FILA if importance file is provided\n",
    "        if self.importance_file:\n",
    "            self.apply_fila()\n",
    "        \n",
    "        # Save initial state for task vector calculation\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.initial_state_dict[name] = param.data.clone()\n",
    "        \n",
    "        print(\"Model setup completed\")\n",
    "    \n",
    "    def apply_fila(self):\n",
    "        \"\"\"Apply Fisher Information Weighted LoRA Adaptation (FILA)\"\"\"\n",
    "        print(f'Loading importance file from {self.importance_file}')\n",
    "        imp_file = torch.load(self.importance_file, map_location='cpu')\n",
    "        \n",
    "        f_cnt = imp_file['f_cnt']\n",
    "        r_cnt = imp_file['r_cnt']\n",
    "        importance_f = imp_file['importance_f']\n",
    "        importance_r = imp_file['importance_r']\n",
    "        \n",
    "        # Calculate importance ratio: forget/retain\n",
    "        importances = {n: torch.div(importance_f[n]/f_cnt, 1e-5+(importance_r[n]/r_cnt)) for n in importance_f.keys()}\n",
    "        \n",
    "        # Get LoRA target modules\n",
    "        lora_targets = self.lora_config.target_modules\n",
    "        \n",
    "        for old_name, importance in importances.items():\n",
    "            if not any([target_name in old_name for target_name in lora_targets]):\n",
    "                continue\n",
    "                \n",
    "            name = old_name.replace(\"module.\", '')\n",
    "            lora_A = 'base_model.model.'+name.replace(\".weight\", '')+'.lora_A'\n",
    "            lora_B = 'base_model.model.'+name.replace(\".weight\", '')+'.lora_B'\n",
    "            base_layer = 'base_model.model.'+name.replace(\".weight\", '')+'.base_layer'\n",
    "            scaling = 'base_model.model.'+name.replace(\".weight\", '')+'.scaling'\n",
    "\n",
    "            try:\n",
    "                lora_A_module = self.get_module_by_name(self.model, lora_A)\n",
    "                lora_B_module = self.get_module_by_name(self.model, lora_B)\n",
    "                base_layer_module = self.get_module_by_name(self.model, base_layer)\n",
    "                scaling_module = self.get_module_by_name(self.model, scaling)\n",
    "\n",
    "                orig_shape = base_layer_module.weight.shape\n",
    "                W = base_layer_module.weight.data.reshape(orig_shape)\n",
    "                dtype = W.dtype\n",
    "                W = W.to(torch.float32)\n",
    "\n",
    "                # Solve row-wise weighted low-rank approximation\n",
    "                row_importance = importance.sum(dim=1).sqrt().to(W.device) # row-wise sum\n",
    "                U, S, V = torch.svd_lowrank(row_importance[:,None] * W, q=self.lora_config.r)\n",
    "\n",
    "                S = S / scaling_module['default']\n",
    "\n",
    "                new_lora_A = (V * torch.sqrt(S)).t()\n",
    "                new_lora_B = (1/(row_importance+1e-5))[:,None] * (U * torch.sqrt(S))\n",
    "                new_residual = base_layer_module.weight.data.reshape(orig_shape) - scaling_module['default'] * new_lora_B @ new_lora_A\n",
    "\n",
    "                lora_A_module['default'].weight.data = new_lora_A.contiguous().to(dtype)\n",
    "                lora_B_module['default'].weight.data = new_lora_B.contiguous().to(dtype)\n",
    "                base_layer_module.weight.data = new_residual.contiguous().to(dtype)\n",
    "                \n",
    "                print(f\"Applied FILA to {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not apply FILA to {name}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    def compute_inverted_hinge_loss(self, batch):\n",
    "        \"\"\"Compute Inverted Hinge Loss (IHL) for forget samples\"\"\"\n",
    "        input_ids = batch[\"input_ids\"].to(self.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "        labels = batch[\"labels\"].to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(input_ids, labels=labels, attention_mask=attention_mask)\n",
    "        \n",
    "        # Compute inverted hinge loss\n",
    "        scores = outputs.logits\n",
    "        shift_logits = scores[..., :-1, :].contiguous().squeeze().view(-1, scores.size(-1)) # [BN, V]\n",
    "        shift_labels = labels[..., 1:].contiguous().squeeze().view(-1) # [BN,]\n",
    "        \n",
    "        forget_loss = multiclass_hinge_loss(\n",
    "            shift_logits[shift_labels != -100,:], # ignore pad tokens\n",
    "            shift_labels[shift_labels != -100],\n",
    "            shift_logits.size(-1),\n",
    "        )\n",
    "        \n",
    "        return forget_loss\n",
    "    \n",
    "    def compute_retain_loss(self, batch):\n",
    "        \"\"\"Compute standard cross-entropy loss for retain samples\"\"\"\n",
    "        input_ids = batch[\"input_ids\"].to(self.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "        labels = batch[\"labels\"].to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(input_ids, labels=labels, attention_mask=attention_mask)\n",
    "        return outputs.loss\n",
    "    \n",
    "    def compute_mixed_loss(self, forget_batch, retain_batch):\n",
    "        \"\"\"Compute combined loss using IHL for forget and CE for retain\"\"\"\n",
    "        forget_loss = self.compute_inverted_hinge_loss(forget_batch)\n",
    "        retain_loss = self.compute_retain_loss(retain_batch)\n",
    "        \n",
    "        total_loss = self.forget_weight * forget_loss + self.retain_weight * retain_loss\n",
    "        \n",
    "        return total_loss, forget_loss, retain_loss\n",
    "    \n",
    "    def train_epoch(self, forget_dataloader, retain_dataloader, optimizer):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_losses = []\n",
    "        epoch_forget_losses = []\n",
    "        epoch_retain_losses = []\n",
    "        \n",
    "        # Make sure both dataloaders have the same length by cycling the shorter one\n",
    "        min_batches = min(len(forget_dataloader), len(retain_dataloader))\n",
    "        \n",
    "        forget_iter = iter(forget_dataloader)\n",
    "        retain_iter = iter(retain_dataloader)\n",
    "        \n",
    "        for _ in tqdm(range(min_batches), desc=\"Training\"):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            try:\n",
    "                forget_batch = next(forget_iter)\n",
    "                retain_batch = next(retain_iter)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            \n",
    "            total_loss, forget_loss, retain_loss = self.compute_mixed_loss(forget_batch, retain_batch)\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(total_loss.item())\n",
    "            epoch_forget_losses.append(forget_loss.item())\n",
    "            epoch_retain_losses.append(retain_loss.item())\n",
    "        \n",
    "        return {\n",
    "            'total_loss': np.mean(epoch_losses),\n",
    "            'forget_loss': np.mean(epoch_forget_losses),\n",
    "            'retain_loss': np.mean(epoch_retain_losses)\n",
    "        }\n",
    "    \n",
    "    def validate(self, forget_val_dataloader, retain_val_dataloader):\n",
    "        \"\"\"Validate model performance\"\"\"\n",
    "        self.model.eval()\n",
    "        val_losses = []\n",
    "        val_forget_losses = []\n",
    "        val_retain_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Validate on a subset to save time\n",
    "            for i, (forget_batch, retain_batch) in enumerate(zip(forget_val_dataloader, retain_val_dataloader)):\n",
    "                if i >= 10:  # Limit validation batches\n",
    "                    break\n",
    "                \n",
    "                total_loss, forget_loss, retain_loss = self.compute_mixed_loss(forget_batch, retain_batch)\n",
    "                \n",
    "                val_losses.append(total_loss.item())\n",
    "                val_forget_losses.append(forget_loss.item())\n",
    "                val_retain_losses.append(retain_loss.item())\n",
    "        \n",
    "        self.model.train()\n",
    "        return {\n",
    "            'val_total_loss': np.mean(val_losses) if val_losses else float('inf'),\n",
    "            'val_forget_loss': np.mean(val_forget_losses) if val_forget_losses else float('inf'),\n",
    "            'val_retain_loss': np.mean(val_retain_losses) if val_retain_losses else float('inf')\n",
    "        }\n",
    "    \n",
    "    def train(self, forget_dataloader, retain_dataloader, \n",
    "              forget_val_dataloader=None, retain_val_dataloader=None,\n",
    "              num_epochs=3, lr=1e-5, patience=3):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(\"Training with TOFU approach (IHL + Fisher Information + FILA)...\")\n",
    "        \n",
    "        self.model.train()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        \n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_metrics = self.train_epoch(forget_dataloader, retain_dataloader, optimizer)\n",
    "            \n",
    "            print(f\"Train - Total: {train_metrics['total_loss']:.4f}, \"\n",
    "                  f\"Forget (IHL): {train_metrics['forget_loss']:.4f}, \"\n",
    "                  f\"Retain: {train_metrics['retain_loss']:.4f}\")\n",
    "            \n",
    "            # Validate\n",
    "            if forget_val_dataloader and retain_val_dataloader:\n",
    "                val_metrics = self.validate(forget_val_dataloader, retain_val_dataloader)\n",
    "                \n",
    "                print(f\"Val - Total: {val_metrics['val_total_loss']:.4f}, \"\n",
    "                      f\"Forget: {val_metrics['val_forget_loss']:.4f}, \"\n",
    "                      f\"Retain: {val_metrics['val_retain_loss']:.4f}\")\n",
    "                \n",
    "                # Early stopping\n",
    "                if val_metrics['val_total_loss'] < self.best_val_loss:\n",
    "                    self.best_val_loss = val_metrics['val_total_loss']\n",
    "                    self.best_epoch = epoch\n",
    "                    patience_counter = 0\n",
    "                    # Save best model state\n",
    "                    self.best_model_state = {name: param.clone() for name, param in self.model.named_parameters()}\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        print(f\"Training completed. Best epoch: {self.best_epoch+1}\")\n",
    "        \n",
    "        # Load best model if we have it\n",
    "        if hasattr(self, 'best_model_state'):\n",
    "            for name, param in self.model.named_parameters():\n",
    "                param.data.copy_(self.best_model_state[name])\n",
    "            print(\"Loaded best model weights\")\n",
    "    \n",
    "    def save_model(self, save_path):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "        self.model.save_pretrained(save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "    \n",
    "    def compute_task_vector(self):\n",
    "        \"\"\"Compute task vector (difference from initial model)\"\"\"\n",
    "        task_vector = {}\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.initial_state_dict:\n",
    "                task_vector[name] = param.data - self.initial_state_dict[name]\n",
    "        return task_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Trainer and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T08:38:06.856124Z",
     "iopub.status.busy": "2025-09-07T08:38:06.855856Z",
     "iopub.status.idle": "2025-09-07T08:38:06.872388Z",
     "shell.execute_reply": "2025-09-07T08:38:06.871770Z",
     "shell.execute_reply.started": "2025-09-07T08:38:06.856107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LoRA configuration for TOFU training\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "# Initialize TOFU trainer with optional Fisher Information file for FILA\n",
    "# Set importance_file=None to disable FILA, or provide path to enable it\n",
    "tofu_trainer = TOFUTrainer(\n",
    "    model_path=MODEL_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    lora_config=lora_config,\n",
    "    device=\"cuda:0\",\n",
    "    importance_file=None  # Set to Fisher information file path to enable FILA\n",
    ")\n",
    "\n",
    "# Setup model\n",
    "tofu_trainer.setup_model()\n",
    "\n",
    "print(\"TOFU Trainer initialized successfully!\")\n",
    "print(\"Features enabled:\")\n",
    "print(\"- Inverted Hinge Loss (IHL) for forget samples\")\n",
    "print(\"- Cross-entropy loss for retain samples\") \n",
    "print(\"- LoRA fine-tuning\")\n",
    "if tofu_trainer.importance_file:\n",
    "    print(\"- FILA (Fisher Information weighted LoRA)\")\n",
    "else:\n",
    "    print(\"- FILA disabled (no importance file provided)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T08:38:06.873257Z",
     "iopub.status.busy": "2025-09-07T08:38:06.873052Z",
     "iopub.status.idle": "2025-09-07T08:38:06.887127Z",
     "shell.execute_reply": "2025-09-07T08:38:06.886445Z",
     "shell.execute_reply.started": "2025-09-07T08:38:06.873242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train with TOFU approach\n",
    "print(\"Starting TOFU training...\")\n",
    "\n",
    "# Train the model using Inverted Hinge Loss + standard cross-entropy\n",
    "tofu_trainer.train(\n",
    "    forget_dataloader=forget_train_dataloader,\n",
    "    retain_dataloader=retain_train_dataloader,\n",
    "    forget_val_dataloader=forget_val_dataloader,\n",
    "    retain_val_dataloader=retain_val_dataloader,\n",
    "    num_epochs=3,\n",
    "    lr=1e-5,\n",
    "    patience=2\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "save_path = STUDENT_PATH + \"_tofu\"\n",
    "tofu_trainer.save_model(save_path)\n",
    "\n",
    "print(f\"TOFU training completed! Model saved to {save_path}\")\n",
    "\n",
    "# Compute task vector (difference from initial weights)\n",
    "task_vector = tofu_trainer.compute_task_vector()\n",
    "print(f\"Task vector computed with {len(task_vector)} parameter differences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-07T08:46:15.369037Z",
     "iopub.status.busy": "2025-09-07T08:46:15.368737Z",
     "iopub.status.idle": "2025-09-07T08:46:19.840435Z",
     "shell.execute_reply": "2025-09-07T08:46:19.839323Z",
     "shell.execute_reply.started": "2025-09-07T08:46:15.369014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def measure_fisher_information(model, forget_dataloader, retain_dataloader, save_path=None):\n",
    "    \"\"\"\n",
    "    Measure Fisher Information for FILA implementation\n",
    "    Based on TOFU's measure_importance.py\n",
    "    \"\"\"\n",
    "    print(\"Measuring Fisher Information...\")\n",
    "    \n",
    "    # Find all linear layer names for importance measurement\n",
    "    def find_all_linear_names(model):\n",
    "        cls = torch.nn.Linear\n",
    "        lora_module_names = set()\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, cls):\n",
    "                names = name.split('.')\n",
    "                lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "        if 'lm_head' in lora_module_names:\n",
    "            lora_module_names.remove('lm_head')\n",
    "        return list(lora_module_names)\n",
    "    \n",
    "    # Force all parameters to require gradients\n",
    "    model.train()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Find target modules\n",
    "    target_modules = find_all_linear_names(model)\n",
    "    print(f\"Target modules for importance: {target_modules}\")\n",
    "    \n",
    "    # Initialize importance tracking\n",
    "    importance_f = {}\n",
    "    importance_r = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        for t in target_modules:\n",
    "            if t in name and 'weight' in name:\n",
    "                importance_f[name] = 0\n",
    "                importance_r[name] = 0\n",
    "    \n",
    "    f_cnt = 0\n",
    "    r_cnt = 0\n",
    "    \n",
    "    # Measure importance on forget samples\n",
    "    print(\"Measuring importance on forget samples...\")\n",
    "    for step, batch in enumerate(tqdm(forget_dataloader, desc=\"Forget importance\")):\n",
    "        if step >= 10:  # Limit to prevent long computation\n",
    "            break\n",
    "            \n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "        \n",
    "        output = model(input_ids, labels=labels, attention_mask=attention_mask)\n",
    "        output.loss.backward()\n",
    "        \n",
    "        cnt = torch.sum(labels != -100)\n",
    "        for n, param in model.named_parameters():\n",
    "            if n in importance_f and param.grad is not None:\n",
    "                importance_f[n] += (param.grad.pow(2) * cnt).detach().cpu()\n",
    "            if param.grad is not None:\n",
    "                param.grad = None\n",
    "        f_cnt += cnt\n",
    "    \n",
    "    # Measure importance on retain samples\n",
    "    print(\"Measuring importance on retain samples...\")\n",
    "    for step, batch in enumerate(tqdm(retain_dataloader, desc=\"Retain importance\")):\n",
    "        if step >= 10:  # Limit to prevent long computation\n",
    "            break\n",
    "            \n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "        \n",
    "        output = model(input_ids, labels=labels, attention_mask=attention_mask)\n",
    "        output.loss.backward()\n",
    "        \n",
    "        cnt = torch.sum(labels != -100)\n",
    "        for n, param in model.named_parameters():\n",
    "            if n in importance_r and param.grad is not None:\n",
    "                importance_r[n] += (param.grad.pow(2) * cnt).detach().cpu()\n",
    "            if param.grad is not None:\n",
    "                param.grad = None\n",
    "        r_cnt += cnt\n",
    "    \n",
    "    # Package results\n",
    "    importances = {\n",
    "        'f_cnt': f_cnt,\n",
    "        'r_cnt': r_cnt,\n",
    "        'importance_f': importance_f,\n",
    "        'importance_r': importance_r\n",
    "    }\n",
    "    \n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(importances, save_path)\n",
    "        print(f\"Fisher information saved to {save_path}\")\n",
    "    \n",
    "    print(\"Fisher information measurement completed!\")\n",
    "    return importances\n",
    "\n",
    "# Example usage to create Fisher information for FILA:\n",
    "# fisher_info = measure_fisher_information(\n",
    "#     tofu_trainer.model, \n",
    "#     forget_train_dataloader, \n",
    "#     retain_train_dataloader,\n",
    "#     save_path=\"fisher_importance.pt\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-07T08:39:17.887920Z",
     "iopub.status.idle": "2025-09-07T08:39:17.888148Z",
     "shell.execute_reply": "2025-09-07T08:39:17.888050Z",
     "shell.execute_reply.started": "2025-09-07T08:39:17.888040Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluation with TOFU trained model\n",
    "print(\"Starting evaluation with TOFU trained model...\")\n",
    "\n",
    "# Check files exist before starting\n",
    "if os.path.exists(\"validation/forget.jsonl\") and os.path.exists(\"validation/retain.jsonl\"):\n",
    "    tofu_model_path = STUDENT_PATH + \"_tofu\"\n",
    "    if os.path.exists(tofu_model_path):\n",
    "        run_evaluation(\n",
    "            data_path=\"validation/\",  \n",
    "            checkpoint_path=tofu_model_path,  \n",
    "            output_dir=\"eval_results_tofu\",\n",
    "            debug=False\n",
    "        )\n",
    "        print(\"✅ TOFU model evaluation completed!\")\n",
    "    else:\n",
    "        print(f\"❌ TOFU model checkpoint not found at {tofu_model_path}\")\n",
    "        print(\"   Make sure the TOFU training completed successfully\")\n",
    "else:\n",
    "    print(\"❌ Validation files not found\")\n",
    "    print(\"   Expected: validation/forget.jsonl and validation/retain.jsonl\")\n",
    "    print(\"   Make sure the data processing completed successfully\")\n",
    "\n",
    "# Optional: Compare with baseline\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(\"✅ TOFU Implementation Features:\")\n",
    "print(\"  - Inverted Hinge Loss (IHL) for forget samples\")\n",
    "print(\"  - Fisher Information measurement capability\")\n",
    "print(\"  - FILA (Fisher Information weighted LoRA) support\")\n",
    "print(\"  - Standard LoRA fine-tuning\")\n",
    "print(\"  - Early stopping with validation\")\n",
    "print(\"\\n🔄 Replaced Dual Teacher approach with TOFU methods\")\n",
    "print(\"📊 Evaluation results saved in eval_results_tofu/\")\n",
    "\n",
    "# Show some example Fisher information usage\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FISHER INFORMATION & FILA USAGE\")\n",
    "print(\"=\"*50)\n",
    "print(\"To enable FILA in future runs:\")\n",
    "print(\"1. First measure Fisher information:\")\n",
    "print(\"   fisher_info = measure_fisher_information(model, forget_dl, retain_dl, 'fisher.pt')\")\n",
    "print(\"2. Then initialize trainer with importance file:\")\n",
    "print(\"   trainer = TOFUTrainer(..., importance_file='fisher.pt')\")\n",
    "print(\"3. FILA will automatically apply weighted LoRA initialization\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8051727,
     "sourceId": 12737770,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8153355,
     "sourceId": 12886974,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8155392,
     "sourceId": 12890024,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8155743,
     "sourceId": 12890625,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8159200,
     "sourceId": 12895681,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8162113,
     "sourceId": 12900193,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
