{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "evaluation-header",
   "metadata": {},
   "source": "# SemEval 2025 Unlearning Challenge - Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-section",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up default paths and parameters for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": "# Default configuration\nDEFAULT_MODEL_PATH = \"./models/best_model\"  # Change this to your model path\nDATA_PATH = \"./data/\"  # SemEval unlearning data (contains parquet files)\nMIA_DATA_PATH = \"./data/mia_data/\"  # MIA attack data (contains JSONL files)\nOUTPUT_DIR = \"./evaluation_results\"\nMMLU_OUTPUT_DIR = \"./mmlu_results\"\nPROCESSED_DATA_PATH = \"./processed_data/\"  # For converted JSONL files\n\n# Evaluation parameters\nMAX_NEW_TOKENS = 256\nBATCH_SIZE = 8  # Adjust based on your GPU memory\nSEED = 42\n\nprint(f\"Configuration set up:\")\nprint(f\"Model path: {DEFAULT_MODEL_PATH}\")\nprint(f\"Data path: {DATA_PATH}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\n\n# List available JSONL files in DATA_PATH\nif os.path.exists(DATA_PATH):\n    jsonl_files = [f for f in os.listdir(DATA_PATH) if f.endswith('.jsonl')]\n    print(f\"Available JSONL files: {jsonl_files}\")\nelse:\n    print(f\"Data directory not found: {DATA_PATH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ggha4j8qkde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick conversion - uncomment to convert parquet files immediately\n",
    "# if parquet_ok and not jsonl_ok:\n",
    "#     print(\"Converting parquet files to JSONL...\")\n",
    "#     convert_parquet_to_jsonl()\n",
    "#     verify_converted_data()\n",
    "#     print(\"Conversion completed!\")\n",
    "# else:\n",
    "#     print(\"Parquet files not found or JSONL files already exist.\")\n",
    "\n",
    "print(\"Ready for data conversion!\")\n",
    "print(\"Uncomment the code above or run convert_parquet_to_jsonl() to convert data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-section",
   "metadata": {},
   "source": [
    "## Verify Data and Model Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_paths():\n",
    "    \"\"\"Verify all required paths exist\"\"\"\n",
    "    checks = {\n",
    "        \"Model path\": DEFAULT_MODEL_PATH,\n",
    "        \"Data path\": DATA_PATH,\n",
    "        \"MIA data path\": MIA_DATA_PATH,\n",
    "        \"MIA member data\": os.path.join(MIA_DATA_PATH, \"member.jsonl\"),\n",
    "        \"MIA nonmember data\": os.path.join(MIA_DATA_PATH, \"nonmember.jsonl\")\n",
    "    }\n",
    "    \n",
    "    # Check parquet files\n",
    "    parquet_checks = {\n",
    "        \"Forget train parquet\": os.path.join(DATA_PATH, \"forget_train-00000-of-00001.parquet\"),\n",
    "        \"Forget validation parquet\": os.path.join(DATA_PATH, \"forget_validation-00000-of-00001.parquet\"),\n",
    "        \"Retain train parquet\": os.path.join(DATA_PATH, \"retain_train-00000-of-00001.parquet\"),\n",
    "        \"Retain validation parquet\": os.path.join(DATA_PATH, \"retain_validation-00000-of-00001.parquet\")\n",
    "    }\n",
    "    \n",
    "    # Check processed JSONL files (if they exist)\n",
    "    jsonl_checks = {\n",
    "        \"Processed forget data\": os.path.join(PROCESSED_DATA_PATH, \"forget.jsonl\"),\n",
    "        \"Processed retain data\": os.path.join(PROCESSED_DATA_PATH, \"retain.jsonl\")\n",
    "    }\n",
    "    \n",
    "    all_good = True\n",
    "    parquet_available = True\n",
    "    jsonl_available = True\n",
    "    \n",
    "    print(\"=== BASIC PATH VERIFICATION ===\")\n",
    "    for name, path in checks.items():\n",
    "        if os.path.exists(path):\n",
    "            print(f\"✓ {name}: {path}\")\n",
    "        else:\n",
    "            print(f\"✗ {name}: {path} [NOT FOUND]\")\n",
    "            all_good = False\n",
    "    \n",
    "    print(\"\\\\n=== PARQUET FILES (SOURCE DATA) ===\")\n",
    "    for name, path in parquet_checks.items():\n",
    "        if os.path.exists(path):\n",
    "            print(f\"✓ {name}: {path}\")\n",
    "        else:\n",
    "            print(f\"✗ {name}: {path} [NOT FOUND]\")\n",
    "            parquet_available = False\n",
    "    \n",
    "    print(\"\\\\n=== PROCESSED JSONL FILES (FOR EVALUATION) ===\")\n",
    "    for name, path in jsonl_checks.items():\n",
    "        if os.path.exists(path):\n",
    "            print(f\"✓ {name}: {path}\")\n",
    "        else:\n",
    "            print(f\"✗ {name}: {path} [NOT FOUND]\")\n",
    "            jsonl_available = False\n",
    "    \n",
    "    if parquet_available and not jsonl_available:\n",
    "        print(\"\\\\n💡 Parquet files found but JSONL files missing.\")\n",
    "        print(\"   Run convert_parquet_to_jsonl() to create evaluation-ready files.\")\n",
    "    elif not parquet_available:\n",
    "        print(\"\\\\n❌ Source parquet files missing. Cannot proceed with evaluation.\")\n",
    "    elif jsonl_available:\n",
    "        print(\"\\\\n✅ Both parquet and JSONL files available. Ready for evaluation!\")\n",
    "    \n",
    "    return all_good, parquet_available, jsonl_available\n",
    "\n",
    "# Create output directories\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(MMLU_OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(PROCESSED_DATA_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "paths_ok, parquet_ok, jsonl_ok = verify_paths()\n",
    "print(f\"\\\\nSummary:\")\n",
    "print(f\"- Basic paths OK: {paths_ok}\")\n",
    "print(f\"- Parquet files available: {parquet_ok}\")  \n",
    "print(f\"- JSONL files available: {jsonl_ok}\")\n",
    "print(f\"- Ready for evaluation: {jsonl_ok}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mmlu-section",
   "metadata": {},
   "source": [
    "## MMLU Evaluation\n",
    "\n",
    "Run MMLU evaluation to assess general knowledge retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mmlu-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mmlu_subject(model, tokenizer, subject, max_samples=None):\n",
    "    \"\"\"Evaluate model on a specific MMLU subject\"\"\"\n",
    "    dataset = load_dataset(\"hendrycks/test\", subject)[\"test\"]\n",
    "    \n",
    "    if max_samples:\n",
    "        dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for example in tqdm(dataset, desc=f\"Evaluating {subject}\"):\n",
    "        question = example[\"question\"]\n",
    "        choices = example[\"choices\"]\n",
    "        correct_answer = example[\"answer\"]\n",
    "        \n",
    "        # Format the question\n",
    "        prompt = f\"Question: {question}\\n\"\n",
    "        for i, choice in enumerate(choices):\n",
    "            prompt += f\"{chr(65+i)}. {choice}\\n\"\n",
    "        prompt += \"Answer:\"\n",
    "        \n",
    "        # Tokenize and generate\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=5,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated = tokenizer.decode(outputs[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Extract the answer (A, B, C, or D)\n",
    "        predicted_answer = None\n",
    "        for char in generated.upper():\n",
    "            if char in \"ABCD\":\n",
    "                predicted_answer = ord(char) - ord(\"A\")\n",
    "                break\n",
    "        \n",
    "        if predicted_answer == correct_answer:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy, correct, total\n",
    "\n",
    "def run_mmlu_evaluation(model_path, subjects=None, max_samples_per_subject=None):\n",
    "    \"\"\"Run MMLU evaluation on specified subjects\"\"\"\n",
    "    if subjects is None:\n",
    "        subjects = MMLU_SUBJECTS\n",
    "    \n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    results = {}\n",
    "    total_correct = 0\n",
    "    total_questions = 0\n",
    "    \n",
    "    for subject in subjects:\n",
    "        try:\n",
    "            print(f\"\\nEvaluating {subject}...\")\n",
    "            accuracy, correct, total = evaluate_mmlu_subject(\n",
    "                model, tokenizer, subject, max_samples_per_subject\n",
    "            )\n",
    "            results[subject] = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"correct\": correct,\n",
    "                \"total\": total\n",
    "            }\n",
    "            total_correct += correct\n",
    "            total_questions += total\n",
    "            print(f\"{subject}: {accuracy:.3f} ({correct}/{total})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating {subject}: {e}\")\n",
    "            results[subject] = {\"error\": str(e)}\n",
    "    \n",
    "    overall_accuracy = total_correct / total_questions if total_questions > 0 else 0\n",
    "    results[\"average_acc\"] = overall_accuracy\n",
    "    results[\"total_correct\"] = total_correct\n",
    "    results[\"total_questions\"] = total_questions\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = os.path.join(MMLU_OUTPUT_DIR, f\"mmlu_results_{timestamp}.json\")\n",
    "    with open(results_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nOverall MMLU accuracy: {overall_accuracy:.3f} ({total_correct}/{total_questions})\")\n",
    "    print(f\"Results saved to {results_file}\")\n",
    "    \n",
    "    return results_file, results\n",
    "\n",
    "# Quick MMLU test on a subset of subjects (for faster testing)\n",
    "QUICK_SUBJECTS = [\"abstract_algebra\", \"anatomy\", \"astronomy\", \"business_ethics\", \"clinical_knowledge\"]\n",
    "\n",
    "print(\"Ready to run MMLU evaluation!\")\n",
    "print(f\"Quick test subjects: {QUICK_SUBJECTS}\")\n",
    "print(\"Run the next cell to start evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-mmlu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MMLU evaluation (uncomment to run)\n",
    "# Choose one of the following options:\n",
    "\n",
    "# Option 1: Quick test (5 subjects, 50 samples each)\n",
    "# mmlu_results_file, mmlu_results = run_mmlu_evaluation(\n",
    "#     DEFAULT_MODEL_PATH, \n",
    "#     subjects=QUICK_SUBJECTS, \n",
    "#     max_samples_per_subject=50\n",
    "# )\n",
    "\n",
    "# Option 2: Full evaluation (all subjects)\n",
    "# mmlu_results_file, mmlu_results = run_mmlu_evaluation(DEFAULT_MODEL_PATH)\n",
    "\n",
    "# Option 3: Manual specification\n",
    "# mmlu_results_file, mmlu_results = run_mmlu_evaluation(\n",
    "#     DEFAULT_MODEL_PATH,\n",
    "#     subjects=[\"high_school_mathematics\", \"college_mathematics\", \"machine_learning\"],\n",
    "#     max_samples_per_subject=100\n",
    "# )\n",
    "\n",
    "print(\"Uncomment one of the options above to run MMLU evaluation.\")\n",
    "\n",
    "# For demonstration, let's assume we have results file\n",
    "mmlu_results_file = None  # Set this to your actual results file path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-eval-section",
   "metadata": {},
   "source": [
    "## Official SemEval Evaluation\n",
    "\n",
    "Run the official evaluation script with proper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-official-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_official_evaluation(model_path, data_path, mia_data_path=None, mmlu_file=None, \n",
    "                           max_new_tokens=256, batch_size=8, debug=False, use_local_script=False):\n",
    "    \"\"\"Run the official evaluation script\"\"\"\n",
    "    \n",
    "    # Choose which evaluation script to use\n",
    "    if use_local_script and os.path.exists(os.path.join(\"data\", \"evaluate_generations.py\")):\n",
    "        script_path = os.path.join(\"data\", \"evaluate_generations.py\")\n",
    "        print(\"Using local evaluation script from data/ folder\")\n",
    "    else:\n",
    "        script_path = \"evaluation.py\"\n",
    "        print(\"Using root evaluation script\")\n",
    "    \n",
    "    # Prepare command\n",
    "    cmd = [\n",
    "        \"python\", script_path,\n",
    "        \"--data_path\", data_path,\n",
    "        \"--checkpoint_path\", model_path,\n",
    "        \"--output_dir\", OUTPUT_DIR,\n",
    "        \"--max_new_tokens\", str(max_new_tokens),\n",
    "        \"--batch_size\", str(batch_size),\n",
    "        \"--seed\", str(SEED)\n",
    "    ]\n",
    "    \n",
    "    if mia_data_path and os.path.exists(mia_data_path):\n",
    "        cmd.extend([\"--mia_data_path\", mia_data_path])\n",
    "    \n",
    "    if mmlu_file and os.path.exists(mmlu_file):\n",
    "        cmd.extend([\"--mmlu_metrics_file_path\", mmlu_file])\n",
    "    \n",
    "    if debug:\n",
    "        cmd.append(\"--debug\")\n",
    "    \n",
    "    print(\"Running official evaluation with command:\")\n",
    "    print(\" \".join(cmd))\n",
    "    print(\"\\\\nThis may take a while...\\\\n\")\n",
    "    \n",
    "    # Run the evaluation\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=7200)  # 2 hour timeout\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"✓ Evaluation completed successfully!\")\n",
    "            print(\"STDOUT:\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # Load results\n",
    "            results_file = os.path.join(OUTPUT_DIR, \"evaluation_results.jsonl\")\n",
    "            if os.path.exists(results_file):\n",
    "                with open(results_file, \"r\") as f:\n",
    "                    results = json.load(f)\n",
    "                return results\n",
    "            else:\n",
    "                print(f\"Results file not found: {results_file}\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"✗ Evaluation failed with return code {result.returncode}\")\n",
    "            print(\"STDERR:\")\n",
    "            print(result.stderr)\n",
    "            print(\"STDOUT:\")\n",
    "            print(result.stdout)\n",
    "            return None\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"✗ Evaluation timed out after 2 hours\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Evaluation failed with error: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Official evaluation function ready!\")\n",
    "print(\"Note: The function will automatically use processed JSONL files for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute-official-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the official evaluation\n",
    "print(\"=== AUTOMATED EVALUATION PIPELINE ===\\\\n\")\n",
    "\n",
    "# Step 1: Convert parquet to JSONL if needed\n",
    "if parquet_ok and not jsonl_ok:\n",
    "    print(\"Step 1: Converting parquet files to JSONL format...\")\n",
    "    convert_parquet_to_jsonl()\n",
    "    print(\"\\\\nStep 1.5: Verifying converted data...\")\n",
    "    data_verification_ok = verify_converted_data()\n",
    "    if not data_verification_ok:\n",
    "        print(\"❌ Data conversion failed. Cannot proceed with evaluation.\")\n",
    "    else:\n",
    "        print(\"✅ Data conversion successful!\")\n",
    "        jsonl_ok = True\n",
    "elif jsonl_ok:\n",
    "    print(\"Step 1: JSONL files already exist, skipping conversion.\")\n",
    "    data_verification_ok = verify_converted_data()\n",
    "else:\n",
    "    print(\"❌ No parquet files found. Cannot convert data.\")\n",
    "    data_verification_ok = False\n",
    "\n",
    "# Step 2: Run evaluation if data is ready\n",
    "if jsonl_ok and data_verification_ok:\n",
    "    print(\"\\\\n\" + \"=\"*50)\n",
    "    print(\"Step 2: Starting official SemEval 2025 evaluation...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    eval_results = run_official_evaluation(\n",
    "        model_path=DEFAULT_MODEL_PATH,\n",
    "        data_path=PROCESSED_DATA_PATH,  # Use processed JSONL data\n",
    "        mia_data_path=MIA_DATA_PATH,\n",
    "        mmlu_file=mmlu_results_file if 'mmlu_results_file' in locals() else None,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        debug=True,  # Set to False for less verbose output\n",
    "        use_local_script=False  # Set to True to use data/evaluate_generations.py\n",
    "    )\n",
    "    \n",
    "    if eval_results:\n",
    "        print(\"\\\\n\" + \"=\"*60)\n",
    "        print(\"EVALUATION RESULTS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Key metrics\n",
    "        if 'aggregate-score' in eval_results:\n",
    "            print(f\"Final Aggregate Score: {eval_results['aggregate-score']:.4f}\")\n",
    "        \n",
    "        if 'harmonic-mean-task-aggregate' in eval_results:\n",
    "            print(f\"Task Aggregate (Harmonic Mean): {eval_results['harmonic-mean-task-aggregate']:.4f}\")\n",
    "        \n",
    "        if 'mmlu_average' in eval_results:\n",
    "            print(f\"MMLU Average: {eval_results['mmlu_average']:.4f}\")\n",
    "        \n",
    "        if 'mia_loss_acc' in eval_results:\n",
    "            print(f\"MIA Loss Accuracy: {eval_results['mia_loss_acc']:.4f}\")\n",
    "            if 'mia_final_score' in eval_results:\n",
    "                print(f\"MIA Final Score: {eval_results['mia_final_score']:.4f}\")\n",
    "        \n",
    "        # Task-specific results\n",
    "        print(\"\\\\nTask-specific Results:\")\n",
    "        for key in ['forget-set', 'retain-set']:\n",
    "            if key in eval_results:\n",
    "                print(f\"\\\\n{key.upper()}:\")\n",
    "                task_results = eval_results[key]\n",
    "                if 'overall-regurgitation-score' in task_results:\n",
    "                    print(f\"  Overall Regurgitation: {task_results['overall-regurgitation-score']:.4f}\")\n",
    "                if 'overall-knowledge-score' in task_results:\n",
    "                    print(f\"  Overall Knowledge: {task_results['overall-knowledge-score']:.4f}\")\n",
    "                \n",
    "                # Task-specific breakdowns\n",
    "                for task_key, task_data in task_results.items():\n",
    "                    if task_key.startswith('Task') and isinstance(task_data, dict):\n",
    "                        print(f\"  {task_key}:\")\n",
    "                        for metric, value in task_data.items():\n",
    "                            print(f\"    {metric}: {value:.4f}\")\n",
    "        \n",
    "        # Step 3: Save detailed analysis\n",
    "        print(\"\\\\n\" + \"=\"*50)\n",
    "        print(\"Step 3: Saving detailed analysis...\")\n",
    "        save_evaluation_summary(eval_results)\n",
    "        print(\"✅ Evaluation pipeline completed successfully!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Evaluation failed. Check the error messages above.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot run evaluation:\")\n",
    "    if not parquet_ok:\n",
    "        print(\"  - Parquet source files not found\")\n",
    "    if not jsonl_ok:\n",
    "        print(\"  - JSONL conversion failed\") \n",
    "    if not data_verification_ok:\n",
    "        print(\"  - Data verification failed\")\n",
    "    \n",
    "    print(\"\\\\n💡 TROUBLESHOOTING TIPS:\")\n",
    "    print(\"1. Ensure the DEFAULT_MODEL_PATH points to your trained model\")\n",
    "    print(\"2. Check that parquet files exist in the data/ folder\")\n",
    "    print(\"3. Verify MIA data files exist in data/mia_data/\")\n",
    "    print(\"4. Run convert_parquet_to_jsonl() manually if conversion fails\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-section",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Analyze and visualize the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_evaluation_results(results):\n",
    "    \"\"\"Provide detailed analysis of evaluation results\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"DETAILED RESULTS ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Overall performance\n",
    "    if 'aggregate-score' in results:\n",
    "        score = results['aggregate-score']\n",
    "        print(f\"\\n🎯 FINAL SCORE: {score:.4f}\")\n",
    "        \n",
    "        if score >= 0.8:\n",
    "            print(\"   Status: Excellent performance! 🏆\")\n",
    "        elif score >= 0.6:\n",
    "            print(\"   Status: Good performance 👍\")\n",
    "        elif score >= 0.4:\n",
    "            print(\"   Status: Moderate performance ⚠️\")\n",
    "        else:\n",
    "            print(\"   Status: Needs improvement 🔧\")\n",
    "    \n",
    "    # Unlearning effectiveness\n",
    "    print(\"\\n📊 UNLEARNING ANALYSIS:\")\n",
    "    if 'forget-set' in results:\n",
    "        forget_results = results['forget-set']\n",
    "        forget_regurg = forget_results.get('overall-regurgitation-score', 0)\n",
    "        forget_knowledge = forget_results.get('overall-knowledge-score', 0)\n",
    "        \n",
    "        print(f\"   Forget Regurgitation: {forget_regurg:.4f} (lower is better)\")\n",
    "        print(f\"   Forget Knowledge: {forget_knowledge:.4f} (lower is better)\")\n",
    "        \n",
    "        if forget_regurg < 0.3 and forget_knowledge < 0.3:\n",
    "            print(\"   ✅ Excellent forgetting - model successfully unlearned target information\")\n",
    "        elif forget_regurg < 0.5 and forget_knowledge < 0.5:\n",
    "            print(\"   🟡 Good forgetting - reasonable unlearning performance\")\n",
    "        else:\n",
    "            print(\"   ❌ Poor forgetting - model retains too much target information\")\n",
    "    \n",
    "    # Knowledge retention\n",
    "    print(\"\\n🧠 KNOWLEDGE RETENTION:\")\n",
    "    if 'retain-set' in results:\n",
    "        retain_results = results['retain-set']\n",
    "        retain_regurg = retain_results.get('overall-regurgitation-score', 0)\n",
    "        retain_knowledge = retain_results.get('overall-knowledge-score', 0)\n",
    "        \n",
    "        print(f\"   Retain Regurgitation: {retain_regurg:.4f} (higher is better)\")\n",
    "        print(f\"   Retain Knowledge: {retain_knowledge:.4f} (higher is better)\")\n",
    "        \n",
    "        if retain_regurg > 0.7 and retain_knowledge > 0.7:\n",
    "            print(\"   ✅ Excellent retention - model preserves important knowledge\")\n",
    "        elif retain_regurg > 0.5 and retain_knowledge > 0.5:\n",
    "            print(\"   🟡 Good retention - acceptable knowledge preservation\")\n",
    "        else:\n",
    "            print(\"   ❌ Poor retention - model lost important knowledge (catastrophic forgetting)\")\n",
    "    \n",
    "    # General knowledge (MMLU)\n",
    "    if 'mmlu_average' in results:\n",
    "        mmlu_score = results['mmlu_average']\n",
    "        print(f\"\\n📚 GENERAL KNOWLEDGE (MMLU): {mmlu_score:.4f}\")\n",
    "        \n",
    "        if mmlu_score >= 0.371:  # 75% of baseline threshold\n",
    "            print(\"   ✅ Meets general knowledge threshold\")\n",
    "        else:\n",
    "            print(\"   ❌ Below general knowledge threshold - may affect final ranking\")\n",
    "    \n",
    "    # MIA resistance\n",
    "    if 'mia_loss_acc' in results:\n",
    "        mia_acc = results['mia_loss_acc']\n",
    "        print(f\"\\n🛡️ MIA RESISTANCE: {mia_acc:.4f}\")\n",
    "        print(f\"   Distance from ideal (0.5): {abs(mia_acc - 0.5):.4f}\")\n",
    "        \n",
    "        if abs(mia_acc - 0.5) < 0.1:\n",
    "            print(\"   ✅ Excellent MIA resistance - balanced unlearning\")\n",
    "        elif abs(mia_acc - 0.5) < 0.2:\n",
    "            print(\"   🟡 Good MIA resistance\")\n",
    "        else:\n",
    "            if mia_acc > 0.7:\n",
    "                print(\"   ❌ Poor MIA resistance - under-unlearning detected\")\n",
    "            else:\n",
    "                print(\"   ❌ Poor MIA resistance - over-unlearning detected\")\n",
    "    \n",
    "    # Task breakdown\n",
    "    print(\"\\n📋 TASK-SPECIFIC BREAKDOWN:\")\n",
    "    for split in ['forget-set', 'retain-set']:\n",
    "        if split not in results:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n   {split.replace('-set', '').upper()} SET:\")\n",
    "        split_data = results[split]\n",
    "        \n",
    "        for key, value in split_data.items():\n",
    "            if key.startswith('Task') and isinstance(value, dict):\n",
    "                print(f\"     {key}:\")\n",
    "                for metric, score in value.items():\n",
    "                    print(f\"       {metric}: {score:.4f}\")\n",
    "\n",
    "# Run analysis if we have results\n",
    "if 'eval_results' in locals() and eval_results:\n",
    "    analyze_evaluation_results(eval_results)\n",
    "else:\n",
    "    print(\"No evaluation results available. Run the evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils-section",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Additional helper functions for evaluation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_previous_results(results_file):\n",
    "    \"\"\"Load results from a previous evaluation\"\"\"\n",
    "    if os.path.exists(results_file):\n",
    "        with open(results_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        print(f\"Results file not found: {results_file}\")\n",
    "        return None\n",
    "\n",
    "def compare_results(results1, results2, names=None):\n",
    "    \"\"\"Compare two evaluation results\"\"\"\n",
    "    if names is None:\n",
    "        names = [\"Model 1\", \"Model 2\"]\n",
    "    \n",
    "    print(f\"COMPARISON: {names[0]} vs {names[1]}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    metrics = [\n",
    "        ('aggregate-score', 'Final Score'),\n",
    "        ('harmonic-mean-task-aggregate', 'Task Aggregate'),\n",
    "        ('mmlu_average', 'MMLU Average'),\n",
    "        ('mia_final_score', 'MIA Final Score')\n",
    "    ]\n",
    "    \n",
    "    for key, label in metrics:\n",
    "        val1 = results1.get(key, 'N/A')\n",
    "        val2 = results2.get(key, 'N/A')\n",
    "        \n",
    "        if isinstance(val1, float) and isinstance(val2, float):\n",
    "            diff = val2 - val1\n",
    "            arrow = \"↑\" if diff > 0 else \"↓\" if diff < 0 else \"→\"\n",
    "            print(f\"{label:20}: {val1:.4f} vs {val2:.4f} ({arrow} {abs(diff):.4f})\")\n",
    "        else:\n",
    "            print(f\"{label:20}: {val1} vs {val2}\")\n",
    "\n",
    "def save_evaluation_summary(results, output_file=None):\n",
    "    \"\"\"Save a human-readable summary of evaluation results\"\"\"\n",
    "    if output_file is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"evaluation_summary_{timestamp}.txt\"\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"SemEval 2025 Unlearning Challenge - Evaluation Summary\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        if 'aggregate-score' in results:\n",
    "            f.write(f\"Final Aggregate Score: {results['aggregate-score']:.4f}\\n\\n\")\n",
    "        \n",
    "        # Write all metrics\n",
    "        f.write(\"Detailed Results:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                f.write(f\"\\n{key}:\\n\")\n",
    "                for subkey, subvalue in value.items():\n",
    "                    f.write(f\"  {subkey}: {subvalue}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    print(f\"Summary saved to {output_file}\")\n",
    "\n",
    "print(\"Utility functions loaded!\")\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"- load_previous_results(file): Load previous evaluation results\")\n",
    "print(\"- compare_results(r1, r2, names): Compare two evaluation results\")\n",
    "print(\"- save_evaluation_summary(results, file): Save human-readable summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-usage",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Here's how to use this notebook step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-workflow",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP-BY-STEP EVALUATION WORKFLOW:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"1. Update DEFAULT_MODEL_PATH in the configuration cell\")\n",
    "print(\"2. Verify all data paths are correct\")\n",
    "print(\"3. (Optional) Run MMLU evaluation first\")\n",
    "print(\"4. Run the official SemEval evaluation\")\n",
    "print(\"5. Analyze results using the analysis functions\")\n",
    "print(\"\")\n",
    "print(\"QUICK START:\")\n",
    "print(\"1. Set DEFAULT_MODEL_PATH = '/path/to/your/model'\")\n",
    "print(\"2. Run all cells in order\")\n",
    "print(\"3. Uncomment the evaluation calls when ready\")\n",
    "print(\"\")\n",
    "print(\"FILES GENERATED:\")\n",
    "print(f\"- MMLU results: {MMLU_OUTPUT_DIR}/mmlu_results_*.json\")\n",
    "print(f\"- SemEval results: {OUTPUT_DIR}/evaluation_results.jsonl\")\n",
    "print(\"- CSV files with detailed predictions (if keep_files=True)\")\n",
    "print(\"\")\n",
    "print(\"Ready to start evaluation! 🚀\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}