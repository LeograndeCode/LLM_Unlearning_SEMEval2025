{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Unlearning for SEMEval 2025: SKU Experiments\n",
    "\n",
    "Authors: [Your Name], [Collaborators]\n",
    "Course: [Course Name], [University]\n",
    "Date: 2025-09-15\n",
    "\n",
    "Abstract: This notebook reproduces SKU experiments for LLM unlearning as part of our SEMEval 2025 submission. It provides a clean, reproducible workflow: environment setup, configuration, data loading, training/evaluation, and results export with metadata. All random seeds are fixed, and package versions are logged for determinism.\n",
    "\n",
    "Outline:\n",
    "- Setup and Reproducibility\n",
    "- Configuration and Paths\n",
    "- Data Preparation\n",
    "- Training and Unlearning\n",
    "- Evaluation\n",
    "- Results and Export\n",
    "- References & Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Reproducibility\n",
    "import os, sys, platform, random, json, time, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Optional imports guarded for environments without these libs\n",
    "try:\n",
    "    import torch\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "except Exception:\n",
    "    torch = None\n",
    "\n",
    "# Seeds\n",
    "SEED = int(os.getenv(\"SEED\", 42))\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch:\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Paths\n",
    "ROOT = Path.cwd()\n",
    "DATA_DIR = ROOT / \"train\"\n",
    "VAL_DIR = ROOT / \"validation\"\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Environment snapshot\n",
    "ENV_INFO = {\n",
    "    \"timestamp\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"platform\": platform.platform(),\n",
    "    \"executable\": sys.executable,\n",
    "    \"packages\": {}\n",
    "}\n",
    "\n",
    "# Capture key packages if installed\n",
    "for pkg in [\"torch\", \"transformers\", \"datasets\", \"accelerate\", \"numpy\", \"pandas\", \"scikit-learn\"]:\n",
    "    try:\n",
    "        mod = __import__(pkg.replace(\"-\", \"_\"))\n",
    "        ver = getattr(mod, \"__version__\", \"unknown\")\n",
    "        ENV_INFO[\"packages\"][pkg] = ver\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(json.dumps(ENV_INFO, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Paths\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional\n",
    "\n",
    "try:\n",
    "    # Prefer project config if available\n",
    "    import config as project_config\n",
    "except Exception:\n",
    "    project_config = None\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name: str = getattr(project_config, \"MODEL_NAME\", \"gpt2\")\n",
    "    batch_size: int = getattr(project_config, \"BATCH_SIZE\", 8)\n",
    "    lr: float = getattr(project_config, \"LEARNING_RATE\", 5e-5)\n",
    "    num_epochs: int = getattr(project_config, \"NUM_EPOCHS\", 1)\n",
    "    max_length: int = getattr(project_config, \"MAX_LENGTH\", 256)\n",
    "    eval_batch_size: int = getattr(project_config, \"EVAL_BATCH_SIZE\", 8)\n",
    "    output_dir: str = str(OUT_DIR)\n",
    "    data_dir: str = str(DATA_DIR)\n",
    "    val_dir: str = str(VAL_DIR)\n",
    "    seed: int = SEED\n",
    "\n",
    "CFG = Config()\n",
    "print(\"Config:\\n\", json.dumps(asdict(CFG), indent=2))\n",
    "\n",
    "# Ensure expected directories exist\n",
    "for p in [CFG.output_dir, CFG.data_dir, CFG.val_dir]:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selective Knowledge Negation Unlearning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Run\n",
    "\n",
    "- Run cells from top to bottom. The setup cells create output folders and log environment details for reproducibility.\n",
    "- Adjust `Config` parameters in the Configuration cell as needed.\n",
    "- Ensure `train/` and `validation/` contain `retain.jsonl` and `forget.jsonl` files as per the project.\n",
    "- Results and logs will be saved under `outputs/` and appended to `evaluation_results.jsonl` at the project root.\n",
    "\n",
    "If you encounter missing packages, install them per `requirements.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T13:55:43.170404Z",
     "iopub.status.busy": "2025-09-14T13:55:43.169759Z",
     "iopub.status.idle": "2025-09-14T13:57:27.675021Z",
     "shell.execute_reply": "2025-09-14T13:57:27.674188Z",
     "shell.execute_reply.started": "2025-09-14T13:55:43.170379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dependencies & GPU Info (avoid pip in academic submission; use requirements.txt)\n",
    "import importlib, shutil\n",
    "REQUIRED = [\"torch\", \"transformers\", \"peft\", \"huggingface_hub\", \"pyarrow\", \"pandas\", \"tqdm\", \"rouge_score\"]\n",
    "missing = []\n",
    "for pkg in REQUIRED:\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except Exception:\n",
    "        missing.append(pkg)\n",
    "if missing:\n",
    "    print(\"⚠️ Missing packages:\", missing)\n",
    "    print(\"Install them via: pip install -r requirements.txt (or project managed environment)\")\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, math, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Model source (local mirror fallback)\n",
    "MODEL_PATH = os.getenv(\"MODEL_PATH\", \"semeval25-unlearning-1B-model\")\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    try:\n",
    "        snapshot_download(repo_id='llmunlearningsemeval2025organization/olmo-1B-model-semeval25-unlearning', local_dir=MODEL_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not download model snapshot: {e}\")\n",
    "\n",
    "print(f\"GPUs available: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Caricamento Dati e Modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T13:58:35.916095Z",
     "iopub.status.busy": "2025-09-14T13:58:35.915799Z",
     "iopub.status.idle": "2025-09-14T13:59:08.611126Z",
     "shell.execute_reply": "2025-09-14T13:59:08.610224Z",
     "shell.execute_reply.started": "2025-09-14T13:58:35.916071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data loading (supports parquet or jsonl)\n",
    "from pathlib import Path\n",
    "\n",
    "def load_split(path: str):\n",
    "    p = Path(path)\n",
    "    if p.suffix == '.parquet':\n",
    "        return pd.read_parquet(p)\n",
    "    elif p.suffix in {'.jsonl', '.json'}:\n",
    "        rows = []\n",
    "        with open(p, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                rows.append(json.loads(line))\n",
    "        return pd.DataFrame(rows)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {p}\")\n",
    "\n",
    "DATA_ROOT = Path(os.getenv('DATA_ROOT', '.'))\n",
    "retain_train_path = os.getenv('RETAIN_TRAIN', str(DATA_ROOT / 'train' / 'retain.jsonl'))\n",
    "forget_train_path = os.getenv('FORGET_TRAIN', str(DATA_ROOT / 'train' / 'forget.jsonl'))\n",
    "retain_val_path = os.getenv('RETAIN_VAL', str(DATA_ROOT / 'validation' / 'retain.jsonl'))\n",
    "forget_val_path = os.getenv('FORGET_VAL', str(DATA_ROOT / 'validation' / 'forget.jsonl'))\n",
    "\n",
    "# Fallback: if parquet paths exist (original Kaggle style), use them\n",
    "parquet_candidates = {\n",
    "    'retain_train': 'retain_train-00000-of-00001.parquet',\n",
    "    'forget_train': 'forget_train-00000-of-00001.parquet',\n",
    "    'retain_validation': 'retain_validation-00000-of-00001.parquet',\n",
    "    'forget_validation': 'forget_validation-00000-of-00001.parquet'\n",
    "}\n",
    "for key, fname in parquet_candidates.items():\n",
    "    candidate = Path('/kaggle/input/olmo-model/semeval25-unlearning-data/data') / fname\n",
    "    if candidate.exists():\n",
    "        if 'retain_train' == key: retain_train_path = str(candidate)\n",
    "        if 'forget_train' == key: forget_train_path = str(candidate)\n",
    "        if 'retain_validation' == key: retain_val_path = str(candidate)\n",
    "        if 'forget_validation' == key: forget_val_path = str(candidate)\n",
    "\n",
    "retain_train_df = load_split(retain_train_path)\n",
    "forget_train_df = load_split(forget_train_path)\n",
    "retain_validation_df = load_split(retain_val_path)\n",
    "forget_validation_df = load_split(forget_val_path)\n",
    "\n",
    "# Tokenizer\n",
    "if 'tokenizer' not in globals():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = 'right'\n",
    "\n",
    "print(\"Datasets loaded & tokenizer ready\")\n",
    "print({\n",
    "    'retain_train': len(retain_train_df),\n",
    "    'forget_train': len(forget_train_df),\n",
    "    'retain_validation': len(retain_validation_df),\n",
    "    'forget_validation': len(forget_validation_df),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T13:59:19.311285Z",
     "iopub.status.busy": "2025-09-14T13:59:19.310997Z",
     "iopub.status.idle": "2025-09-14T13:59:19.333953Z",
     "shell.execute_reply": "2025-09-14T13:59:19.333057Z",
     "shell.execute_reply.started": "2025-09-14T13:59:19.311258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Sanity check lettura\n",
    "print(\"Train (retain, forget) sizes:\", len(retain_train_df), len(forget_train_df))\n",
    "print(\"Columns:\", list(retain_train_df.columns))\n",
    "print(retain_train_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T14:06:17.236231Z",
     "iopub.status.busy": "2025-09-14T14:06:17.235515Z",
     "iopub.status.idle": "2025-09-14T14:06:17.248660Z",
     "shell.execute_reply": "2025-09-14T14:06:17.247839Z",
     "shell.execute_reply.started": "2025-09-14T14:06:17.236197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UnlearningDataset(Dataset):\n",
    "    \"\"\"Dataset for Selective Knowledge Unlearning.\n",
    "\n",
    "    Each record must contain:\n",
    "      - input: prompt text\n",
    "      - output: ground-truth answer text (to be preserved for retain; suppressed for forget)\n",
    "      - split: 'retain' or 'forget'\n",
    "\n",
    "    Strategy:\n",
    "      1. Tokenize prompt and answer separately to create a clean boundary.\n",
    "      2. Truncate prompt first (max_length). Remaining budget allocated to answer tokens.\n",
    "      3. Keep track of how many answer tokens survive truncation (answer_len_kept).\n",
    "      4. start_locs indexes the first answer token BEFORE shift (model targets are shifted by one).\n",
    "\n",
    "    Returns plain Python lists for efficiency; collate_fn converts to tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_source: pd.DataFrame | str, tokenizer, max_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        if isinstance(data_source, pd.DataFrame):\n",
    "            self.data = data_source\n",
    "            print(f\"Loaded {len(self.data)} examples from DataFrame\")\n",
    "        elif isinstance(data_source, str):\n",
    "            data_list = []\n",
    "            with open(data_source, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        data_list.append(json.loads(line))\n",
    "            self.data = pd.DataFrame(data_list)\n",
    "            print(f\"Loaded {len(self.data)} examples from {data_source}\")\n",
    "        else:\n",
    "            raise TypeError(\"data_source must be DataFrame or path to jsonl\")\n",
    "        # Basic validation\n",
    "        expected_cols = {\"input\", \"output\", \"split\"}\n",
    "        missing = expected_cols - set(self.data.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.data.iloc[idx]\n",
    "        prompt_text = str(row[\"input\"]) if row[\"input\"] is not None else \"\"\n",
    "        answer_text = str(row[\"output\"]) if row[\"output\"] is not None else \"\"\n",
    "\n",
    "        prompt_tok = self.tokenizer(\n",
    "            prompt_text,\n",
    "            add_special_tokens=False,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        answer_text_sp = answer_text if answer_text.startswith(\" \") else (\" \" + answer_text)\n",
    "        answer_tok = self.tokenizer(\n",
    "            answer_text_sp,\n",
    "            add_special_tokens=False,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        prompt_ids = prompt_tok[\"input_ids\"]\n",
    "        answer_ids = answer_tok[\"input_ids\"]\n",
    "        prompt_len = len(prompt_ids)\n",
    "        ans_len = len(answer_ids)\n",
    "        if prompt_len >= self.max_length:\n",
    "            input_ids = prompt_ids[: self.max_length]\n",
    "            answer_len_kept = 0\n",
    "        else:\n",
    "            available = self.max_length - prompt_len\n",
    "            answer_len_kept = min(ans_len, max(0, available))\n",
    "            input_ids = prompt_ids + answer_ids[:answer_len_kept]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        labels = list(input_ids)\n",
    "        ans_start = prompt_len\n",
    "        start_locs = min(ans_start, len(input_ids) - 1)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"start_locs\": start_locs,\n",
    "            \"answer_len_kept\": int(answer_len_kept),\n",
    "            \"labels\": labels,\n",
    "            \"split\": 1 if row[\"split\"] == \"forget\" else 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T14:06:20.192039Z",
     "iopub.status.busy": "2025-09-14T14:06:20.191701Z",
     "iopub.status.idle": "2025-09-14T14:06:20.207116Z",
     "shell.execute_reply": "2025-09-14T14:06:20.206208Z",
     "shell.execute_reply.started": "2025-09-14T14:06:20.192012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "# Reduce max_length slightly to lower memory footprint\n",
    "batch_size = 2\n",
    "train_data = pd.concat([retain_train_df, forget_train_df], ignore_index=True)\n",
    "\n",
    "train_data = train_data.dropna(subset=[\"input\", \"output\"]).reset_index(drop=True)\n",
    "\n",
    "dataset = UnlearningDataset(train_data, tokenizer, max_length=384)\n",
    "\n",
    "# Dynamic padding collate_fn\n",
    "def sku_collate_fn(batch, pad_id, max_length=384):\n",
    "    bs = len(batch)\n",
    "    lengths = [min(len(item['input_ids']), max_length) for item in batch]\n",
    "    max_len = max(lengths) if lengths else 1\n",
    "    input_ids = torch.full((bs, max_len), pad_id, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((bs, max_len), dtype=torch.long)\n",
    "    labels = torch.full((bs, max_len), -100, dtype=torch.long)\n",
    "    start_locs = []\n",
    "    answer_lens = []\n",
    "    splits = []\n",
    "    for i, item in enumerate(batch):\n",
    "        ids = item['input_ids'][:max_length]\n",
    "        lbls = item['labels'][:max_length]\n",
    "        L = len(ids)\n",
    "        if L > 0:\n",
    "            input_ids[i, :L] = torch.tensor(ids, dtype=torch.long)\n",
    "            attention_mask[i, :L] = 1\n",
    "            labels[i, :L] = torch.tensor(lbls, dtype=torch.long)\n",
    "        s = min(int(item['start_locs']), max(L - 1, 0))\n",
    "        start_locs.append(s)\n",
    "        # Clamp answer length to what's actually present after truncation\n",
    "        kept = int(item.get('answer_len_kept', 0))\n",
    "        kept = max(0, min(kept, max(0, L - s)))\n",
    "        answer_lens.append(kept)\n",
    "        splits.append(int(item['split']))\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "        'start_locs': torch.tensor(start_locs, dtype=torch.long),\n",
    "        'answer_len_kept': torch.tensor(answer_lens, dtype=torch.long),\n",
    "        'split': torch.tensor(splits, dtype=torch.long),\n",
    "    }\n",
    "\n",
    "pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "use_pin_memory = torch.cuda.is_available()\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda b: sku_collate_fn(b, pad_id, max_length=384),\n",
    "    pin_memory=use_pin_memory,\n",
    "    num_workers=2 if use_pin_memory else 0,\n",
    "    persistent_workers=False,\n",
    ")\n",
    "\n",
    "print(f\"Dataset creato con {len(dataset)} esempi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selective Knowledge Negation Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T14:30:29.515816Z",
     "iopub.status.busy": "2025-09-14T14:30:29.514987Z",
     "iopub.status.idle": "2025-09-14T14:30:29.563733Z",
     "shell.execute_reply": "2025-09-14T14:30:29.562825Z",
     "shell.execute_reply.started": "2025-09-14T14:30:29.515758Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SelectiveKnowledgeNegationTrainer:\n",
    "    \"\"\"\n",
    "    Trainer implementing Selective Knowledge Negation Unlearning (SKU).\n",
    "\n",
    "    Core idea:\n",
    "    - For retain samples: optimize the standard language modeling loss (cross-entropy) to preserve knowledge.\n",
    "    - For forget samples: minimize the probability of producing the forbidden answer tokens via token-level\n",
    "      unlikelihood loss on the answer span while still keeping CE on the prompt context to stabilize training.\n",
    "\n",
    "    Added enhancements:\n",
    "    - L2 anchoring of trainable weights to their initial values (helps preserve general knowledge).\n",
    "    - Entropy regularization on the forget answer span (makes distribution flat to reduce memorization).\n",
    "    - Optional refusal-target CE on forget answer span to steer toward a safe response template.\n",
    "    - Cosine LR scheduler with warmup and unlikelihood ramp-up for stable training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path, tokenizer, lora_config, device=\"cuda\", refusal_text: str = \" I cannot comply with that request.\"):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lora_config = lora_config\n",
    "        self.device = device\n",
    "        self.refusal_text = refusal_text\n",
    "\n",
    "        self.model = None\n",
    "        self.base_model = None\n",
    "        self.initial_state_dict = {}\n",
    "\n",
    "    def _count_trainable(self, model):\n",
    "        total = sum(p.numel() for p in model.parameters())\n",
    "        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        return trainable, total\n",
    "\n",
    "    def setup_model(self):\n",
    "        print(\"🔧 Setting up model (LoRA)...\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_path,\n",
    "            local_files_only=True\n",
    "        )\n",
    "\n",
    "        self.base_model = base_model\n",
    "\n",
    "        model = get_peft_model(base_model, self.lora_config).to(self.device)\n",
    "        \n",
    "        # Disable KV cache during training to save memory\n",
    "        try:\n",
    "            model.config.use_cache = False\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "        # Report trainables and snapshot initial trainable params\n",
    "        try:\n",
    "            self.model.print_trainable_parameters()\n",
    "        except Exception:\n",
    "            trainable, total = self._count_trainable(self.model)\n",
    "            print(f\"Trainable params: {trainable} / {total}\")\n",
    "        # Warn if still zero\n",
    "        tcount, _ = self._count_trainable(self.model)\n",
    "        if tcount == 0:\n",
    "            print(\"❗No trainable parameters detected. Training will be a no-op and backward will be skipped. Check target_modules.\")\n",
    "        self.initial_state_dict.clear()\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.initial_state_dict[name] = p.data.clone()\n",
    "        print(\"✅ Model setup completed\")\n",
    "\n",
    "    def _compute_span_masks_targets_precise(self, attention_mask, start_locs, answer_len_kept):\n",
    "        B, T = attention_mask.shape\n",
    "        device = attention_mask.device\n",
    "        prompt_mask_tgt = torch.zeros((B, T), dtype=torch.bool, device=device)\n",
    "        answer_mask_tgt = torch.zeros((B, T), dtype=torch.bool, device=device)\n",
    "        for i in range(B):\n",
    "            s = int(start_locs[i].item()) if torch.is_tensor(start_locs[i]) else int(start_locs[i])\n",
    "            s = max(0, min(s, T - 1))\n",
    "            L = int(answer_len_kept[i].item()) if torch.is_tensor(answer_len_kept[i]) else int(answer_len_kept[i])\n",
    "            # shift-to-target alignment: target indices correspond to positions 1..T-1\n",
    "            split_t = s - 1\n",
    "            if split_t >= 0:\n",
    "                # Prompt is [0 .. split_t-1]\n",
    "                if split_t > 0:\n",
    "                    prompt_mask_tgt[i, :split_t] = True\n",
    "                # Answer is [split_t .. split_t+L-1], clipped to T\n",
    "                if L > 0:\n",
    "                    end_pos = min(T - 1, split_t + L - 1)\n",
    "                    answer_mask_tgt[i, split_t : end_pos + 1] = True\n",
    "            else:\n",
    "                # No prompt tokens; all start as answer, but limit to L\n",
    "                if L > 0:\n",
    "                    end_pos = min(T - 1, L - 1)\n",
    "                    answer_mask_tgt[i, : end_pos + 1] = True\n",
    "        return prompt_mask_tgt, answer_mask_tgt\n",
    "\n",
    "    def _cross_entropy_loss(self, logits, labels, loss_mask):\n",
    "        vocab = logits.size(-1)\n",
    "        # Use reshape instead of view to safely handle non-contiguous tensors\n",
    "        logits_flat = logits.reshape(-1, vocab)\n",
    "        labels_flat = labels.reshape(-1)\n",
    "        mask_flat = loss_mask.reshape(-1)\n",
    "        if mask_flat.sum() == 0:\n",
    "            # Return a zero that's attached to the current graph if logits require grad; else plain zero\n",
    "            return (logits.sum() * 0.0) if logits.requires_grad else torch.tensor(0.0, device=logits.device)\n",
    "        return F.cross_entropy(\n",
    "            logits_flat[mask_flat],\n",
    "            labels_flat[mask_flat],\n",
    "            reduction=\"mean\",\n",
    "        )\n",
    "\n",
    "    def _unlikelihood_loss(self, logits, labels, loss_mask):\n",
    "        \"\"\"Unlikelihood loss with safe indexing: ignore invalid targets and prevent OOB gather.\"\"\"\n",
    "        V = logits.size(-1)\n",
    "        device = logits.device\n",
    "        # Build validity mask for target indices\n",
    "        pad_id = self.tokenizer.pad_token_id or self.tokenizer.eos_token_id\n",
    "        eos_id = self.tokenizer.eos_token_id\n",
    "    \n",
    "        labels_long = labels.long()\n",
    "        valid_tokens = (labels_long >= 0) & (labels_long < V) & (labels_long != -100)\n",
    "        if pad_id is not None:\n",
    "            valid_tokens = valid_tokens & (labels_long != pad_id)\n",
    "        if eos_id is not None:\n",
    "            valid_tokens = valid_tokens & (labels_long != eos_id)\n",
    "    \n",
    "        # Replace invalid indices with 0 to keep gather in-bounds; they will be masked out later\n",
    "        safe_idx = labels_long.clamp(min=0, max=V - 1)\n",
    "    \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        p_y_all = torch.gather(probs, dim=-1, index=safe_idx.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "        mask = (loss_mask & valid_tokens)\n",
    "        if mask.sum().item() == 0:\n",
    "            return (logits.sum() * 0.0) if logits.requires_grad else torch.tensor(0.0, device=device)\n",
    "    \n",
    "        p_y = p_y_all[mask].clamp(1e-6, 1 - 1e-6)\n",
    "        ul = -torch.log(1.0 - p_y)\n",
    "        return ul.mean()\n",
    "\n",
    "    def _entropy_on_mask(self, logits, loss_mask):\n",
    "        # Computes mean entropy H(p) over masked positions\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        eps = 1e-8\n",
    "        ent = -(probs * (probs.clamp_min(eps).log())).sum(dim=-1)\n",
    "        ent = ent[loss_mask]\n",
    "        if ent.numel() == 0:\n",
    "            return (logits.sum() * 0.0) if logits.requires_grad else torch.tensor(0.0, device=logits.device)\n",
    "        return ent.mean()\n",
    "\n",
    "    def _l2_anchor(self):\n",
    "        if not self.initial_state_dict:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "        total = None\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if p.requires_grad and name in self.initial_state_dict:\n",
    "                diff = p - self.initial_state_dict[name].to(p.device, dtype=p.dtype)\n",
    "                term = (diff * diff).sum()\n",
    "                total = term if total is None else total + term\n",
    "        if total is None:\n",
    "            total = torch.tensor(0.0, device=self.device)\n",
    "        return total / max(1, len(self.initial_state_dict))\n",
    "\n",
    "    def _build_refusal_targets(self, answer_mask_tgt, forget_mask, Tm1):\n",
    "        # answer_mask_tgt, forget_mask: [B, T-1] bool\n",
    "        B = answer_mask_tgt.size(0)\n",
    "        device = answer_mask_tgt.device\n",
    "        # Tokenize refusal template once\n",
    "        refusal_ids = self.tokenizer(\n",
    "            self.refusal_text,\n",
    "            add_special_tokens=False,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=Tm1,\n",
    "            return_tensors=None,\n",
    "        )[\"input_ids\"]\n",
    "        if len(refusal_ids) == 0:\n",
    "            # Fallback to EOS\n",
    "            rid = self.tokenizer.eos_token_id\n",
    "            refusal_ids = [rid if rid is not None else 0]\n",
    "        R = len(refusal_ids)\n",
    "        # Prepare targets of shape [B, T-1] filled with -100\n",
    "        target_ref = torch.full((B, Tm1), -100, dtype=torch.long, device=device)\n",
    "        # For each sample, fill the answer span positions with repeated refusal ids\n",
    "        idxs = torch.arange(Tm1, device=device)\n",
    "        for i in range(B):\n",
    "            mask = (answer_mask_tgt[i] & forget_mask[i])\n",
    "            L = int(mask.sum().item())\n",
    "            if L <= 0:\n",
    "                continue\n",
    "            # Repeat/trim the refusal ids to L\n",
    "            seq = (refusal_ids * ((L + R - 1) // R))[:L]\n",
    "            target_positions = idxs[mask]\n",
    "            target_ref[i, target_positions] = torch.tensor(seq, dtype=torch.long, device=device)\n",
    "        return target_ref\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        dataloader,\n",
    "        num_epochs=4,\n",
    "        lr=1e-4,\n",
    "        ce_weight_prompt=1.0,\n",
    "        ul_weight_answer=1.0,\n",
    "        ce_weight_retain=1.0,\n",
    "        l2_anchor_weight=0,\n",
    "        l2_after_weight=5e-6,\n",
    "        curriculum_switch=2,\n",
    "        entropy_weight_answer=0.0,\n",
    "        refusal_weight=0.0,\n",
    "        grad_clip=1.0,\n",
    "        grad_accum_steps=1,\n",
    "        use_mixed_precision=True,\n",
    "        warmup_ratio=0.1,\n",
    "        ul_ramp_ratio=0.2,\n",
    "    ):\n",
    "        assert self.model is not None, \"Call setup_model() first\"\n",
    "        self.model.train()\n",
    "        # Optimizer: prefer 8-bit if available\n",
    "        optimizer = None\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "            optimizer = bnb.optim.PagedAdamW8bit(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "            print(\"🧮 Using 8-bit PagedAdamW optimizer (bitsandbytes)\")\n",
    "        except Exception:\n",
    "            try:\n",
    "                optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01, fused=True)\n",
    "                print(\"🧮 Using fused AdamW optimizer\")\n",
    "            except Exception:\n",
    "                optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "                print(\"🧮 Using standard AdamW optimizer\")\n",
    "\n",
    "        # Scheduler with warmup and cosine decay\n",
    "        steps_per_epoch = max(1, (len(dataloader) + grad_accum_steps - 1) // grad_accum_steps)\n",
    "        total_steps = num_epochs * steps_per_epoch\n",
    "        warmup_steps = max(1, int(warmup_ratio * total_steps))\n",
    "        ul_ramp_steps = max(1, int(ul_ramp_ratio * total_steps))\n",
    "        try:\n",
    "            from transformers import get_cosine_schedule_with_warmup\n",
    "            scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "            print(f\"📈 Using cosine scheduler with warmup ({warmup_steps}/{total_steps})\")\n",
    "        except Exception:\n",
    "            def lr_lambda(step):\n",
    "                if step < warmup_steps:\n",
    "                    return float(step) / float(max(1, warmup_steps))\n",
    "                progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "                # Cosine from 1 to 0\n",
    "                return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "            print(f\"📈 Using LambdaLR scheduler with warmup ({warmup_steps}/{total_steps})\")\n",
    "\n",
    "        # Mixed precision setup\n",
    "        from contextlib import nullcontext\n",
    "        device_type = 'cuda' if ('cuda' in str(self.device) and torch.cuda.is_available()) else ('mps' if ('mps' in str(self.device) and torch.backends.mps.is_available()) else None)\n",
    "        param_dtype = next(self.model.parameters()).dtype if any(p.requires_grad for p in self.model.parameters()) else torch.float32\n",
    "        use_amp = use_mixed_precision and device_type is not None\n",
    "        amp_dtype = torch.bfloat16 if (device_type == 'cuda' and param_dtype == torch.bfloat16) else torch.float16\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=(device_type == 'cuda' and amp_dtype == torch.float16 and use_amp))\n",
    "        \n",
    "        global_step = 0\n",
    "        no_grad_batches = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_losses = []\n",
    "            cur_l2 = l2_after_weight if epoch >= curriculum_switch else l2_anchor_weight\n",
    "            print(f\"\\n[Epoch {epoch+1}/{EPOCHS}] L2 anchor weight: {cur_l2}\")\n",
    "            with tqdm(total=len(dataloader), desc=f\"SKU Epoch {epoch+1}\") as pbar:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                for step, batch in enumerate(dataloader):\n",
    "                    input_ids = batch[\"input_ids\"].to(self.device, non_blocking=True)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(self.device, non_blocking=True)\n",
    "                    labels_full = batch[\"labels\"].to(self.device, non_blocking=True)\n",
    "                    start_locs = batch[\"start_locs\"].to(self.device, non_blocking=True)\n",
    "                    answer_len_kept = batch[\"answer_len_kept\"].to(self.device, non_blocking=True)\n",
    "                    split = batch[\"split\"].to(self.device, non_blocking=True)\n",
    "\n",
    "                    with (torch.autocast(device_type=device_type, dtype=amp_dtype) if use_amp else nullcontext()):\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            use_cache=False,\n",
    "                            return_dict=True,\n",
    "                        )\n",
    "                        logits_full = outputs.logits  # [B, T, V]\n",
    "                        logits = logits_full[:, :-1, :]\n",
    "                        target = labels_full[:, 1:]\n",
    "                        attn_tgt = attention_mask[:, 1:].bool()\n",
    "                        # Precise masks using kept answer length\n",
    "                        prompt_mask_tgt, answer_mask_tgt = self._compute_span_masks_targets_precise(attention_mask, start_locs, answer_len_kept)\n",
    "                        prompt_mask_tgt = prompt_mask_tgt[:, :-1]\n",
    "                        answer_mask_tgt = answer_mask_tgt[:, :-1]\n",
    "                        retain_mask = (split == 0).unsqueeze(-1).expand_as(prompt_mask_tgt)\n",
    "                        forget_mask = (split == 1).unsqueeze(-1).expand_as(prompt_mask_tgt)\n",
    "                        valid_tgt = attn_tgt\n",
    "\n",
    "                        # Core losses\n",
    "                        retain_loss = self._cross_entropy_loss(\n",
    "                            logits, target, loss_mask=(valid_tgt & retain_mask)\n",
    "                        ) * ce_weight_retain\n",
    "                        forget_prompt_loss = self._cross_entropy_loss(\n",
    "                            logits, target, loss_mask=(valid_tgt & forget_mask & prompt_mask_tgt)\n",
    "                        ) * ce_weight_prompt\n",
    "\n",
    "                        # Ramp-up UL weight\n",
    "                        ul_scale = min(1.0, float(global_step + 1) / float(max(1, ul_ramp_steps)))\n",
    "                        forget_ul_loss = self._unlikelihood_loss(\n",
    "                            logits, target, loss_mask=(valid_tgt & forget_mask & answer_mask_tgt)\n",
    "                        ) * (ul_weight_answer * ul_scale)\n",
    "\n",
    "                        # Entropy regularization on forget answer span (maximize entropy)\n",
    "                        ent_loss = self._entropy_on_mask(\n",
    "                            logits, loss_mask=(valid_tgt & forget_mask & answer_mask_tgt)\n",
    "                        )\n",
    "                        entropy_term = -entropy_weight_answer * ent_loss\n",
    "\n",
    "                        # Optional refusal-target CE on forget answer span\n",
    "                        refusal_term = torch.tensor(0.0, device=logits.device)\n",
    "                        if refusal_weight > 0.0:\n",
    "                            Tm1 = logits.size(1)\n",
    "                            target_ref = self._build_refusal_targets(answer_mask_tgt, forget_mask, Tm1)\n",
    "                            refusal_term = self._cross_entropy_loss(\n",
    "                                logits, target_ref, loss_mask=(valid_tgt & forget_mask & answer_mask_tgt)\n",
    "                            ) * refusal_weight\n",
    "\n",
    "                        # L2 anchor on weights\n",
    "                        l2_term = self._l2_anchor() * cur_l2\n",
    "\n",
    "                        loss = retain_loss + forget_prompt_loss + forget_ul_loss + entropy_term + refusal_term + l2_term\n",
    "                        loss_for_backward = loss / max(1, int(grad_accum_steps))\n",
    "\n",
    "                    if not loss_for_backward.requires_grad:\n",
    "                        no_grad_batches += 1\n",
    "                        pbar.set_postfix({\"Loss\": f\"{float(loss.detach().cpu()):.4f}\", \"note\": \"no-grad-batch\"})\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "\n",
    "                    if scaler.is_enabled():\n",
    "                        scaler.scale(loss_for_backward).backward()\n",
    "                    else:\n",
    "                        loss_for_backward.backward()\n",
    "\n",
    "                    do_step = ((step + 1) % grad_accum_steps == 0) or (step + 1 == len(dataloader))\n",
    "                    if do_step:\n",
    "                        if scaler.is_enabled():\n",
    "                            scaler.unscale_(optimizer)\n",
    "                        if grad_clip is not None:\n",
    "                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n",
    "                        if scaler.is_enabled():\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                        else:\n",
    "                            optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad(set_to_none=True)\n",
    "                        global_step += 1\n",
    "\n",
    "                    epoch_losses.append(float(loss.detach().cpu()))\n",
    "                    pbar.set_postfix({\n",
    "                        \"Loss\": f\"{float(loss.detach().cpu()):.4f}\",\n",
    "                        \"RetCE\": f\"{float(retain_loss.detach().cpu()):.3f}\",\n",
    "                        \"FgtCE\": f\"{float(forget_prompt_loss.detach().cpu()):.3f}\",\n",
    "                        \"FgtUL\": f\"{float(forget_ul_loss.detach().cpu()):.3f}\",\n",
    "                        \"Ent\": f\"{float(entropy_term.detach().cpu()):.3f}\",\n",
    "                        \"Ref\": f\"{float(refusal_term.detach().cpu()):.3f}\",\n",
    "                        \"L2\": f\"{float(l2_term.detach().cpu()):.3f}\",\n",
    "                    })\n",
    "                    pbar.update(1)\n",
    "\n",
    "                    # Proactive cleanup to prevent fragmentation\n",
    "                    del outputs, logits_full, logits, target, attn_tgt, prompt_mask_tgt, answer_mask_tgt, retain_mask, forget_mask, valid_tgt\n",
    "                    if device_type == 'cuda' and ((step + 1) % 50 == 0):\n",
    "                        torch.cuda.empty_cache()\n",
    "            if device_type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            avg_loss = float(np.mean(epoch_losses)) if epoch_losses else 0.0\n",
    "            if no_grad_batches:\n",
    "                print(f\"ℹ️ Epoch {epoch+1}: skipped {no_grad_batches} batches with no grad signal (check trainable params)\")\n",
    "                no_grad_batches = 0\n",
    "            print(f\"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n",
    "            ckpt_dir = f\"enhanced_ckpts/epoch_{epoch+1}\"\n",
    "            os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            print(f\"Saving checkpoint in: {ckpt_dir}\")\n",
    "            self.save_model(ckpt_dir)\n",
    "\n",
    "            adapted_local = PeftModel.from_pretrained(self.base_model, ckpt_dir)\n",
    "            sensitive_prompt = forget_validation_df.iloc[0]['input']\n",
    "            sensitive_answer = forget_validation_df.iloc[0]['output']\n",
    "            lp_base, Lspan = compute_span_logprob(base_model_local, tokenizer, sensitive_prompt, sensitive_answer)\n",
    "            lp_adapt, _ = compute_span_logprob(adapted_local, tokenizer, sensitive_prompt, sensitive_answer)\n",
    "            print(f\"[Epoch {epoch+1}] Span log-prob base {lp_base:.2f} -> adapted {lp_adapt:.2f} Δ {lp_adapt-lp_base:.2f}\")\n",
    "\n",
    "    def save_model(self, output_dir: str):\n",
    "        \"\"\"Save LoRA adapters (preferred) or fallback to saving base model weights.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        try:\n",
    "            # If using PEFT, this saves the adapter weights\n",
    "            self.model.save_pretrained(output_dir)\n",
    "            print(f\"💾 Saved PEFT adapters to {output_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not save PEFT adapters directly: {e}\")\n",
    "            try:\n",
    "                # Fallback: try saving base model\n",
    "                if hasattr(self.model, \"base_model\"):\n",
    "                    self.model.base_model.save_pretrained(output_dir)\n",
    "                    print(f\"💾 Saved base model to {output_dir}\")\n",
    "            except Exception as e2:\n",
    "                print(f\"❌ Failed to save model: {e2}\")\n",
    "\n",
    "    def calculate_task_vector(self):\n",
    "        \"\"\"Compute delta between current trainable params and their initial snapshot.\"\"\"\n",
    "        delta = {}\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if p.requires_grad and name in self.initial_state_dict:\n",
    "                delta[name] = (p.data - self.initial_state_dict[name]).detach().cpu()\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Trainer and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T14:30:34.171052Z",
     "iopub.status.busy": "2025-09-14T14:30:34.170730Z",
     "iopub.status.idle": "2025-09-14T14:30:37.680609Z",
     "shell.execute_reply": "2025-09-14T14:30:37.679682Z",
     "shell.execute_reply.started": "2025-09-14T14:30:34.171026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Detecting linear submodules for targeted adaptation...\")\n",
    "base_tmp = AutoModelForCausalLM.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "linear_names = []\n",
    "for name, module in base_tmp.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        # Use the leaf module name\n",
    "        leaf = name.split('.')[-1]\n",
    "        linear_names.append(leaf)\n",
    "unique_linear = sorted(set(linear_names))\n",
    "print(f\"Found {len(unique_linear)} unique linear leaf names (showing first 25): {unique_linear[:25]}\")\n",
    "\n",
    "# Heuristic filter: keep typical projection/feed-forward names if they exist\n",
    "preferred = [n for n in unique_linear if any(k in n for k in [\"q\", \"k\", \"v\", \"o\", \"proj\", \"gate\", \"up\", \"down\", \"w1\", \"w2\", \"fc\", \"linear\"])]\n",
    "# Fallback to all unique linear names if filter becomes too small\n",
    "if len(preferred) < 4:\n",
    "    preferred = unique_linear\n",
    "print(f\"Using {len(preferred)} target module names for LoRA: {preferred}\")\n",
    "\n",
    "auto_lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,                 # larger rank for stronger capacity\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=preferred,\n",
    ")\n",
    "\n",
    "# Rebuild trainer with new config\n",
    "sku_trainer = SelectiveKnowledgeNegationTrainer(\n",
    "    model_path=MODEL_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    lora_config=auto_lora_config,\n",
    "    device=(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")),\n",
    ")\n",
    "sku_trainer.setup_model()\n",
    "\n",
    "# Force LoRA params to fp32 for better small-gradient resolution\n",
    "for n, p in sku_trainer.model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        p.data = p.data.float()\n",
    "\n",
    "print(\"Trainable parameter count:\", sum(p.numel() for p in sku_trainer.model.parameters() if p.requires_grad))\n",
    "\n",
    "# Optional: improve stability/perf\n",
    "if hasattr(sku_trainer.model, \"gradient_checkpointing_enable\"):\n",
    "    try:\n",
    "        sku_trainer.model.gradient_checkpointing_enable()\n",
    "    except Exception:\n",
    "        pass\n",
    "try:\n",
    "    sku_trainer.model = sku_trainer.model.to(dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FORGET_UPSAMPLE = 1  # 1 to not upsample\n",
    "\n",
    "if 'dataset' in globals():\n",
    "    import pandas as pd\n",
    "    base_df = dataset.data\n",
    "    forget_df = base_df[base_df['split'] == 'forget']\n",
    "    retain_df = base_df[base_df['split'] == 'retain']\n",
    "    if FORGET_UPSAMPLE > 1 and len(forget_df) > 0:\n",
    "        reps_int = int(FORGET_UPSAMPLE)\n",
    "        frac_part = FORGET_UPSAMPLE - reps_int\n",
    "        replicated = [forget_df]*reps_int\n",
    "        if frac_part > 1e-6:\n",
    "            replicated.append(forget_df.sample(frac=frac_part, replace=True, random_state=42))\n",
    "        extra_forget = pd.concat(replicated, ignore_index=True)\n",
    "        aug_df = pd.concat([retain_df, extra_forget], ignore_index=True)\n",
    "        print(f\"[Enhanced Training] Upsampled forget examples: {len(extra_forget)}\")\n",
    "    else:\n",
    "        aug_df = base_df\n",
    "    dataset = UnlearningDataset(aug_df, tokenizer, max_length=384)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: sku_collate_fn(b, pad_id, max_length=384),\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        num_workers=2 if torch.cuda.is_available() else 0,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    print(f\"[Enhanced Training] New dataset size {len(dataset)} | forget {sum(dataset.data['split']=='forget')} | retain {sum(dataset.data['split']=='retain')}\")\n",
    "else:\n",
    "    print(\"[Enhanced Training] Dataset object not found; skipping upsample step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T14:37:57.400549Z",
     "iopub.status.busy": "2025-09-14T14:37:57.400219Z",
     "iopub.status.idle": "2025-09-14T14:38:00.689602Z",
     "shell.execute_reply": "2025-09-14T14:38:00.688531Z",
     "shell.execute_reply.started": "2025-09-14T14:37:57.400520Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainArgs:\n",
    "    epochs: int = 5\n",
    "    lr: float = 1e-3\n",
    "    ul_weight: float = 10.0\n",
    "    refusal_weight: float = 0.5\n",
    "    ce_prompt_weight: float = 0.3\n",
    "    ce_retain_weight: float = 1.0\n",
    "    entropy_weight: float = 0.05\n",
    "    grad_accum: int = 4\n",
    "    warmup: float = 0.08\n",
    "    ul_ramp: float = 0.15\n",
    "    grad_clip: float = 1.0\n",
    "    use_mixed_precision: bool = True\n",
    "    l2_anchor: float = 0.0\n",
    "    curriculum_switch: int = 2\n",
    "    l2_after: float = 5e-6\n",
    "\n",
    "TRAIN_ARGS = TrainArgs()\n",
    "print(TRAIN_ARGS)\n",
    "\n",
    "# Train SKU model\n",
    "sku_trainer.train(\n",
    "    dataloader=dataloader,\n",
    "    num_epochs=TRAIN_ARGS.epochs,\n",
    "    lr=TRAIN_ARGS.lr,\n",
    "    ce_weight_prompt=TRAIN_ARGS.ce_prompt_weight,\n",
    "    ul_weight_answer=TRAIN_ARGS.ul_weight,\n",
    "    ce_weight_retain=TRAIN_ARGS.ce_retain_weight,\n",
    "    l2_anchor_weight=TRAIN_ARGS.l2_anchor,\n",
    "    l2_after_weight=TRAIN_ARGS.l2_after,\n",
    "    curriculum_switch=TRAIN_ARGS.curriculum_switch,\n",
    "    entropy_weight_answer=TRAIN_ARGS.entropy_weight,\n",
    "    refusal_weight=TRAIN_ARGS.refusal_weight,\n",
    "    grad_clip=TRAIN_ARGS.grad_clip,\n",
    "    grad_accum_steps=TRAIN_ARGS.grad_accum,\n",
    "    use_mixed_precision=TRAIN_ARGS.use_mixed_precision,\n",
    "    warmup_ratio=TRAIN_ARGS.warmup,\n",
    "    ul_ramp_ratio=TRAIN_ARGS.ul_ramp,\n",
    ")\n",
    "\n",
    "# Quick delta check: task-vector L2 norm\n",
    "with torch.no_grad():\n",
    "    delta = sku_trainer.calculate_task_vector()\n",
    "    total_norm = 0.0\n",
    "    for t in delta.values():\n",
    "        total_norm += float(t.float().pow(2).sum().sqrt())\n",
    "    print(f\"Δ (task-vector) total L2 norm: {total_norm:.4f}\")\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('balanced_results', exist_ok=True)\n",
    "\n",
    "# Save SKU model\n",
    "sku_trainer.save_model('balanced_results/balanced_model')\n",
    "\n",
    "# Calculate and save task vector\n",
    "task_vector = sku_trainer.calculate_task_vector()\n",
    "torch.save(task_vector, 'balanced_results/task_vector.pt')\n",
    "\n",
    "print(\"✅ Results saved in balanced_results/\")\n",
    "print(\"- balanced_model/: SKU-trained model\")\n",
    "print(\"- task_vector.pt: Task vector for future applications\")\n",
    "\n",
    "# Optional quick A/B: generate one forget sample before vs after adapters to verify effect\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    from peft import PeftModel\n",
    "    prompt = forget_validation_df.iloc[0]['input']\n",
    "    base_tok = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "    if base_tok.pad_token is None:\n",
    "        base_tok.pad_token = base_tok.eos_token\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "    inputs = base_tok(prompt, return_tensors='pt').to(base_model.device)\n",
    "    with torch.no_grad():\n",
    "        out_base = base_model.generate(**inputs, max_new_tokens=64)\n",
    "    txt_base = base_tok.decode(out_base[0], skip_special_tokens=True)\n",
    "\n",
    "    adapted = PeftModel.from_pretrained(base_model, 'balanced_results/balanced_model')\n",
    "    with torch.no_grad():\n",
    "        out_adapt = adapted.generate(**inputs, max_new_tokens=64)\n",
    "    txt_adapt = base_tok.decode(out_adapt[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"--- A/B Quick Check ---\")\n",
    "    print(\"Prompt:\", prompt)\n",
    "    print(\"Base  :\", txt_base)\n",
    "    print(\"Adapt :\", txt_adapt)\n",
    "except Exception as e:\n",
    "    print(f\"A/B quick check skipped: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results export helper\n",
    "import getpass\n",
    "\n",
    "def append_result(record: dict, file_path: str = \"evaluation_results.jsonl\"):\n",
    "    rec = {\n",
    "        **record,\n",
    "        \"meta\": {\n",
    "            \"user\": getpass.getuser(),\n",
    "            \"time\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"seed\": SEED,\n",
    "            \"config\": asdict(CFG),\n",
    "            \"env\": ENV_INFO,\n",
    "            \"notebook\": \"SKU_copia_4.ipynb\"\n",
    "        }\n",
    "    }\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    print(f\"Appended results to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-09-14T14:09:16.484521Z",
     "iopub.status.busy": "2025-09-14T14:09:16.484237Z",
     "iopub.status.idle": "2025-09-14T14:09:16.886654Z",
     "shell.execute_reply": "2025-09-14T14:09:16.885493Z",
     "shell.execute_reply.started": "2025-09-14T14:09:16.484500Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import types\n",
    "from evaluation import inference, mia_attacks, compute_metrics\n",
    "\n",
    "DEFAULT_CHECKPOINT = 'balanced_results/balanced_model'\n",
    "LOCAL_VALIDATION_DIR = 'validation'\n",
    "\n",
    "\n",
    "def run_evaluation(\n",
    "    data_path: str = LOCAL_VALIDATION_DIR,\n",
    "    checkpoint_path: str = DEFAULT_CHECKPOINT,\n",
    "    output_dir: str = \"eval_results\",\n",
    "    mia_data_path=None,\n",
    "    mmlu_metrics_file_path=None,\n",
    "    max_new_tokens: int = 256,\n",
    "    batch_size: int = 16,\n",
    "    debug: bool = False,\n",
    "    compute_metrics_only: bool = False,\n",
    "    seed: int = 42,\n",
    "    keep_files: bool = False,\n",
    "):\n",
    "    try:\n",
    "        args = types.SimpleNamespace(\n",
    "            data_path=data_path,\n",
    "            checkpoint_path=checkpoint_path,\n",
    "            output_dir=output_dir,\n",
    "            mia_data_path=mia_data_path,\n",
    "            mmlu_metrics_file_path=mmlu_metrics_file_path,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            batch_size=batch_size,\n",
    "            debug=debug,\n",
    "            compute_metrics_only=compute_metrics_only,\n",
    "            seed=seed,\n",
    "            keep_files=keep_files,\n",
    "        )\n",
    "        print(f\"🔍 Paths:\\n  data={data_path}\\n  ckpt={checkpoint_path}\\n  out={output_dir}\")\n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"Data path not found: {data_path}\")\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Checkpoint path not found: {checkpoint_path}\")\n",
    "        for split_file in [\"forget.jsonl\", \"retain.jsonl\"]:\n",
    "            if not os.path.exists(os.path.join(data_path, split_file)):\n",
    "                raise FileNotFoundError(f\"Missing {split_file} in {data_path}\")\n",
    "\n",
    "        from pathlib import Path as _P\n",
    "        _P(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        import random, torch, numpy as np\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "\n",
    "        from accelerate import Accelerator\n",
    "        accelerator = Accelerator()\n",
    "\n",
    "        if not args.compute_metrics_only:\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "            from peft import PeftModel\n",
    "            base_tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "            if base_tokenizer.pad_token is None:\n",
    "                base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_PATH,\n",
    "                local_files_only=True,\n",
    "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "            )\n",
    "            try:\n",
    "                model = PeftModel.from_pretrained(base_model, args.checkpoint_path)\n",
    "                print(\"✅ Loaded base + adapters\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Adapter load failed ({e}); trying plain model\")\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    args.checkpoint_path,\n",
    "                    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "                    trust_remote_code=True,\n",
    "                )\n",
    "            model.eval()\n",
    "            print(\"🚀 Inference...\")\n",
    "            inference(args, model, base_tokenizer)\n",
    "            if args.mia_data_path:\n",
    "                print(\"🔍 MIA attacks...\")\n",
    "                mia_attacks(args, model, base_tokenizer)\n",
    "        if accelerator.is_main_process:\n",
    "            print(\"📊 Metrics...\")\n",
    "            compute_metrics(args)\n",
    "            print(\"✅ Done\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Evaluation error: {e}\")\n",
    "        import traceback; traceback.print_exc()\n",
    "\n",
    "print(\"🎯 Launching local evaluation (if validation data present)...\")\n",
    "if os.path.exists(LOCAL_VALIDATION_DIR) and all(os.path.exists(os.path.join(LOCAL_VALIDATION_DIR, f)) for f in [\"forget.jsonl\", \"retain.jsonl\"]):\n",
    "    if os.path.exists(DEFAULT_CHECKPOINT):\n",
    "        run_evaluation()\n",
    "    else:\n",
    "        print(f\"❌ Missing checkpoint at {DEFAULT_CHECKPOINT}\")\n",
    "else:\n",
    "    print(f\"❌ Validation directory {LOCAL_VALIDATION_DIR} with required jsonl files not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-14T14:36:28.525326Z",
     "iopub.status.busy": "2025-09-14T14:36:28.524511Z",
     "iopub.status.idle": "2025-09-14T14:36:40.815396Z",
     "shell.execute_reply": "2025-09-14T14:36:40.814321Z",
     "shell.execute_reply.started": "2025-09-14T14:36:28.525286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 8. Diagnostics & Debugging for SKU Effectiveness ===\n",
    "import torch, math\n",
    "from collections import Counter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "print(\"[Diagnostics] Starting SKU debugging block...\")\n",
    "\n",
    "# 1. Dataset split distribution & basic length stats\n",
    "if 'dataset' in globals():\n",
    "    split_counts = Counter(dataset.data['split']) if hasattr(dataset, 'data') else {}\n",
    "    print(\"Split counts:\", split_counts)\n",
    "    # Approx prompt / answer token stats (first 200 samples)\n",
    "    prompt_lens = []\n",
    "    answer_lens = []\n",
    "    for i in range(min(200, len(dataset))):\n",
    "        item = dataset[i]\n",
    "        prompt_lens.append(int(item['start_locs']))\n",
    "        answer_lens.append(int(item['answer_len_kept']))\n",
    "    if prompt_lens:\n",
    "        print(f\"Avg prompt tokens: {sum(prompt_lens)/len(prompt_lens):.1f} | Avg kept answer tokens: {sum(answer_lens)/len(answer_lens):.1f}\")\n",
    "        zero_ans = sum(1 for x in answer_lens if x == 0)\n",
    "        print(f\"Samples with 0 answer tokens kept: {zero_ans}/{len(answer_lens)} ({100*zero_ans/len(answer_lens):.1f}%)\")\n",
    "else:\n",
    "    print(\"Dataset not found in globals().\")\n",
    "\n",
    "# 2. Inspect one batch to see UL active positions\n",
    "if 'dataloader' in globals():\n",
    "    first_batch = next(iter(dataloader))\n",
    "    # Move minimal tensors\n",
    "    input_ids = first_batch['input_ids']\n",
    "    attention_mask = first_batch['attention_mask']\n",
    "    start_locs = first_batch['start_locs']\n",
    "    answer_len_kept = first_batch['answer_len_kept']\n",
    "    split = first_batch['split']\n",
    "    if 'sku_trainer' in globals():\n",
    "        with torch.no_grad():\n",
    "            prompt_mask_tgt, answer_mask_tgt = sku_trainer._compute_span_masks_targets_precise(attention_mask, start_locs, answer_len_kept)\n",
    "            # Align with target length (T-1)\n",
    "            prompt_mask_tgt = prompt_mask_tgt[:, :-1]\n",
    "            answer_mask_tgt = answer_mask_tgt[:, :-1]\n",
    "            attn_tgt = attention_mask[:, 1:].bool()\n",
    "            retain_mask = (split == 0).unsqueeze(-1).expand_as(prompt_mask_tgt)\n",
    "            forget_mask = (split == 1).unsqueeze(-1).expand_as(prompt_mask_tgt)\n",
    "            ul_active = (attn_tgt & forget_mask & answer_mask_tgt).sum().item()\n",
    "            forget_answer_tokens = (forget_mask & answer_mask_tgt).sum().item()\n",
    "            print(f\"UL active positions in sample batch: {ul_active}\")\n",
    "            print(f\"Total forget answer target positions in sample batch: {forget_answer_tokens}\")\n",
    "            if forget_answer_tokens == 0:\n",
    "                print(\"WARNING: No forget answer tokens available; unlikelihood loss will be zero. Consider increasing max_length or shortening prompts.\")\n",
    "    else:\n",
    "        print(\"sku_trainer not defined.\")\n",
    "else:\n",
    "    print(\"Dataloader not found.\")\n",
    "\n",
    "# 3. Function to compute log-prob of a sensitive answer span before & after adapters\n",
    "\n",
    "def compute_span_logprob(model, tokenizer, prompt, span_text, device=None):\n",
    "    device = device or (next(model.parameters()).device if any(p.requires_grad for p in model.parameters()) else 'cpu')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tok = tokenizer(prompt + span_text, return_tensors='pt')\n",
    "        for k in tok: tok[k] = tok[k].to(device)\n",
    "        outputs = model(**tok, use_cache=False, return_dict=True)\n",
    "        logits = outputs.logits # [1, T, V]\n",
    "        input_ids = tok['input_ids']\n",
    "        # We want log P(span | prompt). Identify boundary.\n",
    "        prompt_ids = tokenizer(prompt, add_special_tokens=False)['input_ids']\n",
    "        plen = len(prompt_ids)\n",
    "        # Shift for causal LM\n",
    "        target_ids = input_ids[:, 1:]  # next-token targets\n",
    "        logits_shifted = logits[:, :-1, :]\n",
    "        # Positions corresponding to span tokens\n",
    "        span_positions = list(range(plen, input_ids.size(1)-1))  # exclude last because of shift alignment\n",
    "        if not span_positions:\n",
    "            return float('nan'), 0\n",
    "        log_probs = torch.log_softmax(logits_shifted[0, span_positions, :], dim=-1)\n",
    "        tgt_tokens = target_ids[0, span_positions]\n",
    "        gathered = log_probs[range(len(span_positions)), tgt_tokens]\n",
    "        return gathered.sum().item(), len(span_positions)\n",
    "\n",
    "# 4. Compare probability of original answer phrase (greedy sensitive segment)\n",
    "try:\n",
    "    sensitive_prompt = forget_validation_df.iloc[0]['input']\n",
    "    # Use ground-truth output field as sensitive answer to suppress\n",
    "    sensitive_answer = forget_validation_df.iloc[0]['output']\n",
    "    base_tok = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "    if base_tok.pad_token is None: base_tok.pad_token = base_tok.eos_token\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, local_files_only=True, torch_dtype=torch.float32)\n",
    "    lp_base, Lspan = compute_span_logprob(base_model, base_tok, sensitive_prompt, sensitive_answer)\n",
    "    print(f\"Base span log-prob (sum over {Lspan} tokens): {lp_base:.2f}\")\n",
    "    adapted = None\n",
    "    from peft import PeftModel\n",
    "    if os.path.exists('enhanced_ckpts/epoch_1'):\n",
    "        try:\n",
    "            adapted = PeftModel.from_pretrained(base_model, 'enhanced_ckpts/epoch_1')\n",
    "        except Exception as e:\n",
    "            print(\"Could not load adapters from sku_model_epoch_last:\", e)\n",
    "    elif os.path.exists('enhanced_ckpts/epoch_1'):\n",
    "        try:\n",
    "            adapted = PeftModel.from_pretrained(base_model, 'enhanced_ckpts/epoch_1')\n",
    "        except Exception as e:\n",
    "            print(\"Could not load adapters from enhanced_ckpts/epoch_1:\", e)\n",
    "    if adapted is not None:\n",
    "        lp_adapt, _ = compute_span_logprob(adapted, base_tok, sensitive_prompt, sensitive_answer)\n",
    "        print(f\"Adapted span log-prob: {lp_adapt:.2f}\")\n",
    "        if math.isfinite(lp_base) and math.isfinite(lp_adapt):\n",
    "            delta = lp_adapt - lp_base\n",
    "            print(f\"Δ log-prob (adapted - base): {delta:.2f} (negative desired for forgetting)\")\n",
    "            if delta > -0.5:\n",
    "                print(\"Span probability not sufficiently reduced. Consider stronger ul_weight_answer, higher lr, or upsampling forget examples.\")\n",
    "    else:\n",
    "        print(\"No adapted model directory found for probability comparison.\")\n",
    "except Exception as e:\n",
    "    print(\"Span log-prob comparison skipped:\", e)\n",
    "\n",
    "# 5. Recommendations print based on quick heuristics\n",
    "print(\"\\n[Heuristic Recommendations]\")\n",
    "if 'split_counts' in locals() and split_counts:\n",
    "    total = sum(split_counts.values())\n",
    "    fgt = split_counts.get('forget', 0)\n",
    "    if fgt / max(1,total) < 0.2:\n",
    "        print(\"- Forget examples <20%: upsample forget or increase ul_weight_answer/refusal_weight.\")\n",
    "if 'answer_lens' in locals() and answer_lens:\n",
    "    if sum(1 for x in answer_lens if x==0) / len(answer_lens) > 0.3:\n",
    "        print(\"- Many samples lose answer tokens (truncation). Increase max_length or shorten prompts.\")\n",
    "print(\"- If Δ log-prob ~0, raise lr (e.g., 1e-4), set ul_weight_answer 6-8, set refusal_weight 0.3, temporarily disable l2_anchor.\")\n",
    "print(\"- Use sampling (top_p=0.9, temperature=0.8) for qualitative A/B instead of greedy only.\")\n",
    "print(\"[Diagnostics] Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References & Appendix\n",
    "\n",
    "- Paper: Task Vectors in Language Models. arXiv:2402.10058\n",
    "- Codebase: This notebook builds on project modules in this repository (see `config.py`, `training_utils.py`, `evaluation_utils.py`).\n",
    "\n",
    "Academic Integrity: This notebook is prepared for coursework submission. All external sources are cited; experiments are reproducible with fixed seeds and logged environment details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility manifest\n",
    "manifest = {\n",
    "    'config': asdict(CFG),\n",
    "    'train_args': TRAIN_ARGS.__dict__,\n",
    "    'env': ENV_INFO,\n",
    "    'data_counts': {\n",
    "        'retain_train': len(retain_train_df),\n",
    "        'forget_train': len(forget_train_df),\n",
    "        'retain_validation': len(retain_validation_df),\n",
    "        'forget_validation': len(forget_validation_df)\n",
    "    }\n",
    "}\n",
    "with open('outputs/manifest.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "print('Wrote outputs/manifest.json')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8243587,
     "sourceId": 13020196,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8243600,
     "sourceId": 13020215,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8051727,
     "sourceId": 12737770,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "LLM_Unlearning_SEMEval2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
