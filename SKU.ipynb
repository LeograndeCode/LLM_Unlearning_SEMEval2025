{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selective Knowledge Negation Unlearning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:54:08.478341Z",
     "iopub.status.busy": "2025-08-16T15:54:08.478082Z",
     "iopub.status.idle": "2025-08-16T15:54:53.925665Z",
     "shell.execute_reply": "2025-08-16T15:54:53.924076Z",
     "shell.execute_reply.started": "2025-08-16T15:54:08.478323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install rouge-score\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Configurazioni\n",
    "MODEL_PATH = \"/kaggle/input/olmo-model/semeval25-unlearning-1B-model\"\n",
    "DATA_PATH = \"/kaggle/input/olmo-model/semeval25-unlearning-data\"\n",
    "\n",
    "print(f\"GPUs disponibili: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Caricamento Dati e Modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:54:53.928809Z",
     "iopub.status.busy": "2025-08-16T15:54:53.927770Z",
     "iopub.status.idle": "2025-08-16T15:54:55.468476Z",
     "shell.execute_reply": "2025-08-16T15:54:55.467556Z",
     "shell.execute_reply.started": "2025-08-16T15:54:53.928773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Caricamento dataset\n",
    "retain_train_df = pd.read_parquet(f\"{DATA_PATH}/data/retain_train-00000-of-00001.parquet\", engine='pyarrow')\n",
    "retain_validation_df = pd.read_parquet(f\"{DATA_PATH}/data/retain_validation-00000-of-00001.parquet\", engine='pyarrow')\n",
    "forget_train_df = pd.read_parquet(f\"{DATA_PATH}/data/forget_train-00000-of-00001.parquet\", engine='pyarrow')\n",
    "forget_validation_df = pd.read_parquet(f\"{DATA_PATH}/data/forget_validation-00000-of-00001.parquet\", engine='pyarrow')\n",
    "\n",
    "# Salvataggio in formato JSONL\n",
    "!mkdir -p train validation\n",
    "retain_train_df.to_json('train/retain.jsonl', orient='records', lines=True)\n",
    "forget_train_df.to_json('train/forget.jsonl', orient='records', lines=True)\n",
    "retain_validation_df.to_json('validation/retain.jsonl', orient='records', lines=True)\n",
    "forget_validation_df.to_json('validation/forget.jsonl', orient='records', lines=True)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Dataset salvati e tokenizer caricato\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:54:55.469690Z",
     "iopub.status.busy": "2025-08-16T15:54:55.469467Z",
     "iopub.status.idle": "2025-08-16T15:54:55.478117Z",
     "shell.execute_reply": "2025-08-16T15:54:55.477328Z",
     "shell.execute_reply.started": "2025-08-16T15:54:55.469667Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UnlearningDataset(Dataset):\n",
    "    def __init__(self, data_source, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if isinstance(data_source, pd.DataFrame):\n",
    "            self.data = data_source\n",
    "            print(f\"Caricati {len(self.data)} esempi dal DataFrame\")\n",
    "        elif isinstance(data_source, str):\n",
    "            data_list = []\n",
    "            with open(data_source, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    item = json.loads(line.strip())\n",
    "                    data_list.append(item)\n",
    "            self.data = pd.DataFrame(data_list)\n",
    "            print(f\"Caricati {len(self.data)} esempi da {data_source}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        prompt_text = self.data.iloc[idx][\"input\"]\n",
    "        answer_text = self.data.iloc[idx][\"output\"]\n",
    "\n",
    "        # Tokenize prompt alone to get the split boundary (answer starts after this index)\n",
    "        prompt_tok = self.tokenizer(\n",
    "            prompt_text,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "\n",
    "        # Tokenize concatenated prompt + answer for model inputs and labels\n",
    "        combined_tok = self.tokenizer(\n",
    "            f\"{prompt_text} {answer_text}\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "\n",
    "        # Length of prompt in tokens (answer starts at this index in labels)\n",
    "        start_locs = len(prompt_tok[\"input_ids\"]) - 1\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(combined_tok[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(combined_tok[\"attention_mask\"], dtype=torch.long),\n",
    "            \"start_locs\": start_locs,\n",
    "            \"labels\": torch.tensor(combined_tok[\"input_ids\"], dtype=torch.long),\n",
    "            \"split\": 1 if self.data.iloc[idx][\"split\"] == \"forget\" else 0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:54:55.480221Z",
     "iopub.status.busy": "2025-08-16T15:54:55.480021Z",
     "iopub.status.idle": "2025-08-16T15:54:55.520835Z",
     "shell.execute_reply": "2025-08-16T15:54:55.520099Z",
     "shell.execute_reply.started": "2025-08-16T15:54:55.480205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "batch_size = 4\n",
    "train_data = pd.concat([retain_train_df, forget_train_df], ignore_index=True)\n",
    "dataset = UnlearningDataset(train_data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Dataset creato con {len(dataset)} esempi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selective Knowledge Negation Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectiveKnowledgeNegationTrainer:\n",
    "    \"\"\"\n",
    "    Trainer implementing Selective Knowledge Negation Unlearning (SKU).\n",
    "\n",
    "    Core idea:\n",
    "    - For retain samples: optimize the standard language modeling loss (cross-entropy) to preserve knowledge.\n",
    "    - For forget samples: minimize the probability of producing the forbidden answer tokens via token-level\n",
    "      unlikelihood loss on the answer span while still keeping CE on the prompt context to stabilize training.\n",
    "\n",
    "    This class retains the previous structure: setup models, train, save, and compute a task vector w.r.t. initial LoRA state.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path, tokenizer, lora_config, device=\"cuda:0\"):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lora_config = lora_config\n",
    "        self.device = device\n",
    "\n",
    "        self.model = None\n",
    "        self.initial_state_dict = {}\n",
    "\n",
    "    def setup_model(self):\n",
    "        \"\"\"Load base model and wrap with LoRA adapters for efficient finetuning.\"\"\"\n",
    "        print(\"🔧 Setting up model (LoRA)...\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_path,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        self.model = get_peft_model(base_model, self.lora_config).to(self.device)\n",
    "        self.model.print_trainable_parameters()\n",
    "\n",
    "        # Snapshot initial LoRA weights to compute a task vector later\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.initial_state_dict[name] = p.data.clone()\n",
    "        print(\"✅ Model setup completed\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _shift_labels_for_ce(labels):\n",
    "        \"\"\"Utility to align labels for causal LM loss if computing manually.\"\"\"\n",
    "        # Not used if we call HF loss directly, but handy for custom computations.\n",
    "        return labels.clone()\n",
    "\n",
    "    def _compute_mask_spans(self, input_ids, start_locs):\n",
    "        \"\"\"\n",
    "        Build boolean masks for prompt and answer spans per sample.\n",
    "        start_locs: integer index where the answer starts (after the prompt tokens)\n",
    "        We assume labels contain both prompt and answer, with the answer appended to the prompt (as created in the dataset).\n",
    "        \"\"\"\n",
    "        # input_ids: [B, T]\n",
    "        B, T = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        prompt_mask = torch.zeros((B, T), dtype=torch.bool, device=device)\n",
    "        answer_mask = torch.zeros((B, T), dtype=torch.bool, device=device)\n",
    "        for i in range(B):\n",
    "            s = int(start_locs[i].item()) if torch.is_tensor(start_locs[i]) else int(start_locs[i])\n",
    "            # prompt part includes up to s (exclusive for prediction on next token)\n",
    "            # answer part starts from s to end (where labels correspond to predicted answer tokens)\n",
    "            # We predict token t using inputs up to t-1, so for loss masking we mark labels positions.\n",
    "            # We'll consider tokens after s as answer tokens for unlikelihood.\n",
    "            # Clamp to valid range\n",
    "            s = max(0, min(s, T - 1))\n",
    "            # labels at position >= s are part of the answer sequence\n",
    "            answer_mask[i, s:] = True\n",
    "            prompt_mask[i, :s] = True\n",
    "        return prompt_mask, answer_mask\n",
    "\n",
    "    def _cross_entropy_loss(self, logits, labels, loss_mask):\n",
    "        \"\"\"\n",
    "        Standard token-level cross-entropy computed only where loss_mask is True.\n",
    "        logits: [B, T, V], labels: [B, T], loss_mask: [B, T] (bool)\n",
    "        \"\"\"\n",
    "        vocab = logits.size(-1)\n",
    "        # Flatten\n",
    "        logits_flat = logits.view(-1, vocab)\n",
    "        labels_flat = labels.view(-1)\n",
    "        mask_flat = loss_mask.view(-1)\n",
    "\n",
    "        if mask_flat.sum() == 0:\n",
    "            return logits.new_tensor(0.0)\n",
    "\n",
    "        ce = F.cross_entropy(\n",
    "            logits_flat[mask_flat],\n",
    "            labels_flat[mask_flat],\n",
    "            reduction=\"mean\"\n",
    "        )\n",
    "        return ce\n",
    "\n",
    "    def _unlikelihood_loss(self, logits, labels, loss_mask):\n",
    "        \"\"\"\n",
    "        Token-level unlikelihood loss from \"The Unlikelihood Training Objective\" (Welleck et al.).\n",
    "        For target (gold) token y, penalize log p(y) to discourage producing y.\n",
    "        Implemented as: L_UL = - log(1 - p(y)) aggregated over masked positions.\n",
    "        To ensure numerical stability: clamp p in (eps, 1 - eps).\n",
    "        \"\"\"\n",
    "        # Compute probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        B, T, V = probs.shape\n",
    "        # Gather p(y)\n",
    "        y = labels.unsqueeze(-1)  # [B, T, 1]\n",
    "        p_y = torch.gather(probs, dim=-1, index=y).squeeze(-1)  # [B, T]\n",
    "        eps = 1e-6\n",
    "        p_y = p_y.clamp(min=eps, max=1 - eps)\n",
    "        ul = -torch.log(1.0 - p_y)\n",
    "        # Mask\n",
    "        ul = ul[loss_mask]\n",
    "        if ul.numel() == 0:\n",
    "            return logits.new_tensor(0.0)\n",
    "        return ul.mean()\n",
    "\n",
    "    def train(self, dataloader, num_epochs=4, lr=1e-4, ce_weight_prompt=1.0, ul_weight_answer=1.0, ce_weight_retain=1.0, grad_clip=1.0):\n",
    "        \"\"\"\n",
    "        Train with SKU:\n",
    "        - For retain (split==0): standard CE on all label positions.\n",
    "        - For forget (split==1): CE on prompt tokens (to keep context fluency) + Unlikelihood on answer tokens.\n",
    "        Weights allow tuning the relative contributions.\n",
    "        \"\"\"\n",
    "        assert self.model is not None, \"Call setup_model() first\"\n",
    "\n",
    "        self.model.train()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_losses = []\n",
    "            with tqdm(total=len(dataloader), desc=f\"SKU Epoch {epoch+1}\") as pbar:\n",
    "                for batch in dataloader:\n",
    "                    input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                    labels = batch[\"labels\"].to(self.device)\n",
    "                    start_locs = batch[\"start_locs\"].to(self.device)\n",
    "                    split = batch[\"split\"].to(self.device)  # 0 retain, 1 forget\n",
    "\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels,\n",
    "                        output_hidden_states=False,\n",
    "                        return_dict=True,\n",
    "                    )\n",
    "                    logits = outputs.logits  # [B, T, V]\n",
    "\n",
    "                    # Build span masks\n",
    "                    prompt_mask, answer_mask = self._compute_mask_spans(input_ids, start_locs)\n",
    "\n",
    "                    # Masks by split\n",
    "                    retain_mask = (split == 0).unsqueeze(-1).expand_as(prompt_mask)\n",
    "                    forget_mask = (split == 1).unsqueeze(-1).expand_as(prompt_mask)\n",
    "\n",
    "                    # Retain: CE on all tokens where labels != -100 (use attention mask to avoid padded positions)\n",
    "                    valid_tokens = attention_mask.bool()\n",
    "                    retain_loss = self._cross_entropy_loss(\n",
    "                        logits, labels, loss_mask=(valid_tokens & retain_mask)\n",
    "                    ) * ce_weight_retain\n",
    "\n",
    "                    # Forget: CE only on prompt to keep fluency\n",
    "                    forget_prompt_loss = self._cross_entropy_loss(\n",
    "                        logits, labels, loss_mask=(valid_tokens & forget_mask & prompt_mask)\n",
    "                    ) * ce_weight_prompt\n",
    "\n",
    "                    # Forget: Unlikelihood on answer tokens to negate specific knowledge\n",
    "                    forget_ul_loss = self._unlikelihood_loss(\n",
    "                        logits, labels, loss_mask=(valid_tokens & forget_mask & answer_mask)\n",
    "                    ) * ul_weight_answer\n",
    "\n",
    "                    loss = retain_loss + forget_prompt_loss + forget_ul_loss\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    if grad_clip is not None:\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_losses.append(loss.item())\n",
    "                    pbar.set_postfix({\n",
    "                        \"Loss\": f\"{loss.item():.4f}\",\n",
    "                        \"RetCE\": f\"{retain_loss.item():.3f}\",\n",
    "                        \"FgtCE\": f\"{forget_prompt_loss.item():.3f}\",\n",
    "                        \"FgtUL\": f\"{forget_ul_loss.item():.3f}\",\n",
    "                    })\n",
    "                    pbar.update(1)\n",
    "\n",
    "            avg_loss = float(np.mean(epoch_losses)) if epoch_losses else 0.0\n",
    "            print(f\"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    def save_model(self, save_path):\n",
    "        \"\"\"Save model adapters and tokenizer.\"\"\"\n",
    "        self.model.save_pretrained(save_path)\n",
    "        self.tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    def calculate_task_vector(self):\n",
    "        \"\"\"Return task vector between initial and trained LoRA parameters.\"\"\"\n",
    "        tv = {}\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if p.requires_grad and name in self.initial_state_dict:\n",
    "                tv[name] = p.data - self.initial_state_dict[name]\n",
    "        return tv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Trainer and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:54:55.548798Z",
     "iopub.status.busy": "2025-08-16T15:54:55.548454Z",
     "iopub.status.idle": "2025-08-16T15:55:48.528242Z",
     "shell.execute_reply": "2025-08-16T15:55:48.527344Z",
     "shell.execute_reply.started": "2025-08-16T15:54:55.548774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configure LoRA (single model)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "# Initialize SKU trainer\n",
    "sku_trainer = SelectiveKnowledgeNegationTrainer(\n",
    "    model_path=MODEL_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    lora_config=lora_config,\n",
    "    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "# Setup model\n",
    "sku_trainer.setup_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-16T15:55:48.529500Z",
     "iopub.status.busy": "2025-08-16T15:55:48.529203Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train SKU model\n",
    "sku_trainer.train(\n",
    "    dataloader=dataloader,\n",
    "    num_epochs=4,\n",
    "    lr=1e-4,\n",
    "    ce_weight_prompt=1.0,       # CE weight on prompt for forget samples\n",
    "    ul_weight_answer=1.0,       # Unlikelihood weight on answer for forget samples\n",
    "    ce_weight_retain=1.0,       # CE weight for retain samples\n",
    "    grad_clip=1.0,\n",
    ")\n",
    "\n",
    "# Save an intermediate checkpoint\n",
    "sku_trainer.save_model(\"sku_model_epoch_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save final model\n",
    "sku_trainer.save_model(\"studentmodel_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results and Task Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "os.makedirs('balanced_results', exist_ok=True)\n",
    "\n",
    "# Save SKU model\n",
    "sku_trainer.save_model('balanced_results/balanced_model')\n",
    "\n",
    "# Calculate and save task vector\n",
    "task_vector = sku_trainer.calculate_task_vector()\n",
    "torch.save(task_vector, 'balanced_results/task_vector.pt')\n",
    "\n",
    "print(\"✅ Results saved in balanced_results/\")\n",
    "print(\"- balanced_model/: SKU-trained model\")\n",
    "print(\"- task_vector.pt: Task vector for future applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import types\n",
    "\n",
    "try:\n",
    "    import evaluation\n",
    "    import importlib\n",
    "    importlib.reload(evaluation)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def run_evaluation(\n",
    "    data_path,\n",
    "    checkpoint_path,\n",
    "    output_dir=\"eval_results\",\n",
    "    mia_data_path=None,\n",
    "    mmlu_metrics_file_path=None,\n",
    "    max_new_tokens=256,\n",
    "    batch_size=25,\n",
    "    debug=False,\n",
    "    compute_metrics_only=False,\n",
    "    seed=42,\n",
    "    keep_files=False,\n",
    "):\n",
    "    try:\n",
    "        # Costruiamo un oggetto args simile a quello di argparse\n",
    "        args = types.SimpleNamespace(\n",
    "            data_path=data_path,\n",
    "            checkpoint_path=checkpoint_path,\n",
    "            output_dir=output_dir,\n",
    "            mia_data_path=mia_data_path,\n",
    "            mmlu_metrics_file_path=mmlu_metrics_file_path,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            batch_size=batch_size,\n",
    "            debug=debug,\n",
    "            compute_metrics_only=compute_metrics_only,\n",
    "            seed=seed,\n",
    "            keep_files=keep_files,\n",
    "        )\n",
    "\n",
    "        # Verifica che i file esistano\n",
    "        print(f\"🔍 Verificando paths...\")\n",
    "        print(f\"  Data path: {data_path}\")\n",
    "        print(f\"  Checkpoint path: {checkpoint_path}\")\n",
    "        print(f\"  Output dir: {output_dir}\")\n",
    "        \n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"Data path not found: {data_path}\")\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise FileNotFoundError(f\"Checkpoint path not found: {checkpoint_path}\")\n",
    "        if not os.path.exists(os.path.join(data_path, 'forget.jsonl')):\n",
    "            raise FileNotFoundError(f\"forget.jsonl not found in {data_path}\")\n",
    "        if not os.path.exists(os.path.join(data_path, 'retain.jsonl')):\n",
    "            raise FileNotFoundError(f\"retain.jsonl not found in {data_path}\")\n",
    "\n",
    "        # Normalizza i path (come nello script originale)\n",
    "        from pathlib import Path\n",
    "        if args.output_dir is None:\n",
    "            args.output_dir = os.getcwd()\n",
    "        else:\n",
    "            args.output_dir = args.output_dir.rstrip('/')\n",
    "            Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Lancia direttamente le funzioni\n",
    "        import random, torch, numpy as np\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "\n",
    "        from accelerate import Accelerator\n",
    "        accelerator = Accelerator()\n",
    "\n",
    "        if not args.compute_metrics_only:\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "            from peft import PeftModel, LoraConfig\n",
    "            \n",
    "            print(f\"📥 Loading model from {args.checkpoint_path}...\")\n",
    "            \n",
    "            # Carica il modello PEFT (LoRA) se è salvato come tale\n",
    "            try:\n",
    "                # Prima prova a caricare come modello PEFT\n",
    "                base_model_path = MODEL_PATH  # Usa il path del modello base\n",
    "                base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                    base_model_path, \n",
    "                    local_files_only=True,\n",
    "                    torch_dtype=torch.bfloat16\n",
    "                )\n",
    "                model = PeftModel.from_pretrained(base_model, args.checkpoint_path)\n",
    "                print(\"✅ Loaded as PEFT model\")\n",
    "            except:\n",
    "                # Se fallisce, prova a caricare come modello normale\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    args.checkpoint_path,\n",
    "                    torch_dtype=torch.bfloat16,\n",
    "                    trust_remote_code=True\n",
    "                )\n",
    "                print(\"✅ Loaded as regular model\")\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(args.checkpoint_path)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            print(\"🚀 Starting inference...\")\n",
    "            evaluation.inference(args, model, tokenizer)\n",
    "            \n",
    "            if args.mia_data_path is not None:\n",
    "                print(\"🔍 Starting MIA attacks...\")\n",
    "                evaluation.mia_attacks(args, model, tokenizer)\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            print(\"📊 Computing metrics...\")\n",
    "            evaluation.compute_metrics(args)\n",
    "            print(\"✅ Evaluation completed!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# === Step 4: Esegui evaluation ===\n",
    "print(\"🎯 Starting evaluation process...\")\n",
    "\n",
    "# Verifica che i file esistano prima di iniziare\n",
    "if os.path.exists(\"validation/forget.jsonl\") and os.path.exists(\"validation/retain.jsonl\"):\n",
    "    if os.path.exists(\"balanced_results/balanced_model/\"):\n",
    "        run_evaluation(\n",
    "            data_path=\"validation/\",  # cartella relativa con forget.jsonl e retain.jsonl\n",
    "            checkpoint_path=\"balanced_results/balanced_model/\",  # cartella relativa con i pesi del modello\n",
    "            output_dir=\"eval_results\",\n",
    "            debug=True  # Attiva debug per vedere cosa succede\n",
    "        )\n",
    "    else:\n",
    "        print(\"❌ Model checkpoint not found at balanced_results/balanced_model/\")\n",
    "        print(\"   Make sure the training completed successfully\")\n",
    "else:\n",
    "    print(\"❌ Validation files not found\")\n",
    "    print(\"   Expected: validation/forget.jsonl and validation/retain.jsonl\")\n",
    "    print(\"   Make sure the data processing completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8051727,
     "sourceId": 12737770,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
