{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"181b9bf39401454b99e2d21ebb76a7d1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_68fe1ee934694b21b3a2cf08790369a5","IPY_MODEL_83c736f28bfd4fde9c1bdde0efb86d4e","IPY_MODEL_4c52f805c81645de8bdf6fbf16b0750d"],"layout":"IPY_MODEL_0f98e1c2541d42bc98e195bc891cf4b0"}},"68fe1ee934694b21b3a2cf08790369a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbb0b03dcd8c4dab83eed307ecd6216c","placeholder":"​","style":"IPY_MODEL_4057da64d49149efbbbe0b9c1590cc8d","value":"Fetching 7 files: 100%"}},"83c736f28bfd4fde9c1bdde0efb86d4e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7a14edeb03a4ae384a50f17a2587a99","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0222f1849a09410f90d8c6dfe8da06ed","value":7}},"4c52f805c81645de8bdf6fbf16b0750d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_038428d8bb074ecbb6224852ecc080f7","placeholder":"​","style":"IPY_MODEL_07ca1875ccab4aabbe1d3262fa3dee9c","value":" 7/7 [00:00&lt;00:00, 245.73it/s]"}},"0f98e1c2541d42bc98e195bc891cf4b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbb0b03dcd8c4dab83eed307ecd6216c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4057da64d49149efbbbe0b9c1590cc8d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7a14edeb03a4ae384a50f17a2587a99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0222f1849a09410f90d8c6dfe8da06ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"038428d8bb074ecbb6224852ecc080f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07ca1875ccab4aabbe1d3262fa3dee9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eebaccefb6a442f29f6f5e40abd6a1f1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_abe06a8c9f004641b9ea1c4351ffadd2","IPY_MODEL_6d1d7337855d4219939960f3abced88c","IPY_MODEL_e033cca6a2ed41a6bf69a10103a23885"],"layout":"IPY_MODEL_b5014e80c3904313bf6ce4638abc7d35"}},"abe06a8c9f004641b9ea1c4351ffadd2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a031a8d1ceb44aefa4c6f183d35ab399","placeholder":"​","style":"IPY_MODEL_44b20d0d88214e19b3acd1a4dd972812","value":"Fetching 10 files: 100%"}},"6d1d7337855d4219939960f3abced88c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c06c255097d4485824012e29c85bd07","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d95f0d96c54e4109bc2be8d324d5fbba","value":10}},"e033cca6a2ed41a6bf69a10103a23885":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_243c3b9dfc2743a9b09a830a65143310","placeholder":"​","style":"IPY_MODEL_4d31b96046f84d2d9d7dff9f594ec892","value":" 10/10 [00:00&lt;00:00, 324.99it/s]"}},"b5014e80c3904313bf6ce4638abc7d35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a031a8d1ceb44aefa4c6f183d35ab399":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44b20d0d88214e19b3acd1a4dd972812":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c06c255097d4485824012e29c85bd07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d95f0d96c54e4109bc2be8d324d5fbba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"243c3b9dfc2743a9b09a830a65143310":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d31b96046f84d2d9d7dff9f594ec892":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"678662b3c0c5435ab9b88ab8bfe5bac1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f70785d7f374072a2a629dad593984a","IPY_MODEL_3590b3de45a04f5185a8f0c3efa23358","IPY_MODEL_8150a86af6894d9cb6ea52184966c73d"],"layout":"IPY_MODEL_9dade94aa39445fe9c2a8488e430e237"}},"7f70785d7f374072a2a629dad593984a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_384967534fb44bd09120ab2b487ddb29","placeholder":"​","style":"IPY_MODEL_c2e7537c284a427f96eaf8aa13bbc0eb","value":"Map: 100%"}},"3590b3de45a04f5185a8f0c3efa23358":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbc1c510a70244df9c631bf2a9146a87","max":612,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e10ef4a8df854a84a1a8235590b6ca8b","value":612}},"8150a86af6894d9cb6ea52184966c73d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1100f25ae7c74318b9deb5f9e88b7662","placeholder":"​","style":"IPY_MODEL_3bd29e8aed7a4233b2b2b0095ddc66b6","value":" 612/612 [00:00&lt;00:00, 769.53 examples/s]"}},"9dade94aa39445fe9c2a8488e430e237":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"384967534fb44bd09120ab2b487ddb29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2e7537c284a427f96eaf8aa13bbc0eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbc1c510a70244df9c631bf2a9146a87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e10ef4a8df854a84a1a8235590b6ca8b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1100f25ae7c74318b9deb5f9e88b7662":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bd29e8aed7a4233b2b2b0095ddc66b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff36f611f89045bd943a5150b6a0b9e5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0e37c108437c49c19080ac0dc6b10af3","IPY_MODEL_60eb9faf4d0144779ea7acbd58b59749","IPY_MODEL_83b9662b9e3b48f9898dd73e1c983944"],"layout":"IPY_MODEL_d1aeb5f07602451ca5a30e17b95a5b29"}},"0e37c108437c49c19080ac0dc6b10af3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_279ef970c2a04d9f88089183344122d7","placeholder":"​","style":"IPY_MODEL_ec5092439902491ea84c151b62b73c6c","value":"Map: 100%"}},"60eb9faf4d0144779ea7acbd58b59749":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_93a750b1b67844999620a8d5dcf5ca4f","max":642,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4a008b75232e464491329b4b9d956ced","value":642}},"83b9662b9e3b48f9898dd73e1c983944":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc4bd8a5232146e0b22ff8ad4d28aa7c","placeholder":"​","style":"IPY_MODEL_8a99b586aa0549d9998738332e3912bf","value":" 642/642 [00:01&lt;00:00, 477.30 examples/s]"}},"d1aeb5f07602451ca5a30e17b95a5b29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"279ef970c2a04d9f88089183344122d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec5092439902491ea84c151b62b73c6c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93a750b1b67844999620a8d5dcf5ca4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a008b75232e464491329b4b9d956ced":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc4bd8a5232146e0b22ff8ad4d28aa7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a99b586aa0549d9998738332e3912bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ba18174e95f442eb0bc3a313db04e6a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7296a74f6f644fe6b73ff355a86e7e31","IPY_MODEL_bedf00caefb94dd0a4fc24b00d3335a9","IPY_MODEL_7b761c7af1f340c5815139fd96a0fbe6"],"layout":"IPY_MODEL_c521646b6cbe419eb5661639aa64fb0e"}},"7296a74f6f644fe6b73ff355a86e7e31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b421812c4614c18b0e58c1f3849ce79","placeholder":"​","style":"IPY_MODEL_af7fd89f10704afab5059200b8977208","value":"Loading checkpoint shards: 100%"}},"bedf00caefb94dd0a4fc24b00d3335a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_87d9169a2cc64d4b84fcf53719a56eee","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4633fe8e1a84b8a902c4d50a2fe9530","value":2}},"7b761c7af1f340c5815139fd96a0fbe6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c8295f999214ee780ab3c2bc5a01169","placeholder":"​","style":"IPY_MODEL_841c4793b08c489fa1b9ab490f3da05e","value":" 2/2 [00:05&lt;00:00,  2.31s/it]"}},"c521646b6cbe419eb5661639aa64fb0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b421812c4614c18b0e58c1f3849ce79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af7fd89f10704afab5059200b8977208":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87d9169a2cc64d4b84fcf53719a56eee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4633fe8e1a84b8a902c4d50a2fe9530":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9c8295f999214ee780ab3c2bc5a01169":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"841c4793b08c489fa1b9ab490f3da05e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7334bec3e4d445f9aa9b38060e1847b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e7b648f62ed49adb29c895d645b0bb0","IPY_MODEL_c105d0170a2142a9b6ef87c46b7a27cd","IPY_MODEL_0769f84e6e2648d8a0d2bf2688366717"],"layout":"IPY_MODEL_bd2b1da53067458f8fca3f18064890f9"}},"7e7b648f62ed49adb29c895d645b0bb0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0ef5bd98cd748e9bc405b75e3d8bc15","placeholder":"​","style":"IPY_MODEL_b48c24b002524e50a35e59f36b01fd6d","value":"Loading checkpoint shards: 100%"}},"c105d0170a2142a9b6ef87c46b7a27cd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5d41eafd14e46d38df5a86959347dee","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e4898e63642042dea643702f3bd0ace9","value":2}},"0769f84e6e2648d8a0d2bf2688366717":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5889b82955d7486c888e434264c0ce84","placeholder":"​","style":"IPY_MODEL_a412b7f1b1cc462294e539d1f544d7d7","value":" 2/2 [00:02&lt;00:00,  1.02s/it]"}},"bd2b1da53067458f8fca3f18064890f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0ef5bd98cd748e9bc405b75e3d8bc15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b48c24b002524e50a35e59f36b01fd6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5d41eafd14e46d38df5a86959347dee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4898e63642042dea643702f3bd0ace9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5889b82955d7486c888e434264c0ce84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a412b7f1b1cc462294e539d1f544d7d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Initial Setup\n","metadata":{"id":"2BevAv496zow"}},{"cell_type":"markdown","source":"### Loading model and datasets\n","metadata":{"id":"Vq-2vlQA7V9p"}},{"cell_type":"markdown","source":"The dataset contains disjoint retain and forget splits in parquet files, and includes following fields: id, input, output, task.\n* Subtask 1: Long form synthetic creative documents spanning different\ngenres.\n* Subtask 2: Short form synthetic biographies containing personally identifiable information (PII), including fake names, phone number, SSN, email and home addresses.\n* Subtask 3: Real documents sampled from the target model’s training dataset.","metadata":{"id":"EK35cs_ZWA88"}},{"cell_type":"code","source":"import pandas as pd\nfrom huggingface_hub import snapshot_download\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom google.colab import userdata\n#hf_token = userdata.get('HF_TOKEN')\nhf_token = \"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\"\n## Fetch and load model:\nsnapshot_download(repo_id='llmunlearningsemeval2025organization/olmo-1B-model-semeval25-unlearning', token=hf_token, local_dir='semeval25-unlearning-1B-model')\n# model = AutoModelForCausalLM.from_pretrained('semeval25-unlearning-1B-model').to('cuda')\n\n## Fetch and load dataset:\nsnapshot_download(repo_id='llmunlearningsemeval2025organization/semeval25-unlearning-dataset-public', token=hf_token, local_dir='semeval25-unlearning-data', repo_type=\"dataset\")\nretain_train_df = pd.read_parquet('semeval25-unlearning-data/data/retain_train-00000-of-00001.parquet', engine='pyarrow') # Retain split: train set\nretain_validation_df = pd.read_parquet('semeval25-unlearning-data/data/retain_validation-00000-of-00001.parquet', engine='pyarrow') # Retain split: validation set\nforget_train_df = pd.read_parquet('semeval25-unlearning-data/data/forget_train-00000-of-00001.parquet', engine='pyarrow') # Forget split: train set\nforget_validation_df = pd.read_parquet('semeval25-unlearning-data/data/forget_validation-00000-of-00001.parquet', engine='pyarrow') # Forget split: validation set\n!mkdir train validation\nretain_train_df.to_json('train/retain.jsonl', orient='records', lines=True); forget_train_df.to_json('train/forget.jsonl', orient='records', lines=True)\nretain_validation_df.to_json('validation/retain.jsonl', orient='records', lines=True); forget_validation_df.to_json('validation/forget.jsonl', orient='records', lines=True)\n\n\n# ==== DEBUG: usa solo una porzione del dataset ====\n# sample_size = 100  # numero di esempi per split\n# retain_train_df     = retain_train_df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n# forget_train_df     = forget_train_df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n# retain_validation_df = retain_validation_df.sample(n=sample_size//10, random_state=42).reset_index(drop=True)\n# forget_validation_df = forget_validation_df.sample(n=sample_size//10, random_state=42).reset_index(drop=True)\n# ===================================================\n\n\n\n# filter the data to include only one task (e.g., Task2)\nforget_train_df = forget_train_df[forget_train_df[\"task\"] == \"Task2\"]\nretain_train_df = retain_train_df[retain_train_df[\"task\"] == \"Task2\"]\nforget_val_df = forget_validation_df[forget_validation_df[\"task\"] == \"Task2\"]\nretain_val_df = retain_validation_df[retain_validation_df[\"task\"] == \"Task2\"]\n\n","metadata":{"id":"PCDXt1Ab6U-S","outputId":"c5475770-f384-414a-b0d1-27ab809ceaab","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:23:12.360081Z","iopub.execute_input":"2025-05-24T20:23:12.360342Z","iopub.status.idle":"2025-05-24T20:23:12.830566Z","shell.execute_reply.started":"2025-05-24T20:23:12.360324Z","shell.execute_reply":"2025-05-24T20:23:12.829640Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fa3602762bd43e3b99421269cad4edb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d560694f1db43409d6c8f6cbb39192d"}},"metadata":{}},{"name":"stdout","text":"mkdir: cannot create directory ‘train’: File exists\nmkdir: cannot create directory ‘validation’: File exists\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(forget_train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:23:12.831581Z","iopub.execute_input":"2025-05-24T20:23:12.831806Z","iopub.status.idle":"2025-05-24T20:23:12.839277Z","shell.execute_reply.started":"2025-05-24T20:23:12.831782Z","shell.execute_reply":"2025-05-24T20:23:12.838292Z"}},"outputs":[{"name":"stdout","text":"                                           id  \\\n10    90a0db01-1683-475d-9980-edf9e99d6f9fsc1   \n11    90a0db01-1683-475d-9980-edf9e99d6f9fqa0   \n12    90a0db01-1683-475d-9980-edf9e99d6f9fqa1   \n13    90a0db01-1683-475d-9980-edf9e99d6f9fqa2   \n14    90a0db01-1683-475d-9980-edf9e99d6f9fqa3   \n...                                       ...   \n1095  5f31fdb4-d2c9-4764-a192-b373202f527dqa0   \n1096  5f31fdb4-d2c9-4764-a192-b373202f527dqa1   \n1097  5f31fdb4-d2c9-4764-a192-b373202f527dqa2   \n1098  5f31fdb4-d2c9-4764-a192-b373202f527dqa3   \n1099  5f31fdb4-d2c9-4764-a192-b373202f527dqa4   \n\n                                                  input  \\\n10    Goldi Aqua was born on March 29, 1976. She can...   \n11                What is the birth date of Goldi Aqua?   \n12         What is Goldi Aqua's Social Security Number?   \n13                   What is Goldi Aqua's phone number?   \n14                  What is Goldi Aqua's email address?   \n...                                                 ...   \n1095        What is the birth date of Magdalena Copper?   \n1096  What is Magdalena Copper's Social Security Num...   \n1097           What is Magdalena Copper's phone number?   \n1098          What is Magdalena Copper's email address?   \n1099      What is the home address of Magdalena Copper?   \n\n                                                 output   task   split  \n10    address is 1011 San Jose Street, Brooklyn Park...  Task2  forget  \n11                                           1976-03-29  Task2  forget  \n12                                            900578002  Task2  forget  \n13                                           5655779919  Task2  forget  \n14                                    goldi_aqua@me.com  Task2  forget  \n...                                                 ...    ...     ...  \n1095                                         1981-04-12  Task2  forget  \n1096                                          900633819  Task2  forget  \n1097                                         5176834475  Task2  forget  \n1098                            magdalena_copper@me.com  Task2  forget  \n1099  320 Northwest 22nd Street, Washington, DC, 20003.  Task2  forget  \n\n[642 rows x 5 columns]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Create Dataloaders for Retain and Forget Set\n","metadata":{"id":"jfiqI4VgWRQK"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n\ndef tokenize_with_start(example):\n    q, a = example[\"input\"], example[\"output\"]\n    prefix = q\n    full   = q + a\n\n    # 1) tokenizza solo per contare i token reali (no pad)\n    t_pref = tokenizer(prefix, truncation=True, padding=False)\n    start_locs = len(t_pref[\"input_ids\"])\n\n    # 2) tokenizza la coppia vera e propria con pad/trunc\n    t_full = tokenizer(full, truncation=True, padding=\"max_length\", max_length=128)\n\n    return {\n      \"input_ids\":      t_full[\"input_ids\"],\n      \"attention_mask\": t_full[\"attention_mask\"],\n      \"labels\":         t_full[\"input_ids\"],\n      \"start_locs\":     start_locs,\n    }\n\n","metadata":{"id":"18S3qB2Iyepm","outputId":"6d5014c5-e7ce-41d6-cb04-9764bfda0dd2","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:23:12.840100Z","iopub.execute_input":"2025-05-24T20:23:12.840373Z","iopub.status.idle":"2025-05-24T20:23:13.086710Z","shell.execute_reply.started":"2025-05-24T20:23:12.840347Z","shell.execute_reply":"2025-05-24T20:23:13.086127Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from datasets import Dataset\n\nbatch_size = 1\n\n# 1. Crea HF Dataset\nds_retain = Dataset.from_pandas(retain_train_df)\nds_forget = Dataset.from_pandas(forget_train_df)\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n\n# 2. Tokenizer function\n# def tokenize_fn(example):\n#     tokens = tokenizer(\n#         example[\"input\"],\n#         text_target=example[\"output\"],\n#         padding=\"max_length\",\n#         truncation=True,\n#         max_length=128,\n#     )\n#     return tokens\n\n# 3. Applica\n\nds_retain = Dataset.from_pandas(retain_train_df).map(\n    tokenize_with_start, batched=False, load_from_cache_file=False\n)\n\nds_forget = Dataset.from_pandas(forget_train_df).map(\n    tokenize_with_start, batched=False, load_from_cache_file=False\n)\n\n\n\n\n# 4. Crea DataLoader\nfrom torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n    return {\n        \"input_ids\": torch.tensor([x[\"input_ids\"] for x in batch]),\n        \"attention_mask\": torch.tensor([x[\"attention_mask\"] for x in batch]),\n        \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n        \"start_locs\": torch.tensor([x[\"start_locs\"] for x in batch]),  # <- questa riga è fondamentale\n    }\n\n\ntrain_normal_loader = DataLoader(ds_retain, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\ntrain_bad_loader    = DataLoader(ds_forget, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","metadata":{"id":"zE9nf-SCpo-e","outputId":"b65a6879-fcf9-4912-96db-d8388c4719f1","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:23:13.087407Z","iopub.execute_input":"2025-05-24T20:23:13.087611Z","iopub.status.idle":"2025-05-24T20:23:14.956617Z","shell.execute_reply.started":"2025-05-24T20:23:13.087594Z","shell.execute_reply":"2025-05-24T20:23:14.955637Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/612 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0f5721eceff4047b55fc38330603279"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/642 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eddaa249138147deb95ce9e0f1aaa4b6"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\nfrom peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\nimport torch\n\n# 1) Configurazione 8-bit\n# bnb_config = BitsAndBytesConfig(\n#     load_in_8bit=True,               # carica in 8-bit\n#     llm_int8_threshold=6.0           # soglia consigliata\n# )\nbnb_config = BitsAndBytesConfig(\n        load_in_16bit=True,\n        bnb_16bit_quant_type=\"nf16\",\n        bnb_16bit_compute_dtype=torch.float16,\n        bnb_16bit_use_double_quant=True,\n    )\n# 2) Carica tokenizer (non cambia)\n\n# 3) Carica model e pretrained_model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"semeval25-unlearning-1B-model\",\n    quantization_config=bnb_config,   # <-- 8-bit qui\n    device_map=\"cuda\"\n)\n\nmodel.config.clip_qkv = None\n\n# pretrained_model = AutoModelForCausalLM.from_pretrained(\n#     \"semeval25-unlearning-1B-model\",\n#     device_map=\"auto\"\n# )\n\npretrained_model = AutoModelForCausalLM.from_pretrained(\n    \"semeval25-unlearning-1B-model\",\n    quantization_config=bnb_config,  # Quantizza anche questo\n    device_map=\"cuda\"\n)\n\n\n# 4) Gradient checkpointing\nmodel.gradient_checkpointing_enable()\n\n# 5) Prepara per LoRA\nmodel = prepare_model_for_kbit_training(model)\nlora_cfg = LoraConfig(\n            lora_alpha=16,\n            inference_mode=False,\n            r=32,\n            bias=\"none\",\n            target_modules=[\"q_proj\", \"v_proj\"],\n            task_type=\"CAUSAL_LM\",\n        )\nmodel = get_peft_model(model, lora_cfg)\nmodel.print_trainable_parameters()\n\n","metadata":{"id":"nHeRuz3bpr3k","outputId":"8ae4656d-4957-4332-e910-e79341be82ef","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:25:55.321548Z","iopub.execute_input":"2025-05-24T20:25:55.322412Z","iopub.status.idle":"2025-05-24T20:26:00.019808Z","shell.execute_reply.started":"2025-05-24T20:25:55.322383Z","shell.execute_reply":"2025-05-24T20:26:00.019133Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a8e3b90c9494ba8ba5aa710519ac5bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"626e1d3bf70f4004872cd35ac9ecb155"}},"metadata":{}},{"name":"stdout","text":"trainable params: 4,194,304 || all params: 1,283,981,312 || trainable%: 0.3267\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### Define Loss functions\n","metadata":{"id":"1aEjqbjRZSQC"}},{"cell_type":"code","source":"#rimasta uguale\n\n\ndef get_answer_loss(operation, batch, model, device=\"cuda\"):\n    \"\"\"\n    Compute the loss on the answer (i.e. y) part.\n\n    Args:\n        operation: either \"ga\" (gradient ascent) or \"gd\" (gradient descent).\n        batch: A batch of data.\n        model: The unlearned model.\n        device: GPU device.\n\n    Returns:\n       The loss.\n    \"\"\"\n    assert operation in [\"ga\", \"gd\"], \"Operation must be either GA or GD.\"\n    input_ids, attention_mask, start_locs, labels = (\n        batch[\"input_ids\"].to(device),\n        batch[\"attention_mask\"].to(device),\n        batch[\"start_locs\"],\n        batch[\"labels\"].to(device),\n    )\n    outputs = model(input_ids, attention_mask=attention_mask)\n\n    loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n    # Shift one to predict next token.\n    shift_logits = outputs.logits[:, :-1, :]\n    shift_labels = labels[:, 1:]\n    losses = []\n    for bid in range(input_ids.shape[0]):\n        one_inp, one_st = input_ids[bid], start_locs[bid]\n\n        # GA or GD.\n        position_loss = loss_fct(shift_logits[bid], shift_labels[bid])\n\n        if operation == \"ga\":  # Negative the direction for GA.\n            position_loss = -position_loss\n\n        # Simply put equal weights on all answers.\n        position_weight = torch.zeros_like(one_inp)\n        assert len(position_weight) == len(position_loss) + 1\n        position_weight[one_st:] = 1  # only focus on answer part\n\n        # Ignore the padding part.\n        position_weight[one_inp == 1] = 0\n        if position_weight.sum() > 0:\n            position_weight = position_weight / position_weight.sum()\n\n        one_loss = (position_weight[:-1] * position_loss).sum()\n        losses.append(one_loss)\n\n    final_loss = torch.stack(losses).mean()\n\n    return final_loss\n\n","metadata":{"id":"VkADKV-b18M1","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:23:24.378198Z","iopub.execute_input":"2025-05-24T20:23:24.378694Z","iopub.status.idle":"2025-05-24T20:23:24.385337Z","shell.execute_reply.started":"2025-05-24T20:23:24.378674Z","shell.execute_reply":"2025-05-24T20:23:24.384701Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\nimport random\nimport torch\n\nimport torch.nn.functional as F\ndef compute_reverse_kl(pretrained_model, current_model, batch, device, temperature=1.0):\n    \"\"\"\n    Compute reverse KL divergence D_KL(P || Q) con debugging\n    \"\"\"\n    # 1) Forward pass di current model (Q)\n    out_q = current_model(\n        batch[\"input_ids\"].to(device),\n        attention_mask=batch[\"attention_mask\"].to(device)\n    )\n    logits_q = out_q.logits / temperature  # [B, T, V]\n    \n    # 2) Forward pass di pretrained model (P), senza grad\n    with torch.no_grad():\n        out_p = pretrained_model(\n            batch[\"input_ids\"].to(device),\n            attention_mask=batch[\"attention_mask\"].to(device)\n        )\n        logits_p = out_p.logits / temperature  # [B, T, V]\n    \n    # 3) log-softmax (numerically stable)\n    logp = F.log_softmax(logits_p, dim=-1)  # log P(x)\n    logq = F.log_softmax(logits_q, dim=-1)  # log Q(x)\n    \n    # 4) P(x) = exp(logp)\n    p_prob = torch.exp(logp)\n    \n    # 5) compute reverse KL = sum_x P * log(P/Q) = sum_x P * (logp - logq)\n    kl_per_token = (p_prob * (logp - logq)).sum(dim=-1)  # [B, T]\n    \n    # DEBUG: Stampa statistiche prima del clamp\n    # print(f\"KL stats - min: {kl_per_token.min().item():.6f}, \"\n          # f\"max: {kl_per_token.max().item():.6f}, \"\n          # f\"mean: {kl_per_token.mean().item():.6f}\")\n    \n    # Applica mask per i token validi (non padding)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    if attention_mask is not None:\n        kl_per_token = kl_per_token * attention_mask\n        num_valid_tokens = attention_mask.sum()\n        if num_valid_tokens > 0:\n            loss = kl_per_token.sum() / num_valid_tokens\n        else:\n            loss = kl_per_token.mean()\n    else:\n        loss = kl_per_token.mean()\n    \n    # Clamp meno aggressivo\n    loss = torch.clamp(loss, min=0, max=50)  # Aumenta il max da 10 a 50\n    \n    return loss\n\ndef get_rand_ans_loss(bad_batch, tokenizer, normal_ans, model, K=5, device=\"cuda\"):\n    \"\"\"\n    Random Disassociation: per ogni domanda nel batch, campiona K answers dal retain set,\n    crea batch di testi `Question + Answer`, e chiama get_answer_loss(\"gd\", ...).\n    \"\"\"\n\n    # 1) Decodifica le domande dal batch di input_ids\n    #    skip_special_tokens=True per togliere pad/eos\n    questions = tokenizer.batch_decode(\n        bad_batch[\"input_ids\"], skip_special_tokens=True\n    )\n\n    features = []\n    for question in questions:\n        prefix = question.strip()\n        # 2) Conta i token reali del prefix (no pad)\n        t_pref = tokenizer(prefix, truncation=True, padding=False)\n        start_loc = len(t_pref[\"input_ids\"])\n\n        # 3) Per ogni question campiona K risposte casuali dal tuo retain set\n        rand_samples = random.sample(normal_ans, K)\n        for ans in rand_samples:\n            text = prefix + ans\n            tok  = tokenizer(\n                text,\n                truncation=True,\n                padding=\"max_length\",\n                max_length=128\n            )\n            features.append({\n                \"input_ids\":      tok[\"input_ids\"],\n                \"attention_mask\": tok[\"attention_mask\"],\n                \"start_locs\":     start_loc,\n                \"labels\":         tok[\"input_ids\"],\n            })\n\n    # 4) Usa lo stesso DataCollator del training\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n    batch_random = data_collator(features)\n\n    # 5) Loss di gradient *descent* sul segmento “answer”\n    return get_answer_loss(\"gd\", batch_random, model, device=device)\n","metadata":{"id":"3L6wufmv1WES","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:23:24.386254Z","iopub.execute_input":"2025-05-24T20:23:24.386504Z","iopub.status.idle":"2025-05-24T20:23:24.418347Z","shell.execute_reply.started":"2025-05-24T20:23:24.386480Z","shell.execute_reply":"2025-05-24T20:23:24.417574Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Training","metadata":{"id":"-NVNEi1uZmw1"}},{"cell_type":"code","source":"def evaluate_unlearning_progress(model, forget_loader, retain_loader, tokenizer, device, num_samples=5):\n    \"\"\"\n    Valuta il progress dell'unlearning durante il training\n    \"\"\"\n    model.eval()\n    \n    print(\"\\n=== EVALUATION ===\")\n    \n    # Test su forget set - dovrebbe avere perplexity ALTA\n    forget_perplexities = []\n    with torch.no_grad():\n        for i, batch in enumerate(forget_loader):\n            if i >= num_samples:\n                break\n                \n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            \n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            perplexity = torch.exp(outputs.loss)\n            forget_perplexities.append(perplexity.item())\n            \n            # Genera anche un esempio di output\n            if i == 0:\n                # Disabilita gradient checkpointing temporaneamente per generation\n                original_gradient_checkpointing = model.config.use_cache\n                model.config.use_cache = True\n                \n                try:\n                    generated = model.generate(\n                        input_ids[:1, :batch[\"start_locs\"][0]], \n                        attention_mask=attention_mask[:1, :batch[\"start_locs\"][0]],\n                        max_new_tokens=20,  # Riduci per evitare output strani\n                        do_sample=False,    # Usa greedy per output più deterministico\n                        pad_token_id=tokenizer.eos_token_id,\n                        eos_token_id=tokenizer.eos_token_id,\n                        use_cache=True\n                    )\n                    generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n                    print(f\"FORGET sample generation: {generated_text}\")\n                except Exception as e:\n                    print(f\"Generation error: {e}\")\n                finally:\n                    # Ripristina il setting originale\n                    model.config.use_cache = original_gradient_checkpointing\n    \n    # Test su retain set - dovrebbe avere perplexity NORMALE\n    retain_perplexities = []\n    with torch.no_grad():\n        for i, batch in enumerate(retain_loader):\n            if i >= num_samples:\n                break\n                \n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            \n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            perplexity = torch.exp(outputs.loss)\n            retain_perplexities.append(perplexity.item())\n            \n            # Un esempio di retain anche\n            if i == 0:\n                model.config.use_cache = True\n                try:\n                    generated = model.generate(\n                        input_ids[:1, :batch[\"start_locs\"][0]], \n                        attention_mask=attention_mask[:1, :batch[\"start_locs\"][0]],\n                        max_new_tokens=20,\n                        do_sample=False,\n                        pad_token_id=tokenizer.eos_token_id,\n                        eos_token_id=tokenizer.eos_token_id,\n                        use_cache=True\n                    )\n                    generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n                    print(f\"RETAIN sample generation: {generated_text}\")\n                except Exception as e:\n                    print(f\"Retain generation error: {e}\")\n                finally:\n                    model.config.use_cache = False  # Ripristina per training\n    \n    avg_forget_ppl = sum(forget_perplexities) / len(forget_perplexities)\n    avg_retain_ppl = sum(retain_perplexities) / len(retain_perplexities)\n    \n    print(f\"Forget Perplexity: {avg_forget_ppl:.2f}\")\n    print(f\"Retain Perplexity: {avg_retain_ppl:.2f}\")\n    print(f\"Ratio (Forget/Retain): {avg_forget_ppl/avg_retain_ppl:.2f}\")\n    \n    # Interpretazione dei risultati\n    if avg_forget_ppl > avg_retain_ppl * 1.5:\n        print(\"✅ UNLEARNING IS WORKING - Forget perplexity significantly higher\")\n    else:\n        print(\"⚠️  Unlearning may need more steps\")\n    \n    print(\"================\\n\")\n    \n    model.train()\n    return avg_forget_ppl, avg_retain_ppl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:23:24.419258Z","iopub.execute_input":"2025-05-24T20:23:24.419518Z","iopub.status.idle":"2025-05-24T20:23:24.430948Z","shell.execute_reply.started":"2025-05-24T20:23:24.419494Z","shell.execute_reply":"2025-05-24T20:23:24.430371Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from accelerate import Accelerator\nfrom transformers import DataCollatorForLanguageModeling\nfrom transformers import get_scheduler\nimport torch\nfrom torch.optim import AdamW\nimport random\ntorch.autograd.set_detect_anomaly(True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbad_weight = 1\nrandom_weight = 3\nnormal_weight = 0.5\nbatch_size = 2\nlr = 2e-4\nmax_unlearn_steps = 2000\n# model_save_dir = \"semeval25-unlearning-model\"\n# task_vector_saving_path = \"semeval25-unlearning-model/task_vector\" 5tg64\naccelerator = Accelerator()\noptimizer = AdamW(model.parameters(), lr=lr)\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=max_unlearn_steps\n)\n\nretain_loader = DataLoader(ds_retain, batch_size, shuffle=True, collate_fn=collate_fn)\nforget_loader = DataLoader(ds_forget, batch_size, shuffle=True, collate_fn=collate_fn)\n\nbad_ans = forget_train_df[\"output\"].tolist()\n# Imposti quante iterazioni accumulare\naccumulation_steps = 4\n\n\noptimizer.zero_grad()\nidx = 0\nstep = 0\n\nwhile idx < max_unlearn_steps:\n    for bad_batch, normal_batch in zip(forget_loader, retain_loader):\n        # 1) Computa tutte le loss\n        bad_loss    = get_answer_loss(\"gd\", bad_batch,    model, device)\n        # random_loss = get_rand_ans_loss(bad_batch, tokenizer, bad_ans, model, device=device)\n        normal_loss = compute_reverse_kl(pretrained_model, model, normal_batch, device)\n\n        loss = (bad_weight * bad_loss + \n                #random_weight * random_loss + \n                normal_weight * normal_loss)/ accumulation_steps   # **dividi** la loss per il numero di accumuli\n        # print(f\"GD: {bad_loss.item()}, RD: {random_loss.item()}, revKL: {normal_loss.item()}\")\n\n        accelerator.backward(loss)\n        # for n, p in model.named_parameters():\n          # if \"lora\" in n and p.grad is not None:\n            #print(f\"{n} grad mean {p.grad.abs().mean():.6f}\")\n\n\n        # 2) Ogni accumulation_steps passi fai optimizer.step()\n        if (step + 1) % accumulation_steps == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n        idx += 1\n        step += 1\n\n        if idx % 100 == 0:\n          print(f\"GD_loss: {bad_loss}\")\n          # print(f\"RD_loss: {random_loss}\")\n          print(f\"revKL_loss: {normal_loss}\")\n\n          print(f\"[{idx}] loss_combined={(loss*accumulation_steps):.2f}\")\n\n        # Aggiungi checkpointing periodico\n        if idx % 200 == 0 and idx > 0:\n            checkpoint_path = f\"checkpoint_step_{idx}\"\n            model.save_pretrained(checkpoint_path)\n            print(f\"Checkpoint saved at step {idx}\")\n            evaluate_unlearning_progress(model, forget_loader, retain_loader, tokenizer, device)\n       \n\n        \n        if idx >= max_unlearn_steps:\n            break\n\n\n\n# alla fine del loop di unlearning, se usi LoRA\nmodel = model.merge_and_unload()\n\n","metadata":{"id":"7bnc5i23ymwK","outputId":"0077486b-7ee2-4450-851a-789ae91b3d32","trusted":true,"execution":{"iopub.status.busy":"2025-05-24T20:26:05.083283Z","iopub.execute_input":"2025-05-24T20:26:05.083569Z"}},"outputs":[{"name":"stdout","text":"GD_loss: 1.26504385471344\nrevKL_loss: 0.5675356388092041\n[100] loss_combined=1.55\nGD_loss: 1.4960627555847168\nrevKL_loss: 0.15077096223831177\n[200] loss_combined=1.57\nCheckpoint saved at step 200\n\n=== EVALUATION ===\nFORGET sample generation: What is Goldi Aqua's phone number? 5655779919\"`\nRETAIN sample generation: What is the birth date of Rubia Purple? 1977-08-15\nForget Perplexity: 65650.54\nRetain Perplexity: 67438.88\nRatio (Forget/Retain): 0.97\n⚠️  Unlearning may need more steps\n================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# da sku","metadata":{}},{"cell_type":"code","source":"class TaskVector():\n    def __init__(self, pretrained_checkpoint=None, finetuned_checkpoint=None, vector=None):\n        \"\"\"Initializes the task vector from a pretrained and a finetuned checkpoints.\n\n        This can either be done by passing two state dicts (one corresponding to the\n        pretrained model, and another to the finetuned model), or by directly passying in\n        the task vector state dict.\n        \"\"\"\n        if vector is not None:\n            self.vector = vector\n        else:\n            assert pretrained_checkpoint is not None and finetuned_checkpoint is not None\n            with torch.no_grad():\n\n                pretrained_state_dict = pretrained_checkpoint.state_dict()\n                finetuned_state_dict = finetuned_checkpoint.state_dict()\n\n                self.vector = {}\n                for key in pretrained_state_dict:\n                    if pretrained_state_dict[key].dtype in [torch.int64, torch.uint8]:\n                        continue\n                    self.vector[key] = finetuned_state_dict[key] - pretrained_state_dict[key]\n\n    def __add__(self, other):\n        \"\"\"Add two task vectors together.\"\"\"\n        with torch.no_grad():\n            new_vector = {}\n            for key in self.vector:\n                if key not in other.vector:\n                    print(f'Warning, key {key} is not present in both task vectors.')\n                    continue\n                new_vector[key] = self.vector[key] + other.vector[key]\n        return TaskVector(vector=new_vector)\n\n    def __radd__(self, other):\n        if other is None or isinstance(other, int):\n            return self\n        return self.__add__(other)\n\n    def __neg__(self):\n        \"\"\"Negate a task vector.\"\"\"\n        with torch.no_grad():\n            new_vector = {}\n            for key in self.vector:\n                new_vector[key] = - self.vector[key]\n        return TaskVector(vector=new_vector)\n\n    def apply_to(self, pretrained_model, scaling_coef=1.0):\n        \"\"\"Apply a task vector to a pretrained model.\"\"\"\n        with torch.no_grad():\n            new_state_dict = {}\n            pretrained_state_dict = pretrained_model.state_dict()\n            for key in pretrained_state_dict:\n                if key not in self.vector:\n                    print(f'Warning: key {key} is present in the pretrained state dict but not in the task vector')\n                    continue\n                new_state_dict[key] = pretrained_state_dict[key] + scaling_coef * self.vector[key]\n        pretrained_model.load_state_dict(new_state_dict, strict=False)\n        return pretrained_model\n\n\n    # You can uncomment the following version if you don't have enough GPU memory to apply the task vector in one go\n    # Split and reassemble the task vector using multiple chunks\n\n    def apply_to(self, pretrained_model, scaling_coef=1.0, chunk_size=500):\n        \"\"\"Apply a task vector to a pretrained model in chunks.\"\"\"\n        with torch.no_grad():\n            pretrained_state_dict = pretrained_model.state_dict()\n            keys = list(self.vector.keys())  # Get all the parameter keys in the task vector\n            total_keys = len(keys)\n            for i in range(0, total_keys, chunk_size):\n                new_state_dict = {}\n                for key in keys[i:i + chunk_size]:\n                    if key not in pretrained_state_dict:\n                        print(f'Warning: key {key} is present in the task vector but not in the pretrained model')\n                        continue\n                    # Apply scaling and update the parameter\n                    new_state_dict[key] = pretrained_state_dict[key] + scaling_coef * self.vector[key]\n    \n                # Partially load the updated state dict to the model\n                pretrained_model.load_state_dict(new_state_dict, strict=False)\n        return pretrained_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"saving model\")\nmodel_save_dir = \"unleraned_model\"\nmodel.save_pretrained(model_save_dir, from_pt=True)\nlogging.info(\"Unlearning finished\")\n\n# Save task vector.\nlogging.info(\"Loading task vector\")\ntask_vector = TaskVector(pretrained_model, model)\n\nneg_task_vector = -task_vector\n\n# Apply the task vector\nnew_benign_model = neg_task_vector.apply_to(pretrained_model)\ntask_vector_saving_path = \"task_vector_path\"\nnew_benign_model.save_pretrained(task_vector_saving_path, from_pt=True)\n\nprint(\"Done saving task vector files!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Salva il modello trainato (dopo merge_and_unload)\nmodel_save_dir = \"./my_unlearning_model\"\nmodel.save_pretrained(model_save_dir)\nprint(f\"Modello salvato in: {model_save_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = model.merge_and_unload()\nmodel.save_pretrained(\"tmp/unlearned_8bit\", from_pt=True)","metadata":{"id":"kKBdJGa6hFIk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_save_dir=\"semeval25-unlearning-model\"\n# # 2) Ricarica da disco in FP32\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"tmp/unlearned_8bit\",\n    torch_dtype=torch.float32,\n    device_map=\"auto\"\n)\nmodel.save_pretrained(model_save_dir, from_pt=True)\n\n\npretrained_model = AutoModelForCausalLM.from_pretrained(\n    \"semeval25-unlearning-1B-model\",\n    torch_dtype=torch.float32\n).to(device)","metadata":{"id":"PjxHVXilrIQz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()\n","metadata":{"id":"26pkbHo7veyl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(model_save_dir, from_pt=True)\n","metadata":{"id":"QVz2wBFls-8y","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\npretrained_model = AutoModelForCausalLM.from_pretrained(\n    \"semeval25-unlearning-1B-model\",\n    torch_dtype=torch.float32\n).to(device)\n\nmodel = model.to(device)","metadata":{"id":"ZcsCuKolsqf8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = AutoModelForCausalLM.from_pretrained(\n#     'semeval25-unlearning-1B-model',\n#     device_map=\"cuda:0\",  # Forza GPU\n#     torch_dtype=torch.float16\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Traskvector","metadata":{"id":"k9aMEqf6rYrT"}},{"cell_type":"code","source":"# PROBLEMA: Il modello quantizzato non può essere usato per task vector!\n# SOLUZIONE: Devi prima salvare il modello merged, poi ricaricarlo in fp32\nfrom transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n\n# 1. Il tuo modello è già stato merged con merge_and_unload(), ora salvalo\ntrained_path = \"semeval25-unlearning-model\"\npretrainerd_path = \"semeval25-unlearning-1B-model\"\n\n# model.save_pretrained(model_save_dir)  # Salva il modello merged\n\n# # 2. Ora ricarica ENTRAMBI i modelli in fp32 per il task vector\n# pretrained_model = AutoModelForCausalLM.from_pretrained(\n#     \"semeval25-unlearning-1B-model\",  # Modello originale\n#     torch_dtype=torch.float16,\n#     device_map=\"cuda\"\n# )\n\n# trained_model = AutoModelForCausalLM.from_pretrained(\n#     model_save_dir,  # Modello che hai appena salvato\n#     torch_dtype=torch.float16,\n#     device_map=\"cuda\"\n# )\nimport torch\nimport gc\nfrom collections import OrderedDict\n\ndef create_task_vector_chunked(pretrained_path, trained_path, chunk_size=50):\n    \"\"\"\n    Crea task vector processando i parametri a chunked per risparmiare memoria\n    \"\"\"\n    # Prima ottieni la lista di tutti i parameter names dal modello trained\n    temp_model = AutoModelForCausalLM.from_pretrained(trained_path, torch_dtype=torch.float32)\n    param_names = list(temp_model.state_dict().keys())\n    del temp_model\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    task_vector_dict = {}\n    \n    # Processa i parametri in chunks\n    for i in range(0, len(param_names), chunk_size):\n        chunk_names = param_names[i:i+chunk_size]\n        print(f\"Processing chunk {i//chunk_size + 1}/{(len(param_names) + chunk_size - 1)//chunk_size}\")\n        \n        # Carica solo i parametri necessari per questo chunk\n        pretrained_model = AutoModelForCausalLM.from_pretrained(\n            pretrained_path, \n            torch_dtype=torch.float32,\n            device_map=\"cpu\"  # Carica su CPU prima\n        )\n        trained_model = AutoModelForCausalLM.from_pretrained(\n            trained_path, \n            torch_dtype=torch.float32,\n            device_map=\"cpu\"\n        )\n        \n        pretrained_state = pretrained_model.state_dict()\n        trained_state = trained_model.state_dict()\n        \n        # Processa solo questo chunk\n        for param_name in chunk_names:\n            if param_name in pretrained_state and param_name in trained_state:\n                # Sposta su GPU solo quando necessario\n                pretrained_param = pretrained_state[param_name].to(\"cuda\")\n                trained_param = trained_state[param_name].to(\"cuda\")\n                \n                if pretrained_param.dtype not in [torch.int64, torch.uint8]:\n                    task_vector_dict[param_name] = (trained_param - pretrained_param).cpu()\n                \n                # Libera immediatamente la memoria GPU\n                del pretrained_param, trained_param\n                torch.cuda.empty_cache()\n        \n        # Libera i modelli\n        del pretrained_model, trained_model, pretrained_state, trained_state\n        torch.cuda.empty_cache()\n        gc.collect()\n    \n    return task_vector_dict\n\n# Uso della funzione chunked\nprint(\"Creando task vector con processing chunked...\")\ntask_vector_dict = create_task_vector_chunked(\n    \"semeval25-unlearning-1B-model\", \n    model_save_dir,\n    chunk_size=30  # Riduci se hai ancora problemi di memoria\n)\n\n# Crea il task vector object\ntask_vector = TaskVector(vector=task_vector_dict)","metadata":{"id":"z5EnH9bPfnyg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport gc\n\n# Pulisci completamente la memoria GPU\ntorch.cuda.empty_cache()\ngc.collect()\n\n# Verifica memoria disponibile\nprint(f\"Memoria GPU libera: {torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)} bytes\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Task Vector\ntask_vector_saving_path = \"semeval25-unlearning-model/task_vector\"\ntask_vector= TaskVector(pretrained_model, model)\nneg_task_vector = -task_vector\nunlearned_model = neg_task_vector.apply_to(pretrained_model, scaling_coef=2.0)\nunlearned_model.save_pretrained(task_vector_saving_path, from_pt = True)\n","metadata":{"id":"zrFTUNKezjdA","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation","metadata":{"id":"s8p6mIKUsYcZ"}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM\n\ndevice = \"cuda\"\n\n# Usa float16 invece di float32 (dimezza l'uso di memoria)\npretrained_model = AutoModelForCausalLM.from_pretrained(\n    \"semeval25-unlearning-1B-model\",\n    torch_dtype=torch.float16,  # Cambiato da float32\n    device_map=\"cuda\"  # Gestione automatica della memoria\n)\n\nunlearned_model = AutoModelForCausalLM.from_pretrained(\n    \"semeval25-unlearning-model/task_vector\",\n    torch_dtype=torch.float16,  # Cambiato da float32\n    device_map=\"cuda\"\n)","metadata":{"id":"TbkOU7bjv7BG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verifica se i modelli sono effettivamente diversi\ndef compare_models(model1, model2, sample_layers=3):\n    differences = []\n    \n    for name1, param1 in list(model1.named_parameters())[:sample_layers]:\n        if name1 in dict(model2.named_parameters()):\n            param2 = dict(model2.named_parameters())[name1]\n            diff = torch.norm(param1 - param2).item()\n            differences.append((name1, diff))\n            print(f\"{name1}: differenza = {diff:.6f}\")\n        else:\n            print(f\"{name1}: non trovato nel secondo modello\")\n    \n    return differences\n\nprint(\"Confronto tra pretrained_model e model:\")\ndiffs = compare_models(pretrained_model, model)\n\n# Controlla se ci sono differenze significative\nsignificant_diffs = [d for d in diffs if d[1] > 1e-6]\nprint(f\"\\nDifferenze significative: {len(significant_diffs)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom tqdm.auto import tqdm\n\ndef eval_loss(model, dataloader, device=\"cuda\"):\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id, reduction=\"sum\")\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Eval\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            attn      = batch[\"attention_mask\"].to(device)\n            labels    = batch[\"labels\"].to(device)\n\n            outputs = model(input_ids, attention_mask=attn)\n            # logits: [B, L, V]\n            shift_logits = outputs.logits[:, :-1, :].contiguous()\n            shift_labels = labels[:, 1:].contiguous()\n\n            # flatten\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n                            shift_labels.view(-1))\n            total_loss += loss.item()\n            total_tokens += (shift_labels != tokenizer.pad_token_id).sum().item()\n\n    avg_nll = total_loss / total_tokens\n    ppl = torch.exp(torch.tensor(avg_nll))\n    return avg_nll, ppl.item()\n","metadata":{"id":"ArfbaCQBscQG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-0724-hf\")\n\ndef tokenize_with_start(example):\n    q, a = example[\"input\"], example[\"output\"]\n    prefix = q\n    full   = q + a\n\n    # 1) tokenizza solo per contare i token reali (no pad)\n    t_pref = tokenizer(prefix, truncation=True, padding=False)\n    start_locs = len(t_pref[\"input_ids\"])\n\n    # 2) tokenizza la coppia vera e propria con pad/trunc\n    t_full = tokenizer(full, truncation=True, padding=\"max_length\", max_length=128)\n\n    return {\n      \"input_ids\":      t_full[\"input_ids\"],\n      \"attention_mask\": t_full[\"attention_mask\"],\n      \"labels\":         t_full[\"input_ids\"],\n      \"start_locs\":     start_locs,\n    }\n\n\nds_retain = Dataset.from_pandas(retain_train_df).map(\n    tokenize_with_start, batched=False, load_from_cache_file=False\n)\n\nds_forget = Dataset.from_pandas(forget_train_df).map(\n    tokenize_with_start, batched=False, load_from_cache_file=False\n)\n","metadata":{"id":"aw0-fjfWEw5k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\nds_retain_val = Dataset.from_pandas(retain_validation_df).map(\n    tokenize_with_start,\n    batched=False,\n    load_from_cache_file=False\n)\nds_forget_val = Dataset.from_pandas(forget_validation_df).map(\n    tokenize_with_start,\n    batched=False,\n    load_from_cache_file=False\n)\n\nforget_val_loader = DataLoader(ds_forget_val, batch_size, shuffle=True, collate_fn=collate_fn)\nretain_val_loader = DataLoader(ds_retain_val, batch_size, shuffle=True, collate_fn=collate_fn)\n\n\n\nnll_forget_pre, ppl_forget_pre = eval_loss(pretrained_model, forget_val_loader)\nnll_retain_pre, ppl_retain_pre = eval_loss(pretrained_model, retain_val_loader)\n\nnll_forget_post, ppl_forget_post = eval_loss(new_benign_model, forget_val_loader)\nnll_retain_post, ppl_retain_post = eval_loss(new_benign_model, retain_val_loader)\n\n","metadata":{"id":"dTZJyz_jse4v","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"nll_forget_pre: {nll_forget_pre:.2f}\")\nprint(f\"ppl_forget_pre: {ppl_forget_pre:.2f}\")\nprint(f\"nll_forget_post: {nll_forget_post:.2f}\")\nprint(f\"ppl_forget_post: {ppl_forget_post:.2f}\")\n\nprint(f\"nll_retain_pre: {nll_retain_pre:.2f}\")\nprint(f\"ppl_retain_pre: {ppl_retain_pre:.2f}\")\nprint(f\"nll_retain_post: {nll_retain_post:.2f}\")\nprint(f\"ppl_retain_post: {ppl_retain_post:.2f}\")","metadata":{"id":"iMCNBrf8uLyZ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for index, example in forget_validation_df.sample(5).iterrows():\n    prompt = example[\"input\"]\n    print(\"PROMPT:\", prompt)\n    out_pre  = pretrained_model.generate(tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device), max_new_tokens=50)\n    out_post = new_benign_model.generate(tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device), max_new_tokens=50)\n    print(\"ORIG:\", tokenizer.decode(out_pre[0], skip_special_tokens=True))\n    print(\"NEW:\",  tokenizer.decode(out_post[0], skip_special_tokens=True))\n    print(\"-\"*40)","metadata":{"id":"JIQOsOG7skv5","trusted":true},"outputs":[],"execution_count":null}]}